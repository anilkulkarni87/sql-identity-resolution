{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SQL Identity Resolution","text":"<p>Production-grade deterministic identity resolution for modern data warehouses.</p> <ul> <li> <p> Multi-Platform</p> <p>Run on DuckDB, Snowflake, BigQuery, or Databricks with the same logic.</p> <p> Platform Setup</p> </li> <li> <p> Production Ready</p> <p>Dry run mode, metrics export, audit logging, and data quality controls.</p> <p> Production Guide</p> </li> <li> <p> SQL-First</p> <p>Pure SQL execution\u2014no Python runtime needed in production.</p> <p> Architecture</p> </li> <li> <p> Open Source</p> <p>Full control, full transparency, zero licensing costs.</p> <p> GitHub</p> </li> </ul>"},{"location":"#what-is-identity-resolution","title":"What is Identity Resolution?","text":"<p>Identity resolution is the process of matching and merging records across multiple data sources to create a unified view of each entity (customer, user, account). </p> <p>This solution uses deterministic matching with label propagation to create stable, explainable identity clusters.</p> <pre><code>graph LR\n    A[CRM: customer_123&lt;br/&gt;email: john@acme.com] --&gt; IDR[Identity Resolution]\n    B[Ecommerce: order_456&lt;br/&gt;email: john@acme.com] --&gt; IDR\n    C[Mobile: device_789&lt;br/&gt;phone: 555-0123] --&gt; IDR\n    IDR --&gt; D[Unified Identity&lt;br/&gt;resolved_id: customer_123]</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"DuckDBSnowflakeBigQueryDatabricks <pre><code># Clone and setup\ngit clone https://github.com/anilkulkarni87/sql-identity-resolution.git\ncd sql-identity-resolution\n\n# Create schema\nduckdb idr.duckdb &lt; sql/duckdb/00_ddl_all.sql\n\n# Run (dry run first!)\npython sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=FULL --dry-run\n</code></pre> <pre><code>-- Create schemas and tables\n-- Run sql/snowflake/00_ddl_all.sql\n\n-- Execute (dry run first!)\nCALL idr_run('FULL', 30, TRUE);  -- TRUE = dry run\n</code></pre> <pre><code># Setup\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/sa.json\n\n# Create schema\nbq query &lt; sql/bigquery/00_ddl_all.sql\n\n# Run (dry run first!)\npython sql/bigquery/idr_run.py --project=my-project --dry-run\n</code></pre> <pre><code># Import notebooks from sql/databricks/notebooks/\n# Run IDR_QuickStart.py to setup\n# Set DRY_RUN widget to \"true\" for preview\n</code></pre> <p> Full Quick Start Guide</p>"},{"location":"#key-features","title":"Key Features","text":"Feature Description Deterministic Matching Rule-based matching on email, phone, loyalty ID, etc. Label Propagation Graph-based clustering using connected components Incremental Processing Watermark-based delta processing Dry Run Mode Preview changes before committing Metrics Export Prometheus, DataDog, webhook integrations Production Hardening Max group size limits, identifier exclusions Golden Profiles Automatic best-record selection"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    subgraph Sources[\"Source Systems\"]\n        S1[CRM]\n        S2[E-commerce]\n        S3[Mobile App]\n    end\n\n    subgraph Meta[\"Metadata Layer\"]\n        M1[source_table]\n        M2[rule]\n        M3[identifier_mapping]\n    end\n\n    subgraph Process[\"Processing\"]\n        P1[Build Edges]\n        P2[Label Propagation]\n        P3[Cluster Assignment]\n    end\n\n    subgraph Output[\"Output Layer\"]\n        O1[Membership]\n        O2[Clusters]\n        O3[Golden Profiles]\n        O4[Metrics]\n    end\n\n    Sources --&gt; Process\n    Meta --&gt; Process\n    Process --&gt; Output</code></pre> <p> Full Architecture</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start</p> <p>Get running in 5 minutes</p> </li> <li> <p> Configuration</p> <p>Set up rules and sources</p> </li> <li> <p> Dry Run Mode</p> <p>Preview changes safely</p> </li> <li> <p> Metrics</p> <p>Monitor your pipeline</p> </li> </ul>"},{"location":"cluster_sizing/","title":"Cluster Sizing Guide","text":"<p>Performance recommendations for running sql-identity-resolution at scale.</p>"},{"location":"cluster_sizing/#quick-reference","title":"Quick Reference","text":"Rows per Table Total Rows Max Edge Pairs Expected Clusters 100K 500K ~1M ~40K-60K 500K 2.5M ~5M ~200K-300K 1M 5M ~10M ~400K-600K 5M 25M ~50M ~2M-3M 10M 50M ~100M ~4M-6M"},{"location":"cluster_sizing/#platform-specific-sizing","title":"Platform-Specific Sizing","text":""},{"location":"cluster_sizing/#databricks","title":"Databricks","text":"Scale Cluster Type Workers Node Size Runtime 100K Standard 2-4 m5.xlarge 13.3 LTS 500K Standard 4-8 m5.xlarge 13.3 LTS 1M Standard 8-16 m5.2xlarge 13.3 LTS 5M Photon 16-32 m5.4xlarge 14.3 LTS 10M Photon 32-64 m5.4xlarge 14.3 LTS <p>Spark Config for Scale: <pre><code># 1M+ rows\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n\n# 5M+ rows  \nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")\nspark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n</code></pre></p>"},{"location":"cluster_sizing/#snowflake","title":"Snowflake","text":"Scale Warehouse Size Expected Duration 100K X-Small 2-5 min 500K Small 5-15 min 1M Medium 15-30 min 5M Large 30-60 min 10M X-Large 1-2 hours <p>Tips: <pre><code>-- Scale up warehouse before running\nALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = 'LARGE';\n\n-- Enable clustering for large tables\nALTER TABLE idr_out.identity_edges_current CLUSTER BY (identifier_type);\n</code></pre></p>"},{"location":"cluster_sizing/#bigquery","title":"BigQuery","text":"Scale Pricing Model Slots Expected Duration 100K On-Demand Auto 2-5 min 500K On-Demand Auto 5-15 min 1M On-Demand Auto 15-30 min 5M Flat-Rate 2000+ 30-60 min 10M Flat-Rate 4000+ 1-2 hours <p>Tips: - Use partitioned tables for &gt; 1M rows - Enable BI Engine for faster aggregations - Monitor slot usage in BigQuery Admin</p>"},{"location":"cluster_sizing/#duckdb","title":"DuckDB","text":"Scale RAM Required Expected Duration 100K 2 GB 30s - 2 min 500K 4 GB 2-5 min 1M 8 GB 5-15 min 2M 16 GB 15-30 min <p>Note: DuckDB is best for testing &lt; 10M rows. For larger scale, use cloud platforms.</p>"},{"location":"cluster_sizing/#overlap-rate-impact","title":"Overlap Rate Impact","text":"<p>The <code>OVERLAP_RATE</code> parameter controls identity density:</p> Overlap Rate Effect Use Case 0.3 Many unique identities, few matches Sparse data 0.5 Moderate matching Typical B2C 0.6 Good matching (default) Retail/CDP 0.7 High matching Loyalty-heavy <p>Higher overlap = larger clusters = more LP iterations needed.</p>"},{"location":"cluster_sizing/#label-propagation-iterations","title":"Label Propagation Iterations","text":"Max Cluster Size Iterations Needed &lt; 10 5-10 10-50 10-20 50-200 20-30 200+ 30-50 <p>Warning signs: - Not converging at 30 iterations \u2192 check for \"super clusters\" (bad identifiers) - Giant clusters (1000+) \u2192 likely data quality issue (generic identifiers)</p>"},{"location":"cluster_sizing/#memory-considerations","title":"Memory Considerations","text":"<p>The label propagation step is memory-intensive:</p> <pre><code>Memory \u2248 (nodes \u00d7 2 + edges \u00d7 3) \u00d7 8 bytes\n</code></pre> <p>For 1M entities with 2M edges: - ~48 MB for labels - Spark/Snowflake handle this automatically - DuckDB needs sufficient RAM</p>"},{"location":"cluster_sizing/#monitoring-during-scale-runs","title":"Monitoring During Scale Runs","text":""},{"location":"cluster_sizing/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"<ol> <li>LP Iterations \u2014 Should converge before max</li> <li>Stage Timing \u2014 Identify bottlenecks</li> <li>Cluster Size Distribution \u2014 Giant clusters indicate issues</li> </ol>"},{"location":"cluster_sizing/#query-to-check-health","title":"Query to Check Health","text":"<pre><code>SELECT \n  run_id,\n  status,\n  duration_seconds,\n  lp_iterations,\n  entities_processed,\n  edges_created\nFROM idr_out.run_history\nORDER BY started_at DESC\nLIMIT 5;\n</code></pre>"},{"location":"cluster_sizing/#cluster-health-check","title":"Cluster Health Check","text":"<pre><code>-- Check for giant clusters (potential data issues)\nSELECT \n  CASE \n    WHEN cluster_size = 1 THEN 'singleton'\n    WHEN cluster_size &lt;= 10 THEN 'small'\n    WHEN cluster_size &lt;= 100 THEN 'medium'\n    WHEN cluster_size &lt;= 1000 THEN 'large'\n    ELSE 'GIANT \u26a0\ufe0f'\n  END AS bucket,\n  COUNT(*) AS count,\n  SUM(cluster_size) AS entities\nFROM idr_out.identity_clusters_current\nGROUP BY 1\nORDER BY 1;\n</code></pre>"},{"location":"cluster_sizing/#running-scale-tests","title":"Running Scale Tests","text":""},{"location":"cluster_sizing/#databricks_1","title":"Databricks","text":"<pre><code># Run: sql/databricks/notebooks/IDR_ScaleTest.py\n# Set widget: SCALE = \"1M\"\n</code></pre>"},{"location":"cluster_sizing/#snowflake_1","title":"Snowflake","text":"<pre><code>SET N_ROWS = 1000000;\n-- Run: sql/snowflake/IDR_ScaleTest.sql\n</code></pre>"},{"location":"cluster_sizing/#bigquery_1","title":"BigQuery","text":"<pre><code>python sql/bigquery/idr_scale_test.py --project=your-project --scale=1M\n</code></pre>"},{"location":"cluster_sizing/#duckdb_1","title":"DuckDB","text":"<pre><code>python sql/duckdb/idr_scale_test.py --db=scale_test.duckdb --scale=1M\n</code></pre>"},{"location":"metadata_configuration/","title":"Metadata Configuration Guide","text":"<p>How to configure sql-identity-resolution for your own source tables with custom naming conventions.</p>"},{"location":"metadata_configuration/#overview","title":"Overview","text":"<p>The IDR system is metadata-driven\u2014you describe your source tables via configuration tables, and the system dynamically generates SQL to extract identifiers and build identity graphs. No code changes required.</p> <pre><code>Your Source Tables \u2192 Metadata Configuration \u2192 IDR Run \u2192 Unified Identities\n</code></pre>"},{"location":"metadata_configuration/#quick-start-complete-example","title":"Quick Start: Complete Example","text":"<p>Copy-paste this example and modify table/column names to match your schema.</p> <p>This example shows a typical retail setup with two source tables: CRM customers and web signups.</p> <pre><code>-- ============================================\n-- STEP 1: Register your source tables\n-- ============================================\nINSERT INTO idr_meta.source_table (table_id, table_fqn, entity_type, entity_key_expr, watermark_column, watermark_lookback_minutes, is_active) VALUES\n    ('crm',     'mydb.sales.customers',     'PERSON', 'customer_id', 'updated_at', 0, TRUE),\n    ('web',     'mydb.web.signups',         'PERSON', 'user_id',     'created_at', 0, TRUE);\n\n-- ============================================\n-- STEP 2: Define trust rankings (lower = more trusted)\n-- ============================================\nINSERT INTO idr_meta.source (table_id, source_name, trust_rank, is_active) VALUES\n    ('crm', 'CRM Master Data',     1, TRUE),   -- Most trusted\n    ('web', 'Web Registrations',   2, TRUE);\n\n-- ============================================\n-- STEP 3: Map columns to identifier types\n-- ============================================\nINSERT INTO idr_meta.identifier_mapping (table_id, identifier_type, identifier_value_expr, is_hashed) VALUES\n    -- CRM source\n    ('crm', 'EMAIL', 'email',    FALSE),\n    ('crm', 'PHONE', 'phone',    FALSE),\n\n    -- Web source\n    ('web', 'EMAIL', 'email',    FALSE),\n    ('web', 'PHONE', 'mobile',   FALSE);  -- Different column name, same identifier type\n\n-- ============================================\n-- STEP 4: Map attributes for golden profile\n-- ============================================\nINSERT INTO idr_meta.entity_attribute_mapping (table_id, attribute_name, attribute_expr) VALUES\n    ('crm', 'first_name',        'first_name'),\n    ('crm', 'last_name',         'last_name'),\n    ('crm', 'email_raw',         'email'),\n    ('crm', 'phone_raw',         'phone'),\n    ('crm', 'record_updated_at', 'updated_at'),\n\n    ('web', 'first_name',        'name'),         -- Column has different name\n    ('web', 'email_raw',         'email'),\n    ('web', 'phone_raw',         'mobile'),\n    ('web', 'record_updated_at', 'created_at');\n\n-- ============================================\n-- DONE! Now run: python idr_run.py --run-mode=FULL --dry-run\n-- ============================================\n</code></pre> <p>What this does: 1. Entities from <code>crm.customers</code> and <code>web.signups</code> are extracted 2. Entities sharing the same <code>EMAIL</code> or <code>PHONE</code> are linked into clusters 3. Golden profiles are built using CRM data first (trust_rank=1), falling back to web data</p>"},{"location":"metadata_configuration/#configuration-tables","title":"Configuration Tables","text":"Table Purpose <code>idr_meta.source_table</code> Register your source tables <code>idr_meta.source</code> Define trust rankings for survivorship <code>idr_meta.rule</code> Configure identifier types and matching rules <code>idr_meta.identifier_mapping</code> Map source columns to identifier types <code>idr_meta.entity_attribute_mapping</code> Map columns for golden profile <code>idr_meta.survivorship_rule</code> Define which value wins for each attribute"},{"location":"metadata_configuration/#step-by-step-configuring-your-source-tables","title":"Step-by-Step: Configuring Your Source Tables","text":""},{"location":"metadata_configuration/#step-1-register-source-tables","title":"Step 1: Register Source Tables","text":"<p>Add each source table to <code>idr_meta.source_table</code>:</p> <pre><code>INSERT INTO idr_meta.source_table (\n    table_id,           -- Your unique identifier for this source\n    table_fqn,          -- Fully qualified table name in YOUR database\n    entity_type,        -- PERSON, ACCOUNT, HOUSEHOLD, etc.\n    entity_key_expr,    -- SQL expression for unique entity key\n    watermark_column,   -- Timestamp column for incremental processing\n    watermark_lookback_minutes,\n    is_active\n) VALUES\n    ('crm_customers',                    -- table_id (your choice)\n     'mydb.sales.customers',             -- YOUR table name\n     'PERSON',\n     'customer_id',                      -- YOUR primary key column\n     'last_modified_date',               -- YOUR timestamp column\n     0,\n     TRUE);\n</code></pre>"},{"location":"metadata_configuration/#column-reference","title":"Column Reference","text":"Column Description Example <code>table_id</code> Unique identifier you assign <code>crm_customers</code>, <code>web_signups</code> <code>table_fqn</code> Full table path in your database <code>catalog.schema.table</code> <code>entity_type</code> Type of entity <code>PERSON</code>, <code>ACCOUNT</code>, <code>HOUSEHOLD</code> <code>entity_key_expr</code> SQL expression for unique key <code>customer_id</code>, <code>id::VARCHAR</code> <code>watermark_column</code> Timestamp for delta detection <code>updated_at</code>, <code>load_ts</code> <code>watermark_lookback_minutes</code> Safety buffer for late-arriving data <code>0</code>, <code>60</code> <code>is_active</code> Enable/disable this source <code>TRUE</code>, <code>FALSE</code>"},{"location":"metadata_configuration/#complex-entity-keys","title":"Complex Entity Keys","text":"<p>If your table has a composite primary key:</p> <pre><code>-- Composite key example\nINSERT INTO idr_meta.source_table VALUES\n    ('order_lines', \n     'warehouse.orders.order_lines',\n     'PERSON',\n     'order_id || ''-'' || line_number',  -- Concatenate columns\n     'created_at',\n     0,\n     TRUE);\n</code></pre>"},{"location":"metadata_configuration/#step-2-add-trust-rankings","title":"Step 2: Add Trust Rankings","text":"<p>Add entries to <code>idr_meta.source</code> to define which sources are more trusted:</p> <pre><code>INSERT INTO idr_meta.source (table_id, source_name, trust_rank, is_active) VALUES\n    ('crm_customers', 'CRM Master Data', 1, TRUE),    -- Most trusted\n    ('web_signups', 'Web Registrations', 2, TRUE),\n    ('mobile_app', 'Mobile App Users', 3, TRUE),\n    ('third_party', 'External Data', 10, TRUE);       -- Least trusted\n</code></pre> <p>Note: Lower <code>trust_rank</code> = more trusted. Used in golden profile survivorship.</p>"},{"location":"metadata_configuration/#step-3-map-identifiers","title":"Step 3: Map Identifiers","text":"<p>Map your source columns to identifier types in <code>idr_meta.identifier_mapping</code>:</p> <pre><code>INSERT INTO idr_meta.identifier_mapping \n    (table_id, identifier_type, identifier_value_expr, is_hashed) \nVALUES\n    -- CRM has email and phone\n    ('crm_customers', 'EMAIL', 'email_address', FALSE),\n    ('crm_customers', 'PHONE', 'mobile_phone', FALSE),\n    ('crm_customers', 'LOYALTY_ID', 'loyalty_card_number', FALSE),\n\n    -- Web signups might have different column names\n    ('web_signups', 'EMAIL', 'user_email', FALSE),\n    ('web_signups', 'PHONE', 'contact_phone', FALSE),\n\n    -- Third-party data has hashed emails\n    ('third_party', 'EMAIL', 'email_sha256', TRUE);\n</code></pre>"},{"location":"metadata_configuration/#column-reference_1","title":"Column Reference","text":"Column Description Example <code>table_id</code> Must match <code>source_table.table_id</code> <code>crm_customers</code> <code>identifier_type</code> Must match a <code>rule.identifier_type</code> <code>EMAIL</code>, <code>PHONE</code>, <code>SSN</code> <code>identifier_value_expr</code> SQL expression for the value <code>email_address</code>, <code>UPPER(email)</code> <code>is_hashed</code> Is the value already hashed? <code>FALSE</code>, <code>TRUE</code>"},{"location":"metadata_configuration/#using-sql-expressions","title":"Using SQL Expressions","text":"<p>You can use SQL expressions, not just column names:</p> <pre><code>-- Clean phone number on extraction\n('crm_customers', 'PHONE', 'REGEXP_REPLACE(phone, ''[^0-9]'', '''')', FALSE),\n\n-- Combine first.last as identifier\n('crm_customers', 'NAME_KEY', 'LOWER(first_name || ''.'' || last_name)', FALSE),\n\n-- Handle NULL-like values\n('web_signups', 'EMAIL', 'NULLIF(email, ''N/A'')', FALSE)\n</code></pre>"},{"location":"metadata_configuration/#phone-number-normalization-patterns","title":"Phone Number Normalization Patterns","text":"<p>Phone numbers are tricky because formats vary by region and source system. Use <code>identifier_value_expr</code> to normalize:</p> Scenario Pattern Example US only (10 digits) <code>RIGHT(REGEXP_REPLACE(phone, '[^0-9]', ''), 10)</code> <code>+1 (555) 123-4567</code> \u2192 <code>5551234567</code> Keep country code <code>REGEXP_REPLACE(phone, '[^0-9+]', '')</code> <code>+44 20 7946 0958</code> \u2192 <code>+442079460958</code> Strip leading 1 (US) <code>REGEXP_REPLACE(REGEXP_REPLACE(phone, '[^0-9]', ''), '^1', '')</code> <code>1-555-123-4567</code> \u2192 <code>5551234567</code> <p>Platform-specific examples:</p> DuckDB / SnowflakeBigQueryDatabricks <pre><code>-- US: Strip to last 10 digits\n('crm', 'PHONE', 'RIGHT(REGEXP_REPLACE(phone, ''[^0-9]'', ''''), 10)', FALSE)\n\n-- International: Keep + and digits\n('crm', 'PHONE', 'REGEXP_REPLACE(phone, ''[^0-9+]'', '''')', FALSE)\n</code></pre> <pre><code>-- US: Strip to last 10 digits  \n('crm', 'PHONE', 'RIGHT(REGEXP_REPLACE(phone, r''[^0-9]'', ''''), 10)', FALSE)\n</code></pre> <pre><code>-- US: Strip to last 10 digits\n('crm', 'PHONE', 'RIGHT(REGEXP_REPLACE(phone, ''[^0-9]'', ''''), 10)', FALSE)\n</code></pre> <p>Tip: Test your regex on sample data before running: <code>SELECT REGEXP_REPLACE('+1 (555) 123-4567', '[^0-9]', '')</code></p>"},{"location":"metadata_configuration/#step-4-configure-rules-if-needed","title":"Step 4: Configure Rules (if needed)","text":"<p>Default rules exist for EMAIL and PHONE. Add custom rules for new identifier types:</p> <pre><code>INSERT INTO idr_meta.rule (\n    rule_id, rule_name, is_active, priority, \n    identifier_type, canonicalize, allow_hashed, require_non_null\n) VALUES\n    ('R_LOYALTY_EXACT', 'Loyalty ID Match', TRUE, 3, \n     'LOYALTY_ID', 'NONE', FALSE, TRUE),\n\n    ('R_SSN_EXACT', 'SSN Exact Match', TRUE, 4, \n     'SSN', 'NONE', FALSE, TRUE);\n</code></pre>"},{"location":"metadata_configuration/#canonicalization-options","title":"Canonicalization Options","text":"Option Effect When to Use <code>LOWERCASE</code> <code>John@Gmail.COM</code> \u2192 <code>john@gmail.com</code> Email, usernames <code>UPPERCASE</code> <code>john</code> \u2192 <code>JOHN</code> Case-insensitive IDs <code>NONE</code> No transformation Exact match IDs <code>TRIM</code> Remove whitespace All identifiers <code>PHONE_DIGITS</code> <code>+1 (555) 123-4567</code> \u2192 <code>15551234567</code> Phone numbers"},{"location":"metadata_configuration/#step-5-map-attributes-for-golden-profile","title":"Step 5: Map Attributes for Golden Profile","text":"<p>Map columns to be included in the golden profile:</p> <pre><code>INSERT INTO idr_meta.entity_attribute_mapping \n    (table_id, attribute_name, attribute_expr) \nVALUES\n    -- CRM: use their column names\n    ('crm_customers', 'email_raw', 'email_address'),\n    ('crm_customers', 'phone_raw', 'mobile_phone'),\n    ('crm_customers', 'first_name', 'first_name'),\n    ('crm_customers', 'last_name', 'surname'),                -- Different column name\n    ('crm_customers', 'record_updated_at', 'last_modified_date'),\n\n    -- Web: different column names\n    ('web_signups', 'email_raw', 'user_email'),\n    ('web_signups', 'first_name', 'given_name'),\n    ('web_signups', 'last_name', 'family_name'),\n    ('web_signups', 'record_updated_at', 'signup_date');\n</code></pre> <p>Important: <code>attribute_name</code> must be consistent across sources (e.g., always <code>email_raw</code>), but <code>attribute_expr</code> can vary.</p>"},{"location":"metadata_configuration/#step-6-set-survivorship-rules","title":"Step 6: Set Survivorship Rules","text":"<p>Define how to pick the \"best\" value for each attribute:</p> <pre><code>INSERT INTO idr_meta.survivorship_rule (attribute_name, strategy) VALUES\n    ('email_primary', 'MOST_RECENT'),       -- Latest email wins\n    ('phone_primary', 'SOURCE_PRIORITY'),   -- CRM phone wins over web\n    ('first_name', 'MOST_RECENT'),\n    ('last_name', 'SOURCE_PRIORITY');\n</code></pre> Strategy Logic <code>MOST_RECENT</code> Value with latest <code>record_updated_at</code> <code>SOURCE_PRIORITY</code> Value from highest trust_rank source <code>SOURCE_PRIORITY + MOST_RECENT</code> Trust rank first, then recency"},{"location":"metadata_configuration/#complete-example","title":"Complete Example","text":"<p>Suppose you have these tables with non-standard names:</p> Your Table Primary Key Email Column Phone Column Timestamp <code>erp.client_master</code> <code>client_no</code> <code>email_addr</code> <code>ph_number</code> <code>mod_date</code> <code>web.registrations</code> <code>reg_id</code> <code>e_mail</code> <code>cell</code> <code>reg_ts</code> <code>pos.receipts</code> <code>txn_id</code> <code>cust_email</code> NULL <code>txn_timestamp</code> <p>Configuration:</p> <pre><code>-- 1. Register sources\nINSERT INTO idr_meta.source_table VALUES\n    ('erp_clients', 'erp.client_master', 'PERSON', 'client_no', 'mod_date', 0, TRUE),\n    ('web_regs', 'web.registrations', 'PERSON', 'reg_id', 'reg_ts', 0, TRUE),\n    ('pos_txns', 'pos.receipts', 'PERSON', 'txn_id', 'txn_timestamp', 0, TRUE);\n\n-- 2. Trust rankings\nINSERT INTO idr_meta.source VALUES\n    ('erp_clients', 'ERP Master', 1, TRUE),\n    ('web_regs', 'Web Signups', 2, TRUE),\n    ('pos_txns', 'POS Transactions', 3, TRUE);\n\n-- 3. Map identifiers (using YOUR column names)\nINSERT INTO idr_meta.identifier_mapping VALUES\n    ('erp_clients', 'EMAIL', 'email_addr', FALSE),\n    ('erp_clients', 'PHONE', 'ph_number', FALSE),\n    ('web_regs', 'EMAIL', 'e_mail', FALSE),\n    ('web_regs', 'PHONE', 'cell', FALSE),\n    ('pos_txns', 'EMAIL', 'cust_email', FALSE);\n    -- Note: pos_txns has no phone\n\n-- 4. Map attributes for golden profile\nINSERT INTO idr_meta.entity_attribute_mapping VALUES\n    ('erp_clients', 'email_raw', 'email_addr'),\n    ('erp_clients', 'phone_raw', 'ph_number'),\n    ('erp_clients', 'record_updated_at', 'mod_date'),\n    ('web_regs', 'email_raw', 'e_mail'),\n    ('web_regs', 'phone_raw', 'cell'),\n    ('web_regs', 'record_updated_at', 'reg_ts'),\n    ('pos_txns', 'email_raw', 'cust_email'),\n    ('pos_txns', 'record_updated_at', 'txn_timestamp');\n\n-- 5. Run IDR\n-- Databricks: IDR_Run.py with RUN_MODE=FULL\n-- Snowflake: CALL idr_run('FULL', 30);\n-- DuckDB: python idr_run.py --db=mydb.duckdb --run-mode=FULL\n</code></pre>"},{"location":"metadata_configuration/#validation","title":"Validation","text":"<p>After configuring metadata, verify before running:</p> <pre><code>-- Check sources registered\nSELECT table_id, table_fqn, is_active FROM idr_meta.source_table;\n\n-- Check identifier mappings\nSELECT st.table_id, im.identifier_type, im.identifier_value_expr\nFROM idr_meta.source_table st\nLEFT JOIN idr_meta.identifier_mapping im ON im.table_id = st.table_id\nORDER BY st.table_id, im.identifier_type;\n\n-- Verify rules exist for your identifier types\nSELECT DISTINCT im.identifier_type, r.rule_id\nFROM idr_meta.identifier_mapping im\nLEFT JOIN idr_meta.rule r ON r.identifier_type = im.identifier_type AND r.is_active = TRUE\nWHERE r.rule_id IS NULL;\n-- Result should be empty (all identifier types have rules)\n</code></pre>"},{"location":"metadata_configuration/#common-mistakes","title":"Common Mistakes","text":"Mistake Symptom Fix <code>table_fqn</code> doesn't exist \"Table not found\" error Verify exact table path <code>identifier_type</code> not in rules No edges created Add rule or use existing type Different <code>attribute_name</code> across sources Inconsistent golden profile Use same name, vary <code>attribute_expr</code> Missing <code>record_updated_at</code> mapping Survivorship fails Map a timestamp column"},{"location":"metadata_configuration/#see-also","title":"See Also","text":"<ul> <li>Architecture \u2014 How metadata drives the pipeline</li> <li>Runbook \u2014 Adding new sources at runtime</li> <li>metadata_samples/ \u2014 Example CSV configurations</li> </ul>"},{"location":"runbook/","title":"Runbook","text":"<p>Operational guide for running sql-identity-resolution in production.</p>"},{"location":"runbook/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quick Start</li> <li>Operational Procedures</li> <li>Monitoring</li> <li>Troubleshooting</li> <li>Incident Response</li> </ol>"},{"location":"runbook/#quick-start","title":"Quick Start","text":""},{"location":"runbook/#databricks","title":"Databricks","text":"<pre><code># 1. Load metadata\nRun: sql/databricks/notebooks/IDR_LoadMetadata_Simple.py\n\n# 2. Generate sample data (optional)\nRun: sql/databricks/notebooks/IDR_SampleData_Generate.py\n\n# 3. Run IDR\nRun: IDR_Run.py with RUN_MODE=FULL, MAX_ITERS=30\n\n# 4. Verify\nRun: sql/databricks/notebooks/IDR_SmokeTest.py\n</code></pre>"},{"location":"runbook/#snowflake","title":"Snowflake","text":"<pre><code>-- 1. Setup\n\\i sql/snowflake/00_ddl_all.sql\n\\i sql/snowflake/IDR_SampleData_Generate.sql\n\n-- 2. Run\nCALL idr_run('FULL', 30);\n</code></pre>"},{"location":"runbook/#bigquery","title":"BigQuery","text":"<pre><code># 1. Setup\nbq query &lt; sql/bigquery/00_ddl_all.sql\nbq query &lt; sql/bigquery/idr_sample_data.sql\n\n# 2. Run\npython sql/bigquery/idr_run.py --project=your-project --run-mode=FULL\n</code></pre>"},{"location":"runbook/#duckdb-localtesting","title":"DuckDB (Local/Testing)","text":"<pre><code># 1. Setup\nduckdb idr.duckdb &lt; sql/duckdb/00_ddl_all.sql\npython sql/duckdb/idr_sample_data.py --db=idr.duckdb\n\n# 2. Run\npython sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=FULL\n</code></pre>"},{"location":"runbook/#operational-procedures","title":"Operational Procedures","text":""},{"location":"runbook/#full-vs-incremental-runs","title":"Full vs Incremental Runs","text":"Mode When to Use Impact <code>FULL</code> Initial load, after rule changes, monthly refresh Re-processes all data <code>INCR</code> Daily/hourly runs Only processes delta since last watermark <p>Recommended schedule: <pre><code>Daily:   RUN_MODE=INCR (process deltas)\nWeekly:  Optional FULL refresh for data hygiene\nMonthly: RUN_MODE=FULL (verify consistency)\n</code></pre></p>"},{"location":"runbook/#adding-new-sources","title":"Adding New Sources","text":"<ol> <li> <p>Create source table entry: <pre><code>INSERT INTO idr_meta.source_table VALUES\n    ('new_source', 'catalog.schema.table', 'PERSON', 'customer_id', 'updated_at', 0, TRUE);\n</code></pre></p> </li> <li> <p>Add trust ranking: <pre><code>INSERT INTO idr_meta.source VALUES\n    ('new_source', 'New Source Name', 5, TRUE);  -- trust_rank: lower = more trusted\n</code></pre></p> </li> <li> <p>Map identifiers: <pre><code>INSERT INTO idr_meta.identifier_mapping VALUES\n    ('new_source', 'EMAIL', 'email_column', FALSE),\n    ('new_source', 'PHONE', 'phone_column', FALSE);\n</code></pre></p> </li> <li> <p>Map attributes (for golden profile): <pre><code>INSERT INTO idr_meta.entity_attribute_mapping VALUES\n    ('new_source', 'email_raw', 'email_column'),\n    ('new_source', 'first_name', 'first_name_column');\n</code></pre></p> </li> <li> <p>Run FULL refresh: <pre><code>-- Run with RUN_MODE=FULL to incorporate new source\n</code></pre></p> </li> </ol>"},{"location":"runbook/#adding-new-rules","title":"Adding New Rules","text":"<pre><code>-- Add a new identifier type\nINSERT INTO idr_meta.rule VALUES\n    ('R_SSN_EXACT', 'SSN exact match', TRUE, 4, 'SSN', 'NONE', FALSE, TRUE);\n\n-- Map which sources have this identifier\nINSERT INTO idr_meta.identifier_mapping VALUES\n    ('existing_source', 'SSN', 'ssn_column', TRUE);  -- is_hashed=TRUE if sensitive\n\n-- Run FULL refresh\n</code></pre>"},{"location":"runbook/#schema-migrations","title":"Schema Migrations","text":"<p>[!WARNING] Always backup before schema changes.</p> <ol> <li> <p>Backup current state: <pre><code>CREATE TABLE idr_out.membership_backup_YYYYMMDD AS \nSELECT * FROM idr_out.identity_resolved_membership_current;\n</code></pre></p> </li> <li> <p>Apply DDL changes</p> </li> <li> <p>Run FULL refresh to rebuild outputs</p> </li> <li> <p>Verify with smoke test</p> </li> </ol>"},{"location":"runbook/#monitoring","title":"Monitoring","text":""},{"location":"runbook/#key-metrics","title":"Key Metrics","text":"Metric Query Alert Threshold Run duration <code>SELECT duration_seconds FROM idr_out.run_history ORDER BY started_at DESC LIMIT 1</code> &gt; 2x average Entities processed <code>SELECT entities_processed FROM idr_out.run_history</code> 0 (unexpected) LP iterations <code>SELECT lp_iterations FROM idr_out.run_history</code> &gt; 20 Failed runs <code>SELECT * FROM idr_out.run_history WHERE status = 'FAILED'</code> Any"},{"location":"runbook/#health-check-queries","title":"Health Check Queries","text":"<p>Cluster size distribution: <pre><code>SELECT \n  CASE \n    WHEN cluster_size = 1 THEN '1 (singletons)'\n    WHEN cluster_size &lt;= 5 THEN '2-5'\n    WHEN cluster_size &lt;= 20 THEN '6-20'\n    WHEN cluster_size &lt;= 100 THEN '21-100'\n    ELSE '100+'\n  END AS size_bucket,\n  COUNT(*) AS cluster_count,\n  SUM(cluster_size) AS total_entities\nFROM idr_out.identity_clusters_current\nGROUP BY 1\nORDER BY 1;\n</code></pre></p> <p>Identify problematic identifiers: <pre><code>-- Find identifiers linking too many entities (data quality issue)\nSELECT identifier_type, identifier_value_norm, COUNT(*) AS entity_count\nFROM idr_work.identifiers_all\nGROUP BY 1, 2\nHAVING COUNT(*) &gt; 1000\nORDER BY entity_count DESC\nLIMIT 20;\n</code></pre></p> <p>Run history trends: <pre><code>SELECT \n  DATE(started_at) AS run_date,\n  COUNT(*) AS runs,\n  AVG(duration_seconds) AS avg_duration,\n  SUM(entities_processed) AS total_entities,\n  SUM(edges_created) AS total_edges\nFROM idr_out.run_history\nWHERE started_at &gt; CURRENT_DATE - INTERVAL 7 DAY\nGROUP BY 1\nORDER BY 1;\n</code></pre></p> <p>Watermark status: <pre><code>SELECT \n  st.table_id,\n  st.table_fqn,\n  rs.last_watermark_value,\n  rs.last_run_ts,\n  TIMESTAMPDIFF(HOUR, rs.last_run_ts, CURRENT_TIMESTAMP) AS hours_since_run\nFROM idr_meta.source_table st\nLEFT JOIN idr_meta.run_state rs ON rs.table_id = st.table_id\nWHERE st.is_active = TRUE\nORDER BY hours_since_run DESC;\n</code></pre></p>"},{"location":"runbook/#troubleshooting","title":"Troubleshooting","text":""},{"location":"runbook/#common-issues","title":"Common Issues","text":""},{"location":"runbook/#1-schemas-not-created-error","title":"1. \"Schemas not created\" Error","text":"<p>Cause: DDL scripts not run before IDR execution.</p> <p>Solution: <pre><code># Run DDL first\n# Databricks: Run IDR_QuickStart.py or 00_ddl scripts\n# Snowflake: \\i sql/snowflake/00_ddl_all.sql\n# DuckDB: duckdb mydb.duckdb &lt; sql/duckdb/00_ddl_all.sql\n</code></pre></p>"},{"location":"runbook/#2-no-entities-processed","title":"2. No Entities Processed","text":"<p>Cause: Watermark ahead of source data timestamps.</p> <p>Diagnostic: <pre><code>SELECT table_id, last_watermark_value \nFROM idr_meta.run_state;\n\nSELECT MAX(updated_at) as max_ts \nFROM your_source_table;\n</code></pre></p> <p>Solution: <pre><code>-- Reset watermark to force re-processing\nUPDATE idr_meta.run_state \nSET last_watermark_value = TIMESTAMP '1900-01-01'\nWHERE table_id = 'your_table_id';\n</code></pre></p>"},{"location":"runbook/#3-giant-clusters-10k-entities","title":"3. Giant Clusters (10K+ entities)","text":"<p>Cause: Bad identifier data (NULL, empty, or generic values like \"test@test.com\").</p> <p>Diagnostic: <pre><code>SELECT identifier_value_norm, COUNT(*) as cnt\nFROM idr_work.identifiers_all\nGROUP BY 1\nHAVING COUNT(*) &gt; 1000\nORDER BY cnt DESC;\n</code></pre></p> <p>Solution: 1. Add data quality rules in source ETL 2. Enable <code>require_non_null = TRUE</code> in rules 3. Add exclusion list for known bad values: <pre><code>-- Exclude in canonicalization or identifier extraction\nWHERE identifier_value NOT IN ('test@test.com', 'null', 'N/A')\n</code></pre></p>"},{"location":"runbook/#4-label-propagation-not-converging","title":"4. Label Propagation Not Converging","text":"<p>Cause: Extremely connected graph or circular references.</p> <p>Diagnostic: <pre><code>SELECT lp_iterations \nFROM idr_out.run_history \nORDER BY started_at DESC LIMIT 5;\n</code></pre></p> <p>Solution: - Increase <code>MAX_ITERS</code> if approaching limit - Investigate data quality\u2014deep chains indicate identifier issues - Consider adding <code>max_group_size</code> to rules</p>"},{"location":"runbook/#5-missing-entities-in-membership","title":"5. Missing Entities in Membership","text":"<p>Cause: Singletons (entities without edges) not being added.</p> <p>Diagnostic: <pre><code>SELECT COUNT(*) FROM idr_work.entities_delta\nWHERE entity_key NOT IN (\n  SELECT entity_key FROM idr_out.identity_resolved_membership_current\n);\n</code></pre></p> <p>Solution: Ensure runner includes singleton handling (fixed in Dec 2024): <pre><code>-- Singletons get resolved_id = entity_key\nINSERT INTO membership\nSELECT entity_key, entity_key AS resolved_id\nFROM entities_delta\nWHERE entity_key NOT IN (SELECT entity_key FROM lp_labels);\n</code></pre></p>"},{"location":"runbook/#6-slow-performance","title":"6. Slow Performance","text":"<p>Causes: - Large identifier groups - Unoptimized tables - Insufficient cluster resources</p> <p>Solutions:</p> <p>Optimize tables (Databricks): <pre><code>OPTIMIZE idr_work.identifiers_all \nZORDER BY (identifier_type, identifier_value_norm);\n</code></pre></p> <p>Increase parallelism: <pre><code>spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n</code></pre></p> <p>Right-size cluster: See cluster_sizing.md</p>"},{"location":"runbook/#incident-response","title":"Incident Response","text":""},{"location":"runbook/#failed-run-investigation","title":"Failed Run Investigation","text":"<ol> <li> <p>Check run history: <pre><code>SELECT run_id, status, started_at, ended_at, \n       entities_processed, edges_created, lp_iterations\nFROM idr_out.run_history\nORDER BY started_at DESC\nLIMIT 10;\n</code></pre></p> </li> <li> <p>Check for partial outputs: <pre><code>SELECT COUNT(*) FROM idr_work.entities_delta;\nSELECT COUNT(*) FROM idr_work.edges_new;\nSELECT COUNT(*) FROM idr_work.lp_labels;\n</code></pre></p> </li> <li> <p>Review error logs (platform-specific)</p> </li> </ol>"},{"location":"runbook/#rollback-procedure","title":"Rollback Procedure","text":"<p>[!CAUTION] Rollback does not restore deleted edges. Consider before proceeding.</p> <ol> <li> <p>Restore membership from last known good state: <pre><code>-- If you have backups\nTRUNCATE TABLE idr_out.identity_resolved_membership_current;\nINSERT INTO idr_out.identity_resolved_membership_current\nSELECT * FROM idr_out.membership_backup_YYYYMMDD;\n</code></pre></p> </li> <li> <p>Reset run state: <pre><code>UPDATE idr_meta.run_state\nSET last_watermark_value = TIMESTAMP 'YYYY-MM-DD HH:MM:SS',\n    last_run_id = 'rollback_manual'\nWHERE table_id IN ('affected_tables');\n</code></pre></p> </li> <li> <p>Re-run with FULL mode after fixing root cause</p> </li> </ol>"},{"location":"runbook/#data-recovery","title":"Data Recovery","text":"<p>If output tables are corrupted:</p> <ol> <li> <p>Rebuild from scratch: <pre><code>-- 1. Truncate outputs\nTRUNCATE TABLE idr_out.identity_edges_current;\nTRUNCATE TABLE idr_out.identity_resolved_membership_current;\nTRUNCATE TABLE idr_out.identity_clusters_current;\nTRUNCATE TABLE idr_out.golden_profile_current;\n\n-- 2. Reset all watermarks\nUPDATE idr_meta.run_state SET last_watermark_value = TIMESTAMP '1900-01-01';\n\n-- 3. Run FULL refresh\n</code></pre></p> </li> <li> <p>Verify integrity: <pre><code>-- Check all entities have membership\nSELECT COUNT(*) as orphaned_entities\nFROM idr_work.entities_delta e\nWHERE NOT EXISTS (\n  SELECT 1 FROM idr_out.identity_resolved_membership_current m\n  WHERE m.entity_key = e.entity_key\n);\n</code></pre></p> </li> </ol>"},{"location":"runbook/#contact-escalation","title":"Contact &amp; Escalation","text":"<p>For issues not covered here: 1. Review architecture.md for design context 2. Check scale_considerations.md for performance 3. Review platform-specific testing guides: <code>testing_*.md</code></p>"},{"location":"scale_considerations/","title":"Scale Considerations","text":"<p>This document covers performance characteristics and optimization strategies for processing millions of rows across multiple sources.</p>"},{"location":"scale_considerations/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"scale_considerations/#complexity-analysis","title":"Complexity Analysis","text":"Component Complexity Notes Entity extraction O(N) Linear scan per source table Identifier extraction O(N \u00d7 M) N entities, M identifier types per entity Edge building O(N) Anchor-based, not O(N\u00b2) Label propagation O(E \u00d7 D) E edges, D graph diameter (iterations) Golden profile O(C \u00d7 A) C clusters, A attributes per cluster"},{"location":"scale_considerations/#expected-scale","title":"Expected Scale","text":"Metric 1M rows 10M rows 100M rows Entities delta (incremental) ~10K-100K ~100K-1M ~1M-10M Edges created per run ~5K-50K ~50K-500K ~500K-5M LP iterations 3-5 5-8 5-10 Typical run time 2-10 min 10-30 min 30-120 min <p>Note: Times assume Databricks with 8-16 workers. Actual performance depends on cluster size, data skew, and cluster connectivity.</p>"},{"location":"scale_considerations/#bottleneck-analysis","title":"Bottleneck Analysis","text":""},{"location":"scale_considerations/#1-large-identifier-groups","title":"1. Large Identifier Groups","text":"<p>Symptom: One identifier (e.g., shared email domain) matches millions of entities.</p> <p>Impact:  - Huge edge tables for that identifier - LP iterations slow to converge - Giant clusters (data quality issue)</p> <p>Mitigation: <pre><code>-- Add to rule table: max_group_size\n-- In edge building, skip groups larger than threshold\nWHERE group_size &lt;= COALESCE(r.max_group_size, 10000)\n</code></pre></p>"},{"location":"scale_considerations/#2-deeply-connected-graphs","title":"2. Deeply Connected Graphs","text":"<p>Symptom: LP takes 20+ iterations to converge.</p> <p>Impact: Each iteration creates/drops large tables.</p> <p>Mitigation: - Set reasonable <code>MAX_ITERS</code> (default: 30) - Investigate data quality\u2014deep chains often indicate bad identifiers - Consider breaking chains at low-confidence edges</p>"},{"location":"scale_considerations/#3-identifier-table-scans","title":"3. Identifier Table Scans","text":"<p>Symptom: <code>identifiers_all</code> table scan is slow.</p> <p>Impact: Edge building step dominates runtime.</p> <p>Mitigation: <pre><code>-- Partition by identifier_type\nALTER TABLE idr_work.identifiers_all \nCLUSTER BY (identifier_type, identifier_value_norm)\n\n-- Or use Delta Z-ORDER\nOPTIMIZE idr_work.identifiers_all \nZORDER BY (identifier_type, identifier_value_norm)\n</code></pre></p>"},{"location":"scale_considerations/#4-golden-profile-correlated-subqueries","title":"4. Golden Profile Correlated Subqueries","text":"<p>Symptom: Golden profile step is slow for large clusters.</p> <p>Impact: One subquery per attribute per resolved_id.</p> <p>Mitigation: Refactor to window functions: <pre><code>-- Instead of correlated subquery per attribute\nSELECT resolved_id,\n  FIRST_VALUE(email_raw) OVER (\n    PARTITION BY resolved_id \n    ORDER BY trust_rank, record_updated_at DESC\n  ) AS email_primary\nFROM ...\n</code></pre></p>"},{"location":"scale_considerations/#cluster-sizing-guidelines","title":"Cluster Sizing Guidelines","text":""},{"location":"scale_considerations/#databricks","title":"Databricks","text":"Data Volume Recommended Cluster &lt; 1M rows 4 workers, Standard_DS3_v2 1-10M rows 8 workers, Standard_DS4_v2 10-100M rows 16+ workers, Standard_DS5_v2 &gt; 100M rows 32+ workers, enable Photon"},{"location":"scale_considerations/#key-settings","title":"Key Settings","text":"<pre><code># Spark configs for large jobs\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # Increase for &gt; 10M rows\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")   # Auto-optimize\nspark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n</code></pre>"},{"location":"scale_considerations/#incremental-vs-full-processing","title":"Incremental vs Full Processing","text":""},{"location":"scale_considerations/#when-to-use-incremental-run_modeincr","title":"When to Use Incremental (<code>RUN_MODE=INCR</code>)","text":"<ul> <li>\u2705 Daily/hourly runs with delta changes</li> <li>\u2705 Source tables have reliable watermark columns</li> <li>\u2705 Processing window is &lt; 10% of total volume</li> </ul>"},{"location":"scale_considerations/#when-to-use-full-run_modefull","title":"When to Use Full (<code>RUN_MODE=FULL</code>)","text":"<ul> <li>\u2705 Initial load / first-time setup</li> <li>\u2705 After schema changes or rule updates</li> <li>\u2705 Monthly full refresh for data hygiene</li> <li>\u2705 When watermarks are unreliable</li> </ul>"},{"location":"scale_considerations/#hybrid-strategy","title":"Hybrid Strategy","text":"<pre><code>Weekly: RUN_MODE=INCR (process deltas)\nMonthly: RUN_MODE=FULL (full refresh, verify consistency)\n</code></pre>"},{"location":"scale_considerations/#partitioning-strategy","title":"Partitioning Strategy","text":""},{"location":"scale_considerations/#recommended-partitioning","title":"Recommended Partitioning","text":"Table Partition By Cluster By <code>identity_edges_current</code> None <code>(identifier_type, identifier_value_norm)</code> <code>identity_resolved_membership_current</code> None <code>(resolved_id)</code> <code>golden_profile_current</code> None <code>(resolved_id)</code>"},{"location":"scale_considerations/#for-very-large-deployments-100m","title":"For Very Large Deployments (&gt; 100M)","text":"<pre><code>-- Partition edges by month for easier maintenance\nCREATE TABLE idr_out.identity_edges_current (\n  ...\n) PARTITIONED BY (first_seen_month STRING)\n</code></pre>"},{"location":"scale_considerations/#monitoring-queries","title":"Monitoring Queries","text":""},{"location":"scale_considerations/#check-cluster-size-distribution","title":"Check Cluster Size Distribution","text":"<pre><code>SELECT \n  CASE \n    WHEN cluster_size = 1 THEN '1 (singletons)'\n    WHEN cluster_size &lt;= 5 THEN '2-5'\n    WHEN cluster_size &lt;= 20 THEN '6-20'\n    WHEN cluster_size &lt;= 100 THEN '21-100'\n    ELSE '100+'\n  END AS size_bucket,\n  COUNT(*) AS cluster_count,\n  SUM(cluster_size) AS total_entities\nFROM idr_out.identity_clusters_current\nGROUP BY 1\nORDER BY 1\n</code></pre>"},{"location":"scale_considerations/#identify-problematic-identifiers","title":"Identify Problematic Identifiers","text":"<pre><code>-- Find identifiers linking too many entities\nSELECT identifier_type, identifier_value_norm, COUNT(*) AS entity_count\nFROM idr_work.identifiers_all\nGROUP BY 1, 2\nHAVING COUNT(*) &gt; 1000\nORDER BY entity_count DESC\nLIMIT 20\n</code></pre>"},{"location":"scale_considerations/#track-run-performance","title":"Track Run Performance","text":"<pre><code>SELECT \n  run_id,\n  SUM(edges_created) AS total_edges,\n  MIN(started_at) AS started,\n  MAX(ended_at) AS ended,\n  TIMESTAMPDIFF(MINUTE, MIN(started_at), MAX(ended_at)) AS duration_min\nFROM idr_out.rule_match_audit_current\nGROUP BY run_id\nORDER BY started DESC\nLIMIT 10\n</code></pre>"},{"location":"scale_considerations/#common-scale-issues-solutions","title":"Common Scale Issues &amp; Solutions","text":"Issue Symptoms Solution Cluster timeout Job killed after 2+ hours Increase workers, check for data skew OOM errors Executor lost, memory exceeded Increase executor memory, reduce shuffle partitions Slow LP convergence 20+ iterations Check for giant clusters, add identifier quality rules Giant clusters 10K+ entities per cluster Likely bad identifier (e.g., NULL, default value) Edge explosion Billions of edges Missing <code>require_non_null</code>, bad canonicalization"},{"location":"scale_considerations/#recommended-testing-path","title":"Recommended Testing Path","text":"<ol> <li>Start small: 10K rows per source, verify correctness</li> <li>Scale to 100K: Check performance, identify bottlenecks</li> <li>Scale to 1M: Tune cluster sizing, validate incremental</li> <li>Production load: Full volume with monitoring</li> </ol>"},{"location":"scale_considerations/#further-reading","title":"Further Reading","text":"<ul> <li>Architecture \u2014 Core algorithms and data model</li> <li>Runbook \u2014 Operational guide</li> </ul>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>This document describes the system architecture of SQL Identity Resolution.</p>"},{"location":"concepts/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph Sources[\"Source Layer\"]\n        S1[CRM System]\n        S2[E-commerce]\n        S3[Mobile App]\n        S4[Support Tickets]\n    end\n\n    subgraph Meta[\"Metadata Layer\"]\n        M1[\"source_table&lt;br/&gt;(table registry)\"]\n        M2[\"rule&lt;br/&gt;(matching rules)\"]\n        M3[\"identifier_mapping&lt;br/&gt;(column mappings)\"]\n        M4[\"config&lt;br/&gt;(settings)\"]\n    end\n\n    subgraph Process[\"Processing Layer\"]\n        P1[\"Extract Entities&lt;br/&gt;(delta detection)\"]\n        P2[\"Build Edges&lt;br/&gt;(identifier matching)\"]\n        P3[\"Label Propagation&lt;br/&gt;(connected components)\"]\n        P4[\"Assign Clusters&lt;br/&gt;(membership update)\"]\n    end\n\n    subgraph Output[\"Output Layer\"]\n        O1[\"identity_resolved_membership_current\"]\n        O2[\"identity_clusters_current\"]\n        O3[\"golden_profile_current\"]\n        O4[\"metrics_export\"]\n        O5[\"run_history\"]\n    end\n\n    Sources --&gt; P1\n    Meta --&gt; P1\n    P1 --&gt; P2\n    P2 --&gt; P3\n    P3 --&gt; P4\n    P4 --&gt; Output</code></pre>"},{"location":"concepts/architecture/#cross-platform-design","title":"Cross-Platform Design","text":"<p>The same core logic runs on all platforms with platform-specific adapters:</p> <pre><code>graph LR\n    subgraph Core[\"Core SQL Logic\"]\n        A[DDL Schema]\n        B[Edge Building SQL]\n        C[Label Propagation SQL]\n        D[Output Generation SQL]\n    end\n\n    subgraph Adapters[\"Platform Adapters\"]\n        DA[\"DuckDB&lt;br/&gt;Python CLI\"]\n        SN[\"Snowflake&lt;br/&gt;Stored Procedure\"]\n        BQ[\"BigQuery&lt;br/&gt;Python CLI\"]\n        DB[\"Databricks&lt;br/&gt;Notebook\"]\n    end\n\n    Core --&gt; DA\n    Core --&gt; SN\n    Core --&gt; BQ\n    Core --&gt; DB</code></pre>"},{"location":"concepts/architecture/#data-flow","title":"Data Flow","text":""},{"location":"concepts/architecture/#1-input-processing","title":"1. Input Processing","text":"<pre><code>sequenceDiagram\n    participant Source as Source Table\n    participant Meta as Metadata\n    participant Work as Work Tables\n\n    Meta-&gt;&gt;Work: Read source_table config\n    Source-&gt;&gt;Work: Extract delta (watermark filter)\n    Work-&gt;&gt;Work: Generate entity_key\n    Work-&gt;&gt;Work: Extract identifiers\n    Note over Work: entities_delta table populated</code></pre>"},{"location":"concepts/architecture/#2-edge-building","title":"2. Edge Building","text":"<pre><code>sequenceDiagram\n    participant Entities as entities_delta\n    participant Rules as rule/mapping\n    participant Edges as edges_new\n\n    Entities-&gt;&gt;Rules: Apply identifier extraction\n    Rules-&gt;&gt;Edges: Match on canonicalized values\n    Note over Edges: Edge = (entity_a, entity_b, identifier_type)</code></pre>"},{"location":"concepts/architecture/#3-label-propagation","title":"3. Label Propagation","text":"<pre><code>sequenceDiagram\n    participant Edges as edges_new\n    participant LP as Label Propagation\n    participant Labels as lp_labels\n\n    Edges-&gt;&gt;LP: Initialize labels (label = entity_key)\n    loop Until convergence or max_iters\n        LP-&gt;&gt;LP: Propagate MIN label along edges\n    end\n    LP-&gt;&gt;Labels: Final cluster assignments</code></pre>"},{"location":"concepts/architecture/#4-output-generation","title":"4. Output Generation","text":"<pre><code>sequenceDiagram\n    participant Labels as lp_labels\n    participant Membership as membership_current\n    participant Clusters as clusters_current\n    participant Golden as golden_profile_current\n\n    Labels-&gt;&gt;Membership: Update resolved_id\n    Membership-&gt;&gt;Clusters: Compute cluster sizes\n    Membership-&gt;&gt;Golden: Build best-record profiles</code></pre>"},{"location":"concepts/architecture/#schema-design","title":"Schema Design","text":""},{"location":"concepts/architecture/#idr_meta-configuration","title":"idr_meta (Configuration)","text":"Table Purpose <code>source_table</code> Registry of source tables to process <code>rule</code> Matching rules with priority and limits <code>identifier_mapping</code> Maps source columns to identifier types <code>run_state</code> Watermark tracking per source <code>config</code> Key-value configuration settings <code>identifier_exclusion</code> Values to exclude from matching"},{"location":"concepts/architecture/#idr_work-processing","title":"idr_work (Processing)","text":"Table Purpose Lifecycle <code>entities_delta</code> Entities to process this run Per-run <code>identifiers</code> Extracted identifier values Per-run <code>edges_new</code> Entity pairs with matching identifiers Per-run <code>lp_labels</code> Label propagation results Per-run <code>membership_updates</code> Proposed membership changes Per-run"},{"location":"concepts/architecture/#idr_out-output","title":"idr_out (Output)","text":"Table Purpose <code>identity_resolved_membership_current</code> Entity \u2192 Cluster mapping <code>identity_clusters_current</code> Cluster metadata (size, updated_ts) <code>golden_profile_current</code> Best-record profiles per cluster <code>run_history</code> Audit log of all runs <code>stage_metrics</code> Per-stage timing metrics <code>metrics_export</code> Exportable metrics <code>dry_run_results</code> Per-entity dry run changes <code>dry_run_summary</code> Aggregate dry run statistics <code>skipped_identifier_groups</code> Audit of skipped groups"},{"location":"concepts/architecture/#processing-modes","title":"Processing Modes","text":""},{"location":"concepts/architecture/#full-mode","title":"Full Mode","text":"<p>Processes all entities from all source tables:</p> <pre><code>graph LR\n    A[All Source Records] --&gt; B[Full Entity Extraction]\n    B --&gt; C[Complete Edge Building]\n    C --&gt; D[Full Label Propagation]\n    D --&gt; E[Complete Membership Update]</code></pre>"},{"location":"concepts/architecture/#incremental-mode","title":"Incremental Mode","text":"<p>Processes only changed entities (watermark-based):</p> <pre><code>graph LR\n    A[Source Records] --&gt; B{updated_at &gt; last_watermark?}\n    B --&gt;|Yes| C[Delta Entity Extraction]\n    B --&gt;|No| D[Skip]\n    C --&gt; E[Incremental Edge Building]\n    E --&gt; F[Focused Label Propagation]\n    F --&gt; G[Delta Membership Update]</code></pre>"},{"location":"concepts/architecture/#dry-run-architecture","title":"Dry Run Architecture","text":"<pre><code>graph TB\n    subgraph Normal[\"Normal Run\"]\n        N1[Process Entities]\n        N2[Build Edges]\n        N3[Label Propagation]\n        N4[Update Membership]\n        N5[Update Clusters]\n        N6[Update Golden Profiles]\n        N7[Update Watermarks]\n    end\n\n    subgraph DryRun[\"Dry Run\"]\n        D1[Process Entities]\n        D2[Build Edges]\n        D3[Label Propagation]\n        D4[Compute Diff]\n        D5[Write dry_run_results]\n        D6[Write dry_run_summary]\n        D7[\"\u26d4 Skip Production Writes\"]\n    end</code></pre>"},{"location":"concepts/architecture/#metrics-export-architecture","title":"Metrics Export Architecture","text":"<pre><code>graph LR\n    subgraph Runner[\"IDR Runner\"]\n        R1[record_metric calls]\n    end\n\n    subgraph Storage[\"metrics_export table\"]\n        S1[(metric_name, value, dimensions)]\n    end\n\n    subgraph Exporter[\"metrics_exporter.py\"]\n        E1[DuckDB Adapter]\n        E2[Snowflake Adapter]\n        E3[BigQuery Adapter]\n    end\n\n    subgraph Destinations[\"Export Destinations\"]\n        D1[stdout]\n        D2[Webhook]\n        D3[Prometheus]\n        D4[DataDog]\n    end\n\n    Runner --&gt; Storage\n    Storage --&gt; Exporter\n    Exporter --&gt; Destinations</code></pre>"},{"location":"concepts/architecture/#security-model","title":"Security Model","text":"<pre><code>graph TB\n    subgraph Access[\"Access Control\"]\n        A1[Service Account / Role]\n    end\n\n    subgraph Permissions[\"Required Permissions\"]\n        P1[\"SELECT on source tables\"]\n        P2[\"ALL on idr_meta.*\"]\n        P3[\"ALL on idr_work.*\"]\n        P4[\"ALL on idr_out.*\"]\n    end\n\n    subgraph Data[\"Data Flow\"]\n        D1[\"Source Data&lt;br/&gt;(read-only)\"]\n        D2[\"Work Tables&lt;br/&gt;(transient)\"]\n        D3[\"Output Tables&lt;br/&gt;(persistent)\"]\n    end\n\n    Access --&gt; Permissions\n    Permissions --&gt; Data</code></pre>"},{"location":"concepts/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Matching Algorithm - Deep dive into label propagation</li> <li>Data Model - Complete schema reference</li> <li>Configuration - Setting up rules and sources</li> </ul>"},{"location":"concepts/data-model/","title":"Data Model","text":"<p>Complete reference for all tables in SQL Identity Resolution.</p>"},{"location":"concepts/data-model/#schema-overview","title":"Schema Overview","text":"<pre><code>erDiagram\n    source_table ||--o{ identifier_mapping : \"has\"\n    source_table ||--o{ run_state : \"tracks\"\n    rule ||--o{ identifier_mapping : \"applies_to\"\n\n    source_table {\n        string table_id PK\n        string table_fqn\n        string entity_type\n        string entity_key_expr\n        string watermark_column\n        int watermark_lookback_minutes\n        boolean is_active\n    }\n\n    rule {\n        string rule_id PK\n        string identifier_type\n        int priority\n        boolean is_active\n        int max_group_size\n    }\n\n    identifier_mapping {\n        string table_id FK\n        string identifier_type FK\n        string column_expr\n        boolean requires_normalization\n    }\n\n    run_state {\n        string table_id FK\n        timestamp last_watermark_value\n        string last_run_id\n        timestamp last_run_ts\n    }\n\n    identity_resolved_membership_current {\n        string entity_key PK\n        string resolved_id\n        timestamp updated_ts\n    }\n\n    identity_clusters_current {\n        string resolved_id PK\n        int cluster_size\n        timestamp updated_ts\n    }</code></pre>"},{"location":"concepts/data-model/#idr_meta-schema","title":"idr_meta Schema","text":""},{"location":"concepts/data-model/#source_table","title":"source_table","text":"<p>Registry of source tables to process.</p> Column Type Description <code>table_id</code> VARCHAR PK Unique identifier for this source <code>table_fqn</code> VARCHAR Fully qualified table name <code>entity_type</code> VARCHAR Entity type (e.g., PERSON, ACCOUNT) <code>entity_key_expr</code> VARCHAR SQL expression for entity key <code>watermark_column</code> VARCHAR Column for incremental processing <code>watermark_lookback_minutes</code> INT Lookback buffer for late-arriving data <code>is_active</code> BOOLEAN Include in processing <p>Example: <pre><code>INSERT INTO idr_meta.source_table VALUES\n  ('customers', 'crm.customers', 'PERSON', 'customer_id', 'updated_at', 0, TRUE),\n  ('orders', 'ecom.orders', 'PERSON', 'user_id', 'order_date', 60, TRUE);\n</code></pre></p>"},{"location":"concepts/data-model/#rule","title":"rule","text":"<p>Matching rules defining how identifiers are used.</p> Column Type Description <code>rule_id</code> VARCHAR PK Unique rule identifier <code>identifier_type</code> VARCHAR Type of identifier (EMAIL, PHONE, etc.) <code>priority</code> INT Processing priority (lower = higher priority) <code>is_active</code> BOOLEAN Include in processing <code>max_group_size</code> INT Max entities per identifier group <p>Example: <pre><code>INSERT INTO idr_meta.rule VALUES\n  ('email_exact', 'EMAIL', 1, TRUE, 10000),\n  ('phone_exact', 'PHONE', 2, TRUE, 5000),\n  ('loyalty_id', 'LOYALTY', 3, TRUE, 1);\n</code></pre></p>"},{"location":"concepts/data-model/#identifier_mapping","title":"identifier_mapping","text":"<p>Maps source columns to identifier types.</p> Column Type Description <code>table_id</code> VARCHAR FK \u2192 source_table <code>identifier_type</code> VARCHAR Type of identifier <code>column_expr</code> VARCHAR SQL expression to extract identifier <code>requires_normalization</code> BOOLEAN Apply normalization (lowercase, etc.) <p>Example: <pre><code>INSERT INTO idr_meta.identifier_mapping VALUES\n  ('customers', 'EMAIL', 'email', TRUE),\n  ('customers', 'PHONE', 'phone', TRUE),\n  ('orders', 'EMAIL', 'customer_email', TRUE);\n</code></pre></p>"},{"location":"concepts/data-model/#run_state","title":"run_state","text":"<p>Tracks watermarks for incremental processing.</p> Column Type Description <code>table_id</code> VARCHAR FK \u2192 source_table <code>last_watermark_value</code> TIMESTAMP Last processed watermark <code>last_run_id</code> VARCHAR ID of last run <code>last_run_ts</code> TIMESTAMP Timestamp of last run"},{"location":"concepts/data-model/#config","title":"config","text":"<p>Key-value configuration settings.</p> Column Type Description <code>config_key</code> VARCHAR PK Configuration key <code>config_value</code> VARCHAR Configuration value <code>description</code> VARCHAR Human-readable description <code>updated_at</code> TIMESTAMP Last update time <p>Default values: <pre><code>INSERT INTO idr_meta.config VALUES\n  ('dry_run_retention_days', '7', 'Days to retain dry run results', NOW()),\n  ('large_cluster_threshold', '5000', 'Threshold for large cluster warnings', NOW());\n</code></pre></p>"},{"location":"concepts/data-model/#identifier_exclusion","title":"identifier_exclusion","text":"<p>Values to exclude from matching.</p> Column Type Description <code>identifier_type</code> VARCHAR Type of identifier <code>identifier_pattern</code> VARCHAR Value or pattern to exclude <code>is_pattern</code> BOOLEAN TRUE if LIKE pattern <code>reason</code> VARCHAR Reason for exclusion <p>Example: <pre><code>INSERT INTO idr_meta.identifier_exclusion VALUES\n  ('EMAIL', 'test@test.com', FALSE, 'Generic test email'),\n  ('EMAIL', '%@example.com', TRUE, 'Example domain'),\n  ('PHONE', '0000000000', FALSE, 'Invalid phone');\n</code></pre></p>"},{"location":"concepts/data-model/#idr_out-schema","title":"idr_out Schema","text":""},{"location":"concepts/data-model/#identity_resolved_membership_current","title":"identity_resolved_membership_current","text":"<p>Maps each entity to its resolved identity.</p> Column Type Description <code>entity_key</code> VARCHAR PK Unique entity identifier <code>resolved_id</code> VARCHAR Cluster identifier (= entity_key of cluster anchor) <code>updated_ts</code> TIMESTAMP Last update time"},{"location":"concepts/data-model/#identity_clusters_current","title":"identity_clusters_current","text":"<p>Cluster metadata.</p> Column Type Description <code>resolved_id</code> VARCHAR PK Cluster identifier <code>cluster_size</code> INT Number of entities in cluster <code>updated_ts</code> TIMESTAMP Last update time"},{"location":"concepts/data-model/#golden_profile_current","title":"golden_profile_current","text":"<p>Best-record golden profiles per cluster.</p> Column Type Description <code>resolved_id</code> VARCHAR PK Cluster identifier <code>email_primary</code> VARCHAR Best email <code>phone_primary</code> VARCHAR Best phone <code>first_name</code> VARCHAR Best first name <code>last_name</code> VARCHAR Best last name <code>updated_ts</code> TIMESTAMP Last update time"},{"location":"concepts/data-model/#run_history","title":"run_history","text":"<p>Audit log of all runs.</p> Column Type Description <code>run_id</code> VARCHAR PK Unique run identifier <code>run_mode</code> VARCHAR FULL or INCR <code>started_at</code> TIMESTAMP Run start time <code>ended_at</code> TIMESTAMP Run end time <code>status</code> VARCHAR SUCCESS, SUCCESS_WITH_WARNINGS, FAILED, DRY_RUN_COMPLETE <code>entities_processed</code> INT Number of entities processed <code>edges_created</code> INT Number of edges created <code>clusters_impacted</code> INT Number of clusters affected <code>lp_iterations</code> INT Label propagation iterations <code>duration_seconds</code> INT Total duration <code>groups_skipped</code> INT Groups skipped (max_group_size) <code>values_excluded</code> INT Values excluded (exclusion list) <code>large_clusters</code> INT Clusters above threshold <code>warnings</code> VARCHAR JSON array of warnings"},{"location":"concepts/data-model/#dry_run_results","title":"dry_run_results","text":"<p>Per-entity changes from dry run.</p> Column Type Description <code>run_id</code> VARCHAR Dry run identifier <code>entity_key</code> VARCHAR Entity identifier <code>current_resolved_id</code> VARCHAR Current cluster (NULL if new) <code>proposed_resolved_id</code> VARCHAR Proposed cluster <code>change_type</code> VARCHAR NEW, MOVED, UNCHANGED <code>current_cluster_size</code> INT Current cluster size <code>proposed_cluster_size</code> INT Proposed cluster size <code>created_at</code> TIMESTAMP Record creation time"},{"location":"concepts/data-model/#dry_run_summary","title":"dry_run_summary","text":"<p>Aggregate dry run statistics.</p> Column Type Description <code>run_id</code> VARCHAR PK Dry run identifier <code>total_entities</code> INT Total entities analyzed <code>new_entities</code> INT Entities with NEW change type <code>moved_entities</code> INT Entities with MOVED change type <code>unchanged_entities</code> INT Entities with UNCHANGED change type <code>merged_clusters</code> INT Clusters that would merge <code>split_clusters</code> INT Clusters that would split <code>largest_proposed_cluster</code> INT Size of largest proposed cluster <code>edges_would_create</code> INT Edges that would be created <code>groups_would_skip</code> INT Groups that would be skipped <code>values_would_exclude</code> INT Values that would be excluded <code>execution_time_seconds</code> INT Processing time <code>created_at</code> TIMESTAMP Record creation time"},{"location":"concepts/data-model/#metrics_export","title":"metrics_export","text":"<p>Exportable metrics for monitoring.</p> Column Type Description <code>metric_id</code> VARCHAR PK Unique metric ID <code>run_id</code> VARCHAR Associated run ID <code>metric_name</code> VARCHAR Metric name <code>metric_value</code> DOUBLE Metric value <code>metric_type</code> VARCHAR gauge, counter <code>dimensions</code> VARCHAR JSON dimensions <code>recorded_at</code> TIMESTAMP Recording time <code>exported_at</code> TIMESTAMP Export time (NULL if not exported)"},{"location":"concepts/data-model/#skipped_identifier_groups","title":"skipped_identifier_groups","text":"<p>Audit log of skipped identifier groups.</p> Column Type Description <code>run_id</code> VARCHAR Run identifier <code>identifier_type</code> VARCHAR Type of identifier <code>identifier_value_norm</code> VARCHAR Normalized identifier value <code>group_size</code> INT Size of skipped group <code>max_allowed</code> INT Configured max_group_size <code>sample_entity_keys</code> VARCHAR Sample of affected entity keys <code>reason</code> VARCHAR Reason for skip <code>skipped_at</code> TIMESTAMP Skip time"},{"location":"concepts/data-model/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>Architecture</li> <li>Schema Reference - Full DDL</li> </ul>"},{"location":"concepts/how-it-works-simple/","title":"How It Works (Simple Version)","text":"<p>Imagine you have a giant box of business cards on the floor. This is your data. Some cards are for \"John Smith\", some for \"J. Smith\", and some for \"Johnny S.\". Your goal is to staple all the cards that belong to the same person together.</p> <p>This tool does exactly that, in 3 simple steps.</p>"},{"location":"concepts/how-it-works-simple/#step-1-connecting-the-dots","title":"Step 1: Connecting the Dots","text":"<p>First, the tool looks at every card and finds things that are unique, like an Email Address or a Phone Number.</p> <ul> <li>Card A says: <code>john@gmail.com</code></li> <li>Card B says: <code>john@gmail.com</code></li> </ul> <p>Aha! Since they have the same email, they must belong to the same person. The tool draws a line connecting Card A and Card B.</p>"},{"location":"concepts/how-it-works-simple/#step-2-friends-of-friends-the-rounds","title":"Step 2: Friends of Friends (The \"Rounds\")","text":"<p>This is the magic part. The tool doesn't just see who is holding hands. It plays a game like \"Telephone\" or \"Pass the Note\".</p> <p>Round 1: *   John knows Jane (they share an Email). *   Jane knows Bob (they share a Phone).</p> <p>Round 2: *   The tool tells John: \"Hey, did you know Jane connects to Bob?\" *   Now John is connected to Bob too!</p> <p>The tool keeps playing these \"Rounds\" (we call them Iterations) until everyone in the whole group knows everyone else. Usually, it takes about 5-10 rounds to find every single connection in a huge web of people.</p>"},{"location":"concepts/how-it-works-simple/#step-3-giving-everyone-a-name-tag","title":"Step 3: Giving Everyone a Name Tag","text":"<p>Once the tool finds a whole group of cards connected together, it gives them a single Group ID.</p> <ul> <li>Card A -&gt; Group #123</li> <li>Card B -&gt; Group #123</li> <li>Card C -&gt; Group #123</li> </ul> <p>Now, whenever you ask \"Who is Group #123?\", the tool shows you a clean profile with the best name (\"John Smith\") and the best email (\"john@gmail.com\") from all the cards combined.</p>"},{"location":"concepts/how-it-works-simple/#why-is-this-hard","title":"Why is this hard?","text":"<p>Sometimes, things get messy. *   The Shared Phone: What if 50 people share the same office phone number? You don't want to staple 50 strangers together!     *   Solution: We tell the tool to ignore \"Office Numbers\" or \"Generic Emails\" (like <code>info@company.com</code>). *   The Typo: What if one card says <code>jon@gmail.com</code> and another says <code>john@gmail.com</code>?     *   Solution: We can tell the tool to fix common mistakes before it starts connecting dots.</p>"},{"location":"concepts/how-it-works-simple/#summary","title":"Summary","text":"<ol> <li>Find Matches: Look for shared emails or phones.</li> <li>Connect the Dots: Link friends of friends.</li> <li>Group Them: Give the whole bunch a single ID.</li> </ol>"},{"location":"concepts/matching-algorithm/","title":"Matching Algorithm","text":"<p>This document explains the deterministic matching algorithm used in SQL Identity Resolution.</p>"},{"location":"concepts/matching-algorithm/#overview","title":"Overview","text":"<p>The algorithm uses Label Propagation on a graph of entities connected by shared identifiers. This is a form of connected components detection that assigns all entities in a connected subgraph to the same cluster.</p>"},{"location":"concepts/matching-algorithm/#algorithm-steps","title":"Algorithm Steps","text":""},{"location":"concepts/matching-algorithm/#step-1-identifier-extraction","title":"Step 1: Identifier Extraction","text":"<p>Extract identifiers from source entities based on mapping rules:</p> <pre><code>graph LR\n    subgraph Source[\"Source Record\"]\n        A[\"customer_id: 123&lt;br/&gt;email: john@acme.com&lt;br/&gt;phone: 555-0123\"]\n    end\n\n    subgraph Identifiers[\"Extracted Identifiers\"]\n        I1[\"EMAIL: john@acme.com\"]\n        I2[\"PHONE: 5550123\"]\n    end\n\n    Source --&gt; Identifiers</code></pre> <p>Normalization rules: - Email: lowercase, trim whitespace - Phone: digits only - Custom: configurable per identifier type</p>"},{"location":"concepts/matching-algorithm/#step-2-edge-building","title":"Step 2: Edge Building","text":"<p>Create edges between entities that share an identifier:</p> <pre><code>graph LR\n    subgraph Entities\n        E1[\"Entity A&lt;br/&gt;email: john@acme.com\"]\n        E2[\"Entity B&lt;br/&gt;email: john@acme.com\"]\n        E3[\"Entity C&lt;br/&gt;phone: 555-0123\"]\n    end\n\n    subgraph Edges\n        EG1[\"(A, B, EMAIL)\"]\n        EG2[\"(B, C, PHONE)\"]\n    end\n\n    E1 &amp; E2 --&gt; EG1\n    E2 &amp; E3 --&gt; EG2</code></pre> <p>SQL Logic: <pre><code>-- Create edges from shared identifiers\nINSERT INTO idr_work.edges_new\nSELECT \n    a.entity_key AS entity_a,\n    b.entity_key AS entity_b,\n    a.identifier_type\nFROM idr_work.identifiers a\nJOIN idr_work.identifiers b \n    ON a.identifier_type = b.identifier_type\n    AND a.identifier_value_norm = b.identifier_value_norm\n    AND a.entity_key &lt; b.entity_key  -- Avoid duplicates\n</code></pre></p>"},{"location":"concepts/matching-algorithm/#step-3-label-propagation","title":"Step 3: Label Propagation","text":"<p>Iteratively propagate the minimum label along edges until convergence:</p> <pre><code>graph TB\n    subgraph Iteration0[\"Iteration 0 (Initial)\"]\n        A0[\"A: label=A\"]\n        B0[\"B: label=B\"]\n        C0[\"C: label=C\"]\n    end\n\n    subgraph Iteration1[\"Iteration 1\"]\n        A1[\"A: label=A\"]\n        B1[\"B: label=A (min of A,B)\"]\n        C1[\"C: label=B (min of B,C)\"]\n    end\n\n    subgraph Iteration2[\"Iteration 2\"]\n        A2[\"A: label=A\"]\n        B2[\"B: label=A\"]\n        C2[\"C: label=A (min of A,B)\"]\n    end\n\n    subgraph Final[\"Converged\"]\n        AF[\"A: label=A \u2713\"]\n        BF[\"B: label=A \u2713\"]\n        CF[\"C: label=A \u2713\"]\n    end\n\n    Iteration0 --&gt; Iteration1\n    Iteration1 --&gt; Iteration2\n    Iteration2 --&gt; Final</code></pre> <p>SQL Logic (one iteration): <pre><code>-- Propagate minimum label along edges\nUPDATE idr_work.lp_labels curr\nSET label = (\n    SELECT MIN(neighbor.label)\n    FROM idr_work.edges_new e\n    JOIN idr_work.lp_labels neighbor\n        ON neighbor.entity_key = CASE\n            WHEN e.entity_a = curr.entity_key THEN e.entity_b\n            ELSE e.entity_a\n        END\n    WHERE e.entity_a = curr.entity_key OR e.entity_b = curr.entity_key\n)\nWHERE EXISTS (\n    SELECT 1 FROM idr_work.edges_new e\n    WHERE e.entity_a = curr.entity_key OR e.entity_b = curr.entity_key\n);\n</code></pre></p>"},{"location":"concepts/matching-algorithm/#step-4-cluster-assignment","title":"Step 4: Cluster Assignment","text":"<p>The final label becomes the <code>resolved_id</code>:</p> entity_key label (resolved_id) customer:A customer:A customer:B customer:A customer:C customer:A customer:D customer:D"},{"location":"concepts/matching-algorithm/#handling-edge-cases","title":"Handling Edge Cases","text":""},{"location":"concepts/matching-algorithm/#singletons","title":"Singletons","text":"<p>Entities with no matching identifiers get <code>resolved_id = entity_key</code>:</p> <pre><code>-- Singletons: entities with no edges\nINSERT INTO idr_work.membership_updates\nSELECT entity_key, entity_key AS resolved_id\nFROM idr_work.entities_delta\nWHERE entity_key NOT IN (\n    SELECT entity_key FROM idr_work.lp_labels\n);\n</code></pre>"},{"location":"concepts/matching-algorithm/#large-groups-max_group_size","title":"Large Groups (max_group_size)","text":"<p>To prevent generic identifiers (e.g., <code>test@test.com</code>) from creating mega-clusters:</p> <pre><code>graph TB\n    subgraph Normal[\"Normal Group (size=5)\"]\n        N1[Entity 1]\n        N2[Entity 2]\n        N3[Entity 3]\n        N4[Entity 4]\n        N5[Entity 5]\n    end\n\n    subgraph Large[\"Large Group (size=15000)\"]\n        L1[Entity 1]\n        L2[\"...\"]\n        L3[Entity 15000]\n    end\n\n    subgraph Result[\"After max_group_size=10000\"]\n        R1[\"Normal: Connected \u2713\"]\n        R2[\"Large: Skipped \u26a0\ufe0f&lt;br/&gt;(logged to skipped_identifier_groups)\"]\n    end\n\n    Normal --&gt; R1\n    Large --&gt; R2</code></pre> <p>Configuration: <pre><code>INSERT INTO idr_meta.rule (rule_id, identifier_type, max_group_size)\nVALUES ('email_exact', 'EMAIL', 10000);\n</code></pre></p>"},{"location":"concepts/matching-algorithm/#identifier-exclusions","title":"Identifier Exclusions","text":"<p>Skip known bad identifiers:</p> <pre><code>INSERT INTO idr_meta.identifier_exclusion\nVALUES ('EMAIL', 'test@test.com', FALSE, 'Generic test email');\n\nINSERT INTO idr_meta.identifier_exclusion\nVALUES ('EMAIL', '%@example.com', TRUE, 'Example domain pattern');\n</code></pre>"},{"location":"concepts/matching-algorithm/#convergence","title":"Convergence","text":"<p>The algorithm converges when no labels change between iterations:</p> <pre><code>graph LR\n    A[Iteration N] --&gt; B{Labels Changed?}\n    B --&gt;|Yes| C[Iteration N+1]\n    B --&gt;|No| D[Converged]\n    C --&gt; A\n\n    E[Max Iterations] --&gt; F{Reached Limit?}\n    F --&gt;|Yes| D</code></pre> <p>Typical convergence: - Small graphs: 2-5 iterations - Medium graphs: 5-10 iterations - Large graphs: 10-20 iterations - Max default: 30 iterations</p>"},{"location":"concepts/matching-algorithm/#complexity-analysis","title":"Complexity Analysis","text":"Step Time Complexity Space Complexity Identifier Extraction O(n) O(n \u00d7 m) Edge Building O(n \u00d7 m) O(e) Label Propagation O(i \u00d7 e) O(n) Cluster Assignment O(n) O(n) <p>Where: - n = number of entities - m = avg identifiers per entity - e = number of edges - i = iterations until convergence</p>"},{"location":"concepts/matching-algorithm/#comparison-with-other-algorithms","title":"Comparison with Other Algorithms","text":"Algorithm Pros Cons Label Propagation (this) Simple, deterministic, SQL-native May not handle probabilistic matches Union-Find Faster for sparse graphs Harder to implement in SQL ML-based (Dedupe) Handles fuzzy matches Requires training, black box Fellegi-Sunter Statistical rigor Complex, requires tuning"},{"location":"concepts/matching-algorithm/#next-steps","title":"Next Steps","text":"<ul> <li>Data Model - Schema reference</li> <li>Configuration - Setting up rules</li> <li>Production Hardening - max_group_size, exclusions</li> </ul>"},{"location":"deployment/ci-cd/","title":"CI/CD","text":"<p>Continuous integration and deployment for SQL Identity Resolution.</p>"},{"location":"deployment/ci-cd/#github-actions","title":"GitHub Actions","text":""},{"location":"deployment/ci-cd/#test-workflow","title":"Test Workflow","text":"<pre><code># .github/workflows/test.yml\nname: Test\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test-duckdb:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install duckdb pytest\n\n      - name: Run DuckDB tests\n        run: |\n          python tests/run_tests_duckdb.py\n\n  lint:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install linters\n        run: |\n          pip install flake8 black\n\n      - name: Lint with flake8\n        run: |\n          flake8 sql/ tools/ tests/ --max-line-length=120\n\n      - name: Check formatting with black\n        run: |\n          black --check sql/ tools/ tests/\n</code></pre>"},{"location":"deployment/ci-cd/#snowflake-test-workflow","title":"Snowflake Test Workflow","text":"<pre><code># .github/workflows/test-snowflake.yml\nname: Test Snowflake\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly\n\njobs:\n  test-snowflake:\n    runs-on: ubuntu-latest\n    environment: snowflake-test\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install snowflake-connector-python pytest\n\n      - name: Run Snowflake tests\n        env:\n          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}\n          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}\n          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n          SNOWFLAKE_DATABASE: IDR_TEST\n        run: |\n          python tests/snowflake/test_integration.py\n</code></pre>"},{"location":"deployment/ci-cd/#bigquery-test-workflow","title":"BigQuery Test Workflow","text":"<pre><code># .github/workflows/test-bigquery.yml\nname: Test BigQuery\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly\n\njobs:\n  test-bigquery:\n    runs-on: ubuntu-latest\n    environment: gcp-test\n\n    permissions:\n      contents: read\n      id-token: write\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Authenticate to Google Cloud\n        uses: google-github-actions/auth@v2\n        with:\n          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}\n          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT }}\n\n      - name: Set up Cloud SDK\n        uses: google-github-actions/setup-gcloud@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install google-cloud-bigquery pytest\n\n      - name: Run BigQuery tests\n        env:\n          GCP_PROJECT: ${{ secrets.GCP_PROJECT }}\n        run: |\n          python tests/bigquery/test_integration.py\n</code></pre>"},{"location":"deployment/ci-cd/#documentation-deployment","title":"Documentation Deployment","text":"<pre><code># .github/workflows/docs.yml\nname: Deploy Docs\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'docs/**'\n      - 'mkdocs.yml'\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install mkdocs-material mkdocs-mermaid2-plugin\n\n      - name: Deploy to GitHub Pages\n        run: |\n          mkdocs gh-deploy --force\n</code></pre>"},{"location":"deployment/ci-cd/#release-workflow","title":"Release Workflow","text":"<pre><code># .github/workflows/release.yml\nname: Release\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Create Release\n        uses: softprops/action-gh-release@v1\n        with:\n          generate_release_notes: true\n          files: |\n            sql/*/00_ddl_all.sql\n</code></pre>"},{"location":"deployment/ci-cd/#ddl-validation","title":"DDL Validation","text":"<p>Validate DDL syntax before merging:</p> <pre><code># .github/workflows/validate-ddl.yml\nname: Validate DDL\n\non:\n  pull_request:\n    paths:\n      - 'sql/**/*.sql'\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install DuckDB\n        run: pip install duckdb\n\n      - name: Validate DuckDB DDL\n        run: |\n          python -c \"\n          import duckdb\n          conn = duckdb.connect(':memory:')\n          with open('sql/duckdb/00_ddl_all.sql') as f:\n              conn.execute(f.read())\n          print('DuckDB DDL valid')\n          \"\n</code></pre>"},{"location":"deployment/ci-cd/#branch-protection","title":"Branch Protection","text":""},{"location":"deployment/ci-cd/#recommended-settings","title":"Recommended Settings","text":"<pre><code># Branch protection rules for 'main'\n\nrequired_status_checks:\n  strict: true\n  checks:\n    - test-duckdb\n    - lint\n\nrequired_pull_request_reviews:\n  required_approving_review_count: 1\n  dismiss_stale_reviews: true\n  require_code_owner_reviews: true\n\nrestrictions:\n  # Restrict who can push to main\n  users: []\n  teams: [maintainers]\n</code></pre>"},{"location":"deployment/ci-cd/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n        language_version: python3.11\n        files: \\.(py)$\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 7.0.0\n    hooks:\n      - id: flake8\n        args: [--max-line-length=120]\n\n  - repo: local\n    hooks:\n      - id: validate-ddl\n        name: Validate DuckDB DDL\n        entry: python -c \"import duckdb; duckdb.connect(':memory:').execute(open('sql/duckdb/00_ddl_all.sql').read())\"\n        language: python\n        files: sql/duckdb/.*\\.sql$\n        additional_dependencies: [duckdb]\n</code></pre> <p>Install: <pre><code>pip install pre-commit\npre-commit install\n</code></pre></p>"},{"location":"deployment/ci-cd/#secrets-management","title":"Secrets Management","text":""},{"location":"deployment/ci-cd/#required-secrets","title":"Required Secrets","text":"Secret Platform Description <code>SNOWFLAKE_ACCOUNT</code> Snowflake Account identifier <code>SNOWFLAKE_USER</code> Snowflake Username <code>SNOWFLAKE_PASSWORD</code> Snowflake Password <code>GCP_PROJECT</code> BigQuery GCP project ID <code>WIF_PROVIDER</code> BigQuery Workload Identity Federation provider <code>GCP_SERVICE_ACCOUNT</code> BigQuery Service account email <code>DATABRICKS_HOST</code> Databricks Workspace URL <code>DATABRICKS_TOKEN</code> Databricks Personal access token"},{"location":"deployment/ci-cd/#github-environments","title":"GitHub Environments","text":"<p>Use separate environments for secrets:</p> <ul> <li><code>snowflake-test</code></li> <li><code>gcp-test</code></li> <li><code>databricks-test</code></li> </ul>"},{"location":"deployment/ci-cd/#next-steps","title":"Next Steps","text":"<ul> <li>Security</li> <li>Scheduling</li> <li>Troubleshooting</li> </ul>"},{"location":"deployment/scheduling/","title":"Scheduling","text":"<p>Set up automated, recurring IDR runs on each platform.</p>"},{"location":"deployment/scheduling/#platform-options","title":"Platform Options","text":"Platform Scheduler Recommended DuckDB Cron, Airflow, Prefect Airflow for production Snowflake Snowflake Tasks \u2705 Native BigQuery Cloud Scheduler + Functions \u2705 Native Databricks Workflows/Jobs \u2705 Native"},{"location":"deployment/scheduling/#duckdb-scheduling","title":"DuckDB Scheduling","text":""},{"location":"deployment/scheduling/#cron","title":"Cron","text":"<p>Simple scheduling for standalone deployments:</p> <pre><code># Edit crontab\ncrontab -e\n\n# Run every hour\n0 * * * * cd /path/to/repo &amp;&amp; python sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=INCR &gt;&gt; /var/log/idr.log 2&gt;&amp;1\n\n# Run every 15 minutes\n*/15 * * * * cd /path/to/repo &amp;&amp; python sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=INCR &gt;&gt; /var/log/idr.log 2&gt;&amp;1\n</code></pre>"},{"location":"deployment/scheduling/#airflow-dag","title":"Airflow DAG","text":"<pre><code># dags/idr_dag.py\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'email_on_failure': True,\n    'email': ['alerts@company.com'],\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\nwith DAG(\n    'idr_hourly',\n    default_args=default_args,\n    description='Hourly Identity Resolution',\n    schedule_interval='0 * * * *',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    max_active_runs=1,\n) as dag:\n\n    dry_run = BashOperator(\n        task_id='dry_run',\n        bash_command='cd /opt/idr &amp;&amp; python sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=INCR --dry-run',\n    )\n\n    check_dry_run = BashOperator(\n        task_id='check_dry_run',\n        bash_command='''\n            python -c \"\n            import duckdb\n            conn = duckdb.connect('/opt/idr/idr.duckdb')\n            result = conn.execute('SELECT status FROM idr_out.run_history ORDER BY started_at DESC LIMIT 1').fetchone()\n            if result[0] != 'DRY_RUN_COMPLETE':\n                raise Exception('Dry run failed')\n            \"\n        ''',\n    )\n\n    live_run = BashOperator(\n        task_id='live_run',\n        bash_command='cd /opt/idr &amp;&amp; python sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=INCR',\n    )\n\n    dry_run &gt;&gt; check_dry_run &gt;&gt; live_run\n</code></pre>"},{"location":"deployment/scheduling/#snowflake-tasks","title":"Snowflake Tasks","text":""},{"location":"deployment/scheduling/#create-task","title":"Create Task","text":"<pre><code>-- Create a scheduled task\nCREATE OR REPLACE TASK idr_hourly_task\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = '60 MINUTE'\nAS\n  CALL idr_run('INCR', 30, FALSE);\n\n-- Enable the task\nALTER TASK idr_hourly_task RESUME;\n</code></pre>"},{"location":"deployment/scheduling/#monitor-tasks","title":"Monitor Tasks","text":"<pre><code>-- Check task history\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())\nWHERE NAME = 'IDR_HOURLY_TASK'\nORDER BY SCHEDULED_TIME DESC\nLIMIT 20;\n\n-- Check task status\nSHOW TASKS LIKE 'IDR%';\n</code></pre>"},{"location":"deployment/scheduling/#task-with-dry-run-validation","title":"Task with Dry Run Validation","text":"<pre><code>-- Two-step task with dry run\nCREATE OR REPLACE TASK idr_dry_run_task\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = '60 MINUTE'\nAS\n  CALL idr_run('INCR', 30, TRUE);\n\nCREATE OR REPLACE TASK idr_live_run_task\n  WAREHOUSE = COMPUTE_WH\n  AFTER idr_dry_run_task\n  WHEN (\n    SYSTEM$STREAM_HAS_DATA('idr_dry_run_check') = FALSE\n    AND (SELECT MAX(status) FROM idr_out.run_history WHERE started_at &gt;= DATEADD('hour', -1, CURRENT_TIMESTAMP)) = 'DRY_RUN_COMPLETE'\n  )\nAS\n  CALL idr_run('INCR', 30, FALSE);\n</code></pre>"},{"location":"deployment/scheduling/#bigquery-scheduling","title":"BigQuery Scheduling","text":""},{"location":"deployment/scheduling/#cloud-functions","title":"Cloud Functions","text":"<pre><code># main.py\nimport subprocess\nimport functions_framework\n\n@functions_framework.http\ndef run_idr(request):\n    project = 'your-project'\n    result = subprocess.run([\n        'python', '/app/idr_run.py',\n        f'--project={project}',\n        '--run-mode=INCR'\n    ], capture_output=True, text=True)\n\n    return {\n        'stdout': result.stdout,\n        'stderr': result.stderr,\n        'returncode': result.returncode\n    }\n</code></pre>"},{"location":"deployment/scheduling/#deploy-function","title":"Deploy Function","text":"<pre><code>gcloud functions deploy idr-runner \\\n  --gen2 \\\n  --runtime=python311 \\\n  --region=us-central1 \\\n  --source=./sql/bigquery \\\n  --entry-point=run_idr \\\n  --trigger-http \\\n  --timeout=540s \\\n  --memory=2048MB\n</code></pre>"},{"location":"deployment/scheduling/#cloud-scheduler","title":"Cloud Scheduler","text":"<pre><code># Create job\ngcloud scheduler jobs create http idr-hourly \\\n  --location=us-central1 \\\n  --schedule=\"0 * * * *\" \\\n  --uri=\"https://us-central1-your-project.cloudfunctions.net/idr-runner\" \\\n  --http-method=GET \\\n  --oidc-service-account-email=idr-runner@your-project.iam.gserviceaccount.com\n\n# View job\ngcloud scheduler jobs describe idr-hourly --location=us-central1\n\n# Trigger manually\ngcloud scheduler jobs run idr-hourly --location=us-central1\n</code></pre>"},{"location":"deployment/scheduling/#terraform","title":"Terraform","text":"<pre><code># scheduler.tf\nresource \"google_cloud_scheduler_job\" \"idr_hourly\" {\n  name      = \"idr-hourly\"\n  schedule  = \"0 * * * *\"\n  time_zone = \"UTC\"\n\n  http_target {\n    uri         = google_cloudfunctions2_function.idr_runner.service_config[0].uri\n    http_method = \"GET\"\n\n    oidc_token {\n      service_account_email = google_service_account.idr_runner.email\n    }\n  }\n}\n</code></pre>"},{"location":"deployment/scheduling/#databricks-workflows","title":"Databricks Workflows","text":""},{"location":"deployment/scheduling/#create-job-ui","title":"Create Job (UI)","text":"<ol> <li>Go to Workflows \u2192 Create Job</li> <li>Add task:</li> <li>Type: Notebook</li> <li>Path: <code>/Repos/your-org/sql-identity-resolution/sql/databricks/notebooks/IDR_Run</code></li> <li>Set parameters:    <pre><code>{\"RUN_MODE\": \"INCR\", \"DRY_RUN\": \"false\"}\n</code></pre></li> <li>Configure schedule</li> </ol>"},{"location":"deployment/scheduling/#create-job-api","title":"Create Job (API)","text":"<pre><code>{\n  \"name\": \"IDR Hourly\",\n  \"tasks\": [\n    {\n      \"task_key\": \"idr_run\",\n      \"notebook_task\": {\n        \"notebook_path\": \"/Repos/your-org/sql-identity-resolution/sql/databricks/notebooks/IDR_Run\",\n        \"base_parameters\": {\n          \"RUN_MODE\": \"INCR\",\n          \"DRY_RUN\": \"false\",\n          \"MAX_ITERS\": \"30\"\n        }\n      },\n      \"existing_cluster_id\": \"your-cluster-id\"\n    }\n  ],\n  \"schedule\": {\n    \"quartz_cron_expression\": \"0 0 * * * ?\",\n    \"timezone_id\": \"UTC\"\n  },\n  \"email_notifications\": {\n    \"on_failure\": [\"alerts@company.com\"]\n  },\n  \"max_concurrent_runs\": 1\n}\n</code></pre>"},{"location":"deployment/scheduling/#terraform_1","title":"Terraform","text":"<pre><code>resource \"databricks_job\" \"idr_hourly\" {\n  name = \"IDR Hourly\"\n\n  task {\n    task_key = \"idr_run\"\n\n    notebook_task {\n      notebook_path = \"/Repos/your-org/sql-identity-resolution/sql/databricks/notebooks/IDR_Run\"\n      base_parameters = {\n        RUN_MODE = \"INCR\"\n        DRY_RUN  = \"false\"\n      }\n    }\n\n    existing_cluster_id = var.cluster_id\n  }\n\n  schedule {\n    quartz_cron_expression = \"0 0 * * * ?\"\n    timezone_id            = \"UTC\"\n  }\n\n  email_notifications {\n    on_failure = [\"alerts@company.com\"]\n  }\n\n  max_concurrent_runs = 1\n}\n</code></pre>"},{"location":"deployment/scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Run incrementally: After initial FULL run, use INCR</li> <li>Stagger runs: Avoid running at exact hour marks</li> <li>Monitor: Set up alerts for failures</li> <li>Dry run first: For major config changes, dry run before live</li> <li>Concurrency: Prevent overlapping runs (<code>max_concurrent_runs=1</code>)</li> </ol>"},{"location":"deployment/scheduling/#next-steps","title":"Next Steps","text":"<ul> <li>CI/CD</li> <li>Security</li> <li>Metrics &amp; Monitoring</li> </ul>"},{"location":"deployment/security/","title":"Security","text":"<p>Security best practices for SQL Identity Resolution deployments.</p>"},{"location":"deployment/security/#principle-of-least-privilege","title":"Principle of Least Privilege","text":"<p>Grant only the minimum permissions needed for the IDR runner.</p>"},{"location":"deployment/security/#duckdb","title":"DuckDB","text":"<p>DuckDB runs locally with file permissions:</p> <pre><code># Restrict file access\nchmod 600 idr.duckdb\n\n# Run with non-root user\nuseradd -r -s /bin/false idr-runner\nchown idr-runner:idr-runner idr.duckdb\nsu - idr-runner -c \"python sql/duckdb/idr_run.py --db=idr.duckdb\"\n</code></pre>"},{"location":"deployment/security/#snowflake","title":"Snowflake","text":"<pre><code>-- Create dedicated role\nCREATE ROLE IDR_EXECUTOR;\n\n-- Grant only required permissions\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE IDR_EXECUTOR;\nGRANT USAGE ON DATABASE analytics TO ROLE IDR_EXECUTOR;\n\n-- Read-only on source schemas\nGRANT USAGE ON SCHEMA crm TO ROLE IDR_EXECUTOR;\nGRANT SELECT ON ALL TABLES IN SCHEMA crm TO ROLE IDR_EXECUTOR;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA crm TO ROLE IDR_EXECUTOR;\n\n-- Full access on IDR schemas\nGRANT ALL ON SCHEMA idr_meta TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_work TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_out TO ROLE IDR_EXECUTOR;\nGRANT ALL ON ALL TABLES IN SCHEMA idr_meta TO ROLE IDR_EXECUTOR;\nGRANT ALL ON ALL TABLES IN SCHEMA idr_work TO ROLE IDR_EXECUTOR;\nGRANT ALL ON ALL TABLES IN SCHEMA idr_out TO ROLE IDR_EXECUTOR;\n\n-- Create service user\nCREATE USER idr_service_account\n  PASSWORD = 'CHANGE_ME'\n  DEFAULT_ROLE = IDR_EXECUTOR\n  DEFAULT_WAREHOUSE = compute_wh;\nGRANT ROLE IDR_EXECUTOR TO USER idr_service_account;\n</code></pre>"},{"location":"deployment/security/#bigquery","title":"BigQuery","text":"<pre><code># Create service account\ngcloud iam service-accounts create idr-runner \\\n  --display-name=\"IDR Runner Service Account\"\n\n# Grant minimal permissions\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n  --member=\"serviceAccount:idr-runner@PROJECT_ID.iam.gserviceaccount.com\" \\\n  --role=\"roles/bigquery.jobUser\"\n\n# Grant dataset-level access\nbq query --use_legacy_sql=false \"\n  GRANT \\`roles/bigquery.dataViewer\\` ON SCHEMA crm \n  TO 'serviceAccount:idr-runner@PROJECT_ID.iam.gserviceaccount.com'\n\"\n\nbq query --use_legacy_sql=false \"\n  GRANT \\`roles/bigquery.dataEditor\\` ON SCHEMA idr_meta \n  TO 'serviceAccount:idr-runner@PROJECT_ID.iam.gserviceaccount.com'\n\"\n\nbq query --use_legacy_sql=false \"\n  GRANT \\`roles/bigquery.dataEditor\\` ON SCHEMA idr_out \n  TO 'serviceAccount:idr-runner@PROJECT_ID.iam.gserviceaccount.com'\n\"\n</code></pre>"},{"location":"deployment/security/#databricks","title":"Databricks","text":"<pre><code># Unity Catalog permissions\nspark.sql(\"\"\"\n  GRANT USAGE ON CATALOG main TO `idr-runner-group`\n\"\"\")\n\nspark.sql(\"\"\"\n  GRANT SELECT ON SCHEMA main.crm TO `idr-runner-group`\n\"\"\")\n\nspark.sql(\"\"\"\n  GRANT ALL PRIVILEGES ON SCHEMA main.idr_meta TO `idr-runner-group`\n\"\"\")\n\nspark.sql(\"\"\"\n  GRANT ALL PRIVILEGES ON SCHEMA main.idr_out TO `idr-runner-group`\n\"\"\")\n</code></pre>"},{"location":"deployment/security/#secrets-management","title":"Secrets Management","text":""},{"location":"deployment/security/#never-hardcode-credentials","title":"Never Hardcode Credentials","text":"<pre><code># \u274c Bad\npassword = \"my_secret_password\"\n\n# \u2705 Good\nimport os\npassword = os.environ.get('DB_PASSWORD')\n</code></pre>"},{"location":"deployment/security/#environment-variables","title":"Environment Variables","text":"<pre><code># .env (never commit this file!)\nSNOWFLAKE_ACCOUNT=xxx\nSNOWFLAKE_USER=idr_service_account\nSNOWFLAKE_PASSWORD=xxx\n</code></pre> <pre><code># Add to .gitignore\necho \".env\" &gt;&gt; .gitignore\necho \"*.pem\" &gt;&gt; .gitignore\necho \"*.json\" &gt;&gt; .gitignore  # For service account keys\n</code></pre>"},{"location":"deployment/security/#cloud-secret-managers","title":"Cloud Secret Managers","text":"AWS Secrets ManagerGCP Secret ManagerAzure Key VaultHashiCorp Vault <pre><code>import boto3\nimport json\n\nclient = boto3.client('secretsmanager')\nsecret = client.get_secret_value(SecretId='idr/snowflake')\ncreds = json.loads(secret['SecretString'])\n</code></pre> <pre><code>from google.cloud import secretmanager\n\nclient = secretmanager.SecretManagerServiceClient()\nname = f\"projects/PROJECT/secrets/idr-credentials/versions/latest\"\nresponse = client.access_secret_version(request={\"name\": name})\npassword = response.payload.data.decode(\"UTF-8\")\n</code></pre> <pre><code>from azure.keyvault.secrets import SecretClient\nfrom azure.identity import DefaultAzureCredential\n\nvault_url = \"https://idr-vault.vault.azure.net/\"\nclient = SecretClient(vault_url=vault_url, credential=DefaultAzureCredential())\npassword = client.get_secret(\"snowflake-password\").value\n</code></pre> <pre><code>import hvac\n\nclient = hvac.Client(url='https://vault.company.com')\nsecret = client.secrets.kv.v2.read_secret_version(path='idr/snowflake')\npassword = secret['data']['data']['password']\n</code></pre>"},{"location":"deployment/security/#data-protection","title":"Data Protection","text":""},{"location":"deployment/security/#pii-handling","title":"PII Handling","text":"<p>Identity resolution inherently deals with PII. Protect it:</p> <ol> <li>Encrypt at rest: Enable encryption on all databases/storage</li> <li>Encrypt in transit: Use TLS/SSL connections</li> <li>Mask in logs: Never log raw identifier values</li> <li>Limit retention: Delete old dry run data</li> </ol> <pre><code>-- Clean up old dry run data (contains PII)\nDELETE FROM idr_out.dry_run_results \nWHERE created_at &lt; CURRENT_TIMESTAMP - INTERVAL '7 days';\n</code></pre>"},{"location":"deployment/security/#column-level-encryption","title":"Column-Level Encryption","text":"<p>If needed, encrypt sensitive columns:</p> <pre><code>-- Snowflake example\nALTER TABLE idr_out.golden_profile_current MODIFY COLUMN email_primary \n  SET MASKING POLICY email_mask;\n\nCREATE MASKING POLICY email_mask AS (val STRING) RETURNS STRING -&gt;\n  CASE\n    WHEN CURRENT_ROLE() IN ('ADMIN', 'IDR_EXECUTOR') THEN val\n    ELSE '***MASKED***'\n  END;\n</code></pre>"},{"location":"deployment/security/#network-security","title":"Network Security","text":""},{"location":"deployment/security/#private-endpoints","title":"Private Endpoints","text":"SnowflakeBigQueryDatabricks <ul> <li>Use AWS PrivateLink or Azure Private Link</li> <li>Restrict network policies</li> </ul> <ul> <li>Use VPC Service Controls</li> <li>Configure Private Google Access</li> </ul> <ul> <li>Deploy in private subnet</li> <li>Use Private Link</li> </ul>"},{"location":"deployment/security/#firewall-rules","title":"Firewall Rules","text":"<pre><code>-- Snowflake Network Policy\nCREATE NETWORK POLICY idr_policy\n  ALLOWED_IP_LIST = ('10.0.0.0/8', '192.168.0.0/16')\n  BLOCKED_IP_LIST = ('0.0.0.0/0');\n\nALTER USER idr_service_account SET NETWORK_POLICY = idr_policy;\n</code></pre>"},{"location":"deployment/security/#audit-logging","title":"Audit Logging","text":""},{"location":"deployment/security/#enable-platform-audit-logs","title":"Enable Platform Audit Logs","text":"SnowflakeBigQueryDatabricks <pre><code>-- Query access history\nSELECT *\nFROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY\nWHERE USER_NAME = 'IDR_SERVICE_ACCOUNT'\nORDER BY QUERY_START_TIME DESC;\n</code></pre> <pre><code>-- Cloud Audit Logs (BigQuery Admin Activity)\nSELECT *\nFROM `project.region.cloudaudit_googleapis_com_activity`\nWHERE protopayload_auditlog.methodName LIKE '%bigquery%'\n</code></pre> <ul> <li>Enable audit logging in Admin Console</li> <li>Logs go to cloud storage</li> </ul>"},{"location":"deployment/security/#application-level-logging","title":"Application-Level Logging","text":"<p>The IDR runner logs all activity to <code>idr_out.run_history</code>:</p> <pre><code>SELECT \n    run_id,\n    run_mode,\n    started_at,\n    ended_at,\n    status,\n    entities_processed\nFROM idr_out.run_history\nORDER BY started_at DESC;\n</code></pre>"},{"location":"deployment/security/#compliance","title":"Compliance","text":""},{"location":"deployment/security/#gdpr","title":"GDPR","text":"<ul> <li>Right to be forgotten: Delete entity from source tables, run FULL mode to remove from clusters</li> <li>Data portability: Query membership table for entity's cluster info</li> <li>Purpose limitation: Only use IDR for stated purposes</li> </ul>"},{"location":"deployment/security/#ccpa","title":"CCPA","text":"<ul> <li>Similar to GDPR requirements</li> <li>Maintain audit trail of processing</li> </ul>"},{"location":"deployment/security/#soc-2","title":"SOC 2","text":"<ul> <li>Enable all audit logging</li> <li>Implement access controls</li> <li>Document procedures</li> </ul>"},{"location":"deployment/security/#security-checklist","title":"Security Checklist","text":""},{"location":"deployment/security/#pre-deployment","title":"Pre-Deployment","text":"<ul> <li>[ ] Service accounts created with minimal permissions</li> <li>[ ] Secrets stored in secret manager (not code)</li> <li>[ ] Network policies configured</li> <li>[ ] Encryption enabled (at rest and in transit)</li> <li>[ ] Audit logging enabled</li> </ul>"},{"location":"deployment/security/#operations","title":"Operations","text":"<ul> <li>[ ] Regular credential rotation (90 days)</li> <li>[ ] Review access logs monthly</li> <li>[ ] Clean up old dry run data</li> <li>[ ] Monitor for unusual activity</li> </ul>"},{"location":"deployment/security/#incident-response","title":"Incident Response","text":"<ul> <li>[ ] Document data breach procedures</li> <li>[ ] Test rollback procedures</li> <li>[ ] Maintain contact list for security incidents</li> </ul>"},{"location":"deployment/security/#next-steps","title":"Next Steps","text":"<ul> <li>Scheduling</li> <li>CI/CD</li> <li>Troubleshooting</li> </ul>"},{"location":"getting-started/overview/","title":"Overview","text":""},{"location":"getting-started/overview/#what-is-sql-identity-resolution","title":"What is SQL Identity Resolution?","text":"<p>SQL Identity Resolution (IDR) is a deterministic identity resolution engine that runs entirely within your data warehouse. It matches and clusters entities across multiple source systems to create a unified view of each identity.</p>"},{"location":"getting-started/overview/#key-differentiators","title":"Key Differentiators","text":"Aspect Traditional IDR SQL Identity Resolution Execution External SaaS Your data warehouse Data Movement Data leaves your infra Data stays in place Matching ML black box Transparent SQL rules Cost $60K-$480K/year Free (open source) Platforms Vendor-specific DuckDB, Snowflake, BigQuery, Databricks"},{"location":"getting-started/overview/#how-it-works","title":"How It Works","text":""},{"location":"getting-started/overview/#1-configure-sources-rules","title":"1. Configure Sources &amp; Rules","text":"<p>Define your source tables and matching rules in metadata tables:</p> <pre><code>-- Register a source table\nINSERT INTO idr_meta.source_table VALUES\n  ('customers', 'crm.customers', 'PERSON', 'customer_id', 'updated_at', 0, TRUE);\n\n-- Define matching rules\nINSERT INTO idr_meta.rule VALUES\n  ('email_exact', 'EMAIL', 1, TRUE, 10000);\n</code></pre>"},{"location":"getting-started/overview/#2-build-identity-graph","title":"2. Build Identity Graph","text":"<p>The engine extracts identifiers and builds edges between entities:</p> <pre><code>graph LR\n    A[Entity A&lt;br/&gt;email: john@acme.com] --- E1[Edge]\n    B[Entity B&lt;br/&gt;email: john@acme.com] --- E1\n    B --- E2[Edge]\n    C[Entity C&lt;br/&gt;phone: 555-0123] --- E2</code></pre>"},{"location":"getting-started/overview/#3-label-propagation","title":"3. Label Propagation","text":"<p>Connected components algorithm assigns cluster labels:</p> <pre><code>graph TB\n    subgraph Cluster1[\"Cluster: entity_A\"]\n        A[Entity A] \n        B[Entity B]\n        C[Entity C]\n    end\n\n    subgraph Cluster2[\"Cluster: entity_D\"]\n        D[Entity D]\n    end</code></pre>"},{"location":"getting-started/overview/#4-generate-outputs","title":"4. Generate Outputs","text":"<ul> <li>Membership Table: Maps each entity to its resolved_id</li> <li>Clusters Table: Cluster sizes and metadata</li> <li>Golden Profiles: Best-record selection per cluster</li> </ul>"},{"location":"getting-started/overview/#supported-platforms","title":"Supported Platforms","text":"Platform Runner Type Dry Run Metrics DuckDB Python CLI \u2705 <code>--dry-run</code> \u2705 Snowflake Stored Procedure \u2705 3rd parameter \u2705 BigQuery Python CLI \u2705 <code>--dry-run</code> \u2705 Databricks Notebook \u2705 Widget \u2705"},{"location":"getting-started/overview/#use-cases","title":"Use Cases","text":""},{"location":"getting-started/overview/#customer-data-unification","title":"Customer Data Unification","text":"<p>Merge customer records from CRM, e-commerce, support tickets, and mobile apps into a single customer view.</p>"},{"location":"getting-started/overview/#household-identification","title":"Household Identification","text":"<p>Group individuals into households using address matching and relationship detection.</p>"},{"location":"getting-started/overview/#account-deduplication","title":"Account Deduplication","text":"<p>Identify and merge duplicate business accounts across sales and marketing systems.</p>"},{"location":"getting-started/overview/#fraud-detection","title":"Fraud Detection","text":"<p>Detect when multiple accounts share identifiers that shouldn't be shared.</p>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get running in 5 minutes</li> <li>Architecture - Understand the system design</li> <li>Platform Setup - Detailed setup guides</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get SQL Identity Resolution running in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#60-second-demo-fastest-way","title":"\u26a1 60-Second Demo (Fastest Way)","text":"<p>Want to see it work immediately? Run one command:</p> <pre><code>git clone https://github.com/anilkulkarni87/sql-identity-resolution.git\ncd sql-identity-resolution\nmake demo\n</code></pre> <p>This will:</p> <ol> <li>Create a DuckDB database</li> <li>Generate 10K sample customers with shared identifiers</li> <li>Run a dry run (preview changes)</li> <li>Run a live IDR pass (create clusters)</li> <li>Generate an HTML dashboard</li> </ol> <p>Open <code>demo_results.html</code> to explore the results!</p>"},{"location":"getting-started/quickstart/#full-setup-guide","title":"Full Setup Guide","text":""},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+ (for DuckDB/BigQuery CLI)</li> <li>Access to your target platform (Snowflake account, GCP project, Databricks workspace)</li> </ul>"},{"location":"getting-started/quickstart/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/anilkulkarni87/sql-identity-resolution.git\ncd sql-identity-resolution\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-choose-your-platform","title":"Step 2: Choose Your Platform","text":"DuckDB (Local)SnowflakeBigQueryDatabricks <p>Best for: Development, testing, small datasets (&lt;10M records)</p> <pre><code># Install DuckDB\npip install duckdb\n\n# Create database with schema\nduckdb idr.duckdb &lt; sql/duckdb/core/00_ddl_all.sql\n</code></pre> <p>Best for: Enterprise data warehouses, existing Snowflake users</p> <pre><code>-- Run in Snowflake worksheet\n-- Execute sql/snowflake/core/00_ddl_all.sql\n\n-- Verify schemas created\nSHOW SCHEMAS IN DATABASE your_database;\n</code></pre> <p>Best for: GCP users, serverless preference</p> <pre><code># Set credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n\n# Create datasets\nbq mk --dataset your_project:idr_meta\nbq mk --dataset your_project:idr_work\nbq mk --dataset your_project:idr_out\n\n# Run DDL\nbq query --use_legacy_sql=false &lt; sql/bigquery/core/00_ddl_all.sql\n</code></pre> <p>Best for: Existing Databricks/Spark users, large-scale processing</p> <pre><code>1. Import notebooks from sql/databricks/core/ and sql/databricks/ops/\n2. Run IDR_QuickStart.py notebook\n3. This creates schemas and sample data\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-configure-your-sources","title":"Step 3: Configure Your Sources","text":""},{"location":"getting-started/quickstart/#31-register-source-tables","title":"3.1 Register Source Tables","text":"<pre><code>INSERT INTO idr_meta.source_table (\n  table_id, \n  table_fqn, \n  entity_type, \n  entity_key_expr, \n  watermark_column, \n  watermark_lookback_minutes, \n  is_active\n) VALUES (\n  'customers',           -- Unique identifier for this source\n  'crm.customers',       -- Fully qualified table name\n  'PERSON',              -- Entity type\n  'customer_id',         -- Expression for entity key\n  'updated_at',          -- Watermark column for incremental\n  0,                     -- Lookback minutes\n  TRUE                   -- Is active\n);\n</code></pre>"},{"location":"getting-started/quickstart/#32-define-matching-rules","title":"3.2 Define Matching Rules","text":"<pre><code>INSERT INTO idr_meta.rule (\n  rule_id, \n  identifier_type, \n  priority, \n  is_active,\n  max_group_size\n) VALUES \n  ('email_exact', 'EMAIL', 1, TRUE, 10000),\n  ('phone_exact', 'PHONE', 2, TRUE, 5000),\n  ('loyalty_id', 'LOYALTY', 3, TRUE, 1000);\n</code></pre>"},{"location":"getting-started/quickstart/#33-map-identifiers","title":"3.3 Map Identifiers","text":"<pre><code>INSERT INTO idr_meta.identifier_mapping (\n  table_id, \n  identifier_type, \n  column_expr, \n  requires_normalization\n) VALUES \n  ('customers', 'EMAIL', 'email', TRUE),\n  ('customers', 'PHONE', 'phone', TRUE);\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-run-a-dry-run","title":"Step 4: Run a Dry Run","text":"<p>Always Dry Run First</p> <p>Before committing changes, preview the impact with a dry run.</p> DuckDBSnowflakeBigQueryDatabricks <pre><code>python sql/duckdb/core/idr_run.py \\\n  --db=idr.duckdb \\\n  --run-mode=FULL \\\n  --dry-run\n</code></pre> <pre><code>CALL idr_run('FULL', 30, TRUE);  -- TRUE = dry run\n</code></pre> <pre><code>python sql/bigquery/core/idr_run.py \\\n  --project=your-project \\\n  --run-mode=FULL \\\n  --dry-run\n</code></pre> <p>Set widget <code>DRY_RUN</code> to <code>true</code>, then run the notebook.</p>"},{"location":"getting-started/quickstart/#dry-run-output","title":"Dry Run Output","text":"<pre><code>============================================================\nDRY RUN SUMMARY (No changes committed)\n============================================================\nRun ID:          dry_run_abc123\nMode:            FULL (DRY RUN)\nDuration:        5s\nStatus:          DRY_RUN_COMPLETE\n\nIMPACT PREVIEW:\n  New Entities:      1,234\n  Moved Entities:    89\n  Edges Would Create: 5,678\n  Largest Cluster:   523 entities\n\nREVIEW QUERIES:\n  \u2192 All changes:  SELECT * FROM idr_out.dry_run_results WHERE run_id = 'dry_run_abc123'\n  \u2192 Moved only:   SELECT * FROM idr_out.dry_run_results WHERE ... AND change_type = 'MOVED'\n\n\u26a0\ufe0f  THIS WAS A DRY RUN - NO CHANGES COMMITTED\n============================================================\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-review-and-commit","title":"Step 5: Review and Commit","text":"<p>After reviewing the dry run results:</p> DuckDBSnowflakeBigQueryDatabricks <pre><code># Remove --dry-run to commit\npython sql/duckdb/core/idr_run.py \\\n  --db=idr.duckdb \\\n  --run-mode=FULL\n</code></pre> <pre><code>CALL idr_run('FULL', 30, FALSE);  -- FALSE = live run\n</code></pre> <pre><code>python sql/bigquery/core/idr_run.py \\\n  --project=your-project \\\n  --run-mode=FULL\n</code></pre> <p>Set widget <code>DRY_RUN</code> to <code>false</code>, then run.</p>"},{"location":"getting-started/quickstart/#step-6-query-results","title":"Step 6: Query Results","text":"<pre><code>-- View cluster membership\nSELECT \n  entity_key,\n  resolved_id,\n  updated_ts\nFROM idr_out.identity_resolved_membership_current\nLIMIT 100;\n\n-- View cluster sizes\nSELECT \n  resolved_id,\n  cluster_size\nFROM idr_out.identity_clusters_current\nORDER BY cluster_size DESC\nLIMIT 20;\n\n-- View golden profiles\nSELECT * FROM idr_out.golden_profile_current LIMIT 10;\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Advanced rule setup</li> <li>Dry Run Mode - Understanding previews</li> <li>Metrics &amp; Monitoring - Observability setup</li> <li>Production Hardening - Data quality controls</li> </ul>"},{"location":"getting-started/platforms/bigquery/","title":"BigQuery Setup","text":"<p>BigQuery is ideal for GCP-native organizations seeking serverless identity resolution.</p>"},{"location":"getting-started/platforms/bigquery/#prerequisites","title":"Prerequisites","text":"<ul> <li>GCP project with BigQuery enabled</li> <li>Service account with BigQuery Admin role (or equivalent)</li> <li>Python 3.9+ with <code>google-cloud-bigquery</code> package</li> </ul>"},{"location":"getting-started/platforms/bigquery/#installation","title":"Installation","text":"<pre><code># Clone repository\ngit clone https://github.com/anilkulkarni87/sql-identity-resolution.git\ncd sql-identity-resolution\n\n# Install dependencies\npip install google-cloud-bigquery\n\n# Set credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#create-datasets-and-tables","title":"Create Datasets and Tables","text":"<pre><code># Create datasets\nbq mk --dataset your_project:idr_meta\nbq mk --dataset your_project:idr_work\nbq mk --dataset your_project:idr_out\n\n# Run DDL (creates all tables)\nbq query --use_legacy_sql=false &lt; sql/bigquery/00_ddl_all.sql\n</code></pre> <p>Or run the DDL directly in BigQuery console.</p>"},{"location":"getting-started/platforms/bigquery/#configure-sources","title":"Configure Sources","text":"<pre><code>-- Register source table\nINSERT INTO `your_project.idr_meta.source_table` VALUES\n  ('customers', 'your_project.crm.customers', 'PERSON', 'customer_id', 'updated_at', 0, TRUE);\n\n-- Define matching rules\nINSERT INTO `your_project.idr_meta.rule` VALUES\n  ('email_exact', 'EMAIL', 1, TRUE, 10000),\n  ('phone_exact', 'PHONE', 2, TRUE, 5000);\n\n-- Map identifiers\nINSERT INTO `your_project.idr_meta.identifier_mapping` VALUES\n  ('customers', 'EMAIL', 'email', TRUE),\n  ('customers', 'PHONE', 'phone', TRUE);\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#run-idr","title":"Run IDR","text":""},{"location":"getting-started/platforms/bigquery/#cli-options","title":"CLI Options","text":"Option Description Default <code>--project</code> GCP project ID Required <code>--run-mode</code> <code>FULL</code> or <code>INCR</code> <code>INCR</code> <code>--max-iters</code> Max label propagation iterations 30 <code>--dry-run</code> Preview mode (no commits) False"},{"location":"getting-started/platforms/bigquery/#dry-run-preview","title":"Dry Run (Preview)","text":"<pre><code>python sql/bigquery/idr_run.py \\\n  --project=your-project \\\n  --run-mode=FULL \\\n  --dry-run\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#live-run","title":"Live Run","text":"<pre><code>python sql/bigquery/idr_run.py \\\n  --project=your-project \\\n  --run-mode=FULL\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#incremental-run","title":"Incremental Run","text":"<pre><code>python sql/bigquery/idr_run.py \\\n  --project=your-project \\\n  --run-mode=INCR\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#review-dry-run-results","title":"Review Dry Run Results","text":"<pre><code>-- View proposed changes\nSELECT entity_key, current_resolved_id, proposed_resolved_id, change_type\nFROM `your_project.idr_out.dry_run_results`\nWHERE run_id = 'dry_run_abc123'\nORDER BY change_type;\n\n-- Summary statistics\nSELECT * FROM `your_project.idr_out.dry_run_summary`\nWHERE run_id = 'dry_run_abc123';\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#verify-results","title":"Verify Results","text":"<pre><code>-- Run history\nSELECT run_id, status, entities_processed, duration_seconds\nFROM `your_project.idr_out.run_history`\nORDER BY started_at DESC\nLIMIT 10;\n\n-- Cluster distribution\nSELECT cluster_size, COUNT(*) as count\nFROM `your_project.idr_out.identity_clusters_current`\nGROUP BY cluster_size\nORDER BY cluster_size;\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#scheduling-with-cloud-scheduler","title":"Scheduling with Cloud Scheduler","text":""},{"location":"getting-started/platforms/bigquery/#1-create-a-cloud-function","title":"1. Create a Cloud Function","text":"<pre><code># main.py\nimport subprocess\nimport functions_framework\n\n@functions_framework.http\ndef run_idr(request):\n    result = subprocess.run([\n        'python', 'idr_run.py',\n        '--project=your-project',\n        '--run-mode=INCR'\n    ], capture_output=True, text=True)\n    return result.stdout\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#2-deploy-the-function","title":"2. Deploy the Function","text":"<pre><code>gcloud functions deploy idr-runner \\\n  --runtime=python311 \\\n  --trigger-http \\\n  --source=./sql/bigquery\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#3-create-cloud-scheduler-job","title":"3. Create Cloud Scheduler Job","text":"<pre><code>gcloud scheduler jobs create http idr-hourly \\\n  --schedule=\"0 * * * *\" \\\n  --uri=\"https://REGION-PROJECT.cloudfunctions.net/idr-runner\" \\\n  --http-method=GET\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#terraform-deployment","title":"Terraform Deployment","text":"<pre><code># main.tf\nresource \"google_bigquery_dataset\" \"idr_meta\" {\n  dataset_id = \"idr_meta\"\n  location   = \"US\"\n}\n\nresource \"google_bigquery_dataset\" \"idr_work\" {\n  dataset_id = \"idr_work\"\n  location   = \"US\"\n}\n\nresource \"google_bigquery_dataset\" \"idr_out\" {\n  dataset_id = \"idr_out\"\n  location   = \"US\"\n}\n\nresource \"google_cloud_scheduler_job\" \"idr_hourly\" {\n  name     = \"idr-hourly\"\n  schedule = \"0 * * * *\"\n\n  http_target {\n    uri         = google_cloudfunctions_function.idr_runner.https_trigger_url\n    http_method = \"GET\"\n  }\n}\n</code></pre>"},{"location":"getting-started/platforms/bigquery/#cost-optimization","title":"Cost Optimization","text":"<ol> <li>Use Slot Reservations: For predictable costs on large datasets</li> <li>Partition Tables: Partition by updated_at for efficient incremental processing</li> <li>Use BI Engine: For dashboard queries on output tables</li> </ol>"},{"location":"getting-started/platforms/bigquery/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>Dry Run Mode</li> <li>CI/CD</li> </ul>"},{"location":"getting-started/platforms/databricks/","title":"Databricks Setup","text":"<p>Databricks is ideal for large-scale processing and organizations with existing Spark infrastructure.</p>"},{"location":"getting-started/platforms/databricks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Databricks workspace (AWS, Azure, or GCP)</li> <li>Cluster with Spark 3.x</li> <li>Unity Catalog or Hive metastore</li> </ul>"},{"location":"getting-started/platforms/databricks/#installation","title":"Installation","text":""},{"location":"getting-started/platforms/databricks/#1-import-notebooks","title":"1. Import Notebooks","text":"<p>Clone the repository to your Databricks Repos:</p> <pre><code>Workspace \u2192 Repos \u2192 Add Repo \u2192 \nURL: https://github.com/anilkulkarni87/sql-identity-resolution.git\n</code></pre> <p>Or manually import notebooks from <code>sql/databricks/notebooks/</code>:</p> <ul> <li><code>IDR_QuickStart.py</code> - Setup and demo</li> <li><code>IDR_Run.py</code> - Main runner</li> </ul>"},{"location":"getting-started/platforms/databricks/#2-run-quick-start","title":"2. Run Quick Start","text":"<p>Open and run <code>IDR_QuickStart.py</code>. This will:</p> <ol> <li>Create schemas (<code>idr_meta</code>, <code>idr_work</code>, <code>idr_out</code>)</li> <li>Create all required tables</li> <li>Insert sample data</li> <li>Run a demo IDR process</li> </ol>"},{"location":"getting-started/platforms/databricks/#configure-sources","title":"Configure Sources","text":"<pre><code># In a notebook cell\nspark.sql(\"\"\"\nINSERT INTO idr_meta.source_table VALUES\n  ('customers', 'crm.customers', 'PERSON', 'customer_id', 'updated_at', 0, TRUE)\n\"\"\")\n\nspark.sql(\"\"\"\nINSERT INTO idr_meta.rule VALUES\n  ('email_exact', 'EMAIL', 1, TRUE, 10000),\n  ('phone_exact', 'PHONE', 2, TRUE, 5000)\n\"\"\")\n\nspark.sql(\"\"\"\nINSERT INTO idr_meta.identifier_mapping VALUES\n  ('customers', 'EMAIL', 'email', TRUE),\n  ('customers', 'PHONE', 'phone', TRUE)\n\"\"\")\n</code></pre>"},{"location":"getting-started/platforms/databricks/#run-idr","title":"Run IDR","text":""},{"location":"getting-started/platforms/databricks/#widget-parameters","title":"Widget Parameters","text":"<p>The <code>IDR_Run.py</code> notebook uses widgets for configuration:</p> Widget Description Options <code>RUN_MODE</code> Processing mode <code>INCR</code>, <code>FULL</code> <code>MAX_ITERS</code> Max LP iterations Integer (default: 30) <code>DRY_RUN</code> Preview mode <code>true</code>, <code>false</code> <code>RUN_ID</code> Custom run ID String (optional)"},{"location":"getting-started/platforms/databricks/#dry-run-preview","title":"Dry Run (Preview)","text":"<ol> <li>Open <code>IDR_Run.py</code> notebook</li> <li>Set widgets:</li> <li><code>RUN_MODE</code> = <code>FULL</code></li> <li><code>DRY_RUN</code> = <code>true</code></li> <li>Run All</li> </ol>"},{"location":"getting-started/platforms/databricks/#live-run","title":"Live Run","text":"<ol> <li>Set <code>DRY_RUN</code> = <code>false</code></li> <li>Run All</li> </ol>"},{"location":"getting-started/platforms/databricks/#review-dry-run-results","title":"Review Dry Run Results","text":"<pre><code># View proposed changes\ndisplay(spark.sql(\"\"\"\n    SELECT entity_key, current_resolved_id, proposed_resolved_id, change_type\n    FROM idr_out.dry_run_results\n    WHERE run_id = 'dry_run_abc123'\n    ORDER BY change_type\n\"\"\"))\n\n# Summary\ndisplay(spark.sql(\"\"\"\n    SELECT * FROM idr_out.dry_run_summary\n    WHERE run_id = 'dry_run_abc123'\n\"\"\"))\n</code></pre>"},{"location":"getting-started/platforms/databricks/#verify-results","title":"Verify Results","text":"<pre><code># Run history\ndisplay(spark.sql(\"\"\"\n    SELECT run_id, status, entities_processed, duration_seconds, warnings\n    FROM idr_out.run_history\n    ORDER BY started_at DESC\n    LIMIT 10\n\"\"\"))\n\n# Cluster distribution\ndisplay(spark.sql(\"\"\"\n    SELECT cluster_size, COUNT(*) as count\n    FROM idr_out.identity_clusters_current\n    GROUP BY cluster_size\n    ORDER BY cluster_size\n\"\"\"))\n</code></pre>"},{"location":"getting-started/platforms/databricks/#scheduling-with-databricks-workflows","title":"Scheduling with Databricks Workflows","text":""},{"location":"getting-started/platforms/databricks/#create-a-job","title":"Create a Job","text":"<ol> <li>Go to Workflows \u2192 Create Job</li> <li>Add task:</li> <li>Type: Notebook</li> <li>Path: <code>/Repos/.../IDR_Run</code></li> <li>Cluster: Select your cluster</li> <li>Set parameters:    <pre><code>{\n  \"RUN_MODE\": \"INCR\",\n  \"DRY_RUN\": \"false\"\n}\n</code></pre></li> <li>Add schedule (e.g., hourly)</li> </ol>"},{"location":"getting-started/platforms/databricks/#job-definition-json","title":"Job Definition (JSON)","text":"<pre><code>{\n  \"name\": \"IDR Hourly\",\n  \"tasks\": [\n    {\n      \"task_key\": \"idr_run\",\n      \"notebook_task\": {\n        \"notebook_path\": \"/Repos/your-user/sql-identity-resolution/sql/databricks/notebooks/IDR_Run\",\n        \"base_parameters\": {\n          \"RUN_MODE\": \"INCR\",\n          \"DRY_RUN\": \"false\"\n        }\n      },\n      \"existing_cluster_id\": \"your-cluster-id\"\n    }\n  ],\n  \"schedule\": {\n    \"quartz_cron_expression\": \"0 0 * * * ?\",\n    \"timezone_id\": \"UTC\"\n  }\n}\n</code></pre>"},{"location":"getting-started/platforms/databricks/#unity-catalog-integration","title":"Unity Catalog Integration","text":"<p>For Unity Catalog, update table references:</p> <pre><code># Use three-part names\ncatalog = \"main\"\nschema_meta = f\"{catalog}.idr_meta\"\nschema_work = f\"{catalog}.idr_work\"\nschema_out = f\"{catalog}.idr_out\"\n</code></pre>"},{"location":"getting-started/platforms/databricks/#performance-tuning","title":"Performance Tuning","text":"<ol> <li>Cluster Sizing: Use memory-optimized instances for large graphs</li> <li>Caching: Cache frequently accessed tables    <pre><code>spark.sql(\"CACHE TABLE idr_work.edges_new\")\n</code></pre></li> <li>Partitioning: Partition source tables by watermark column</li> <li>Delta Lake: Use Delta format for ACID transactions</li> </ol>"},{"location":"getting-started/platforms/databricks/#monitoring","title":"Monitoring","text":""},{"location":"getting-started/platforms/databricks/#view-job-runs","title":"View Job Runs","text":"<pre><code># Query run history\ndisplay(spark.sql(\"\"\"\n    SELECT \n        run_id,\n        status,\n        entities_processed,\n        duration_seconds,\n        started_at\n    FROM idr_out.run_history\n    WHERE started_at &gt;= current_date - 7\n    ORDER BY started_at DESC\n\"\"\"))\n</code></pre>"},{"location":"getting-started/platforms/databricks/#set-up-alerts","title":"Set Up Alerts","text":"<p>Use Databricks SQL Alerts to notify on: - Run failures (<code>status != 'SUCCESS'</code>) - Large clusters exceeding threshold - Long-running jobs</p>"},{"location":"getting-started/platforms/databricks/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>Dry Run Mode</li> <li>Metrics &amp; Monitoring</li> </ul>"},{"location":"getting-started/platforms/duckdb/","title":"DuckDB Setup","text":"<p>DuckDB is the recommended platform for development, testing, and small-to-medium datasets.</p>"},{"location":"getting-started/platforms/duckdb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>DuckDB (<code>pip install duckdb</code>)</li> </ul>"},{"location":"getting-started/platforms/duckdb/#installation","title":"Installation","text":"<pre><code># Clone repository\ngit clone https://github.com/anilkulkarni87/sql-identity-resolution.git\ncd sql-identity-resolution\n\n# Install dependencies\npip install duckdb\n</code></pre>"},{"location":"getting-started/platforms/duckdb/#create-database","title":"Create Database","text":"<pre><code># Create database with all schemas and tables\nduckdb idr.duckdb &lt; sql/duckdb/00_ddl_all.sql\n</code></pre> <p>This creates three schemas:</p> Schema Purpose <code>idr_meta</code> Configuration (sources, rules, mappings) <code>idr_work</code> Temporary processing tables <code>idr_out</code> Output tables (membership, clusters, metrics)"},{"location":"getting-started/platforms/duckdb/#configure-sources","title":"Configure Sources","text":"<p>Connect to DuckDB and insert your configuration:</p> <pre><code>duckdb idr.duckdb\n</code></pre> <pre><code>-- Register your source table\nINSERT INTO idr_meta.source_table VALUES\n  ('customers', 'main.customers', 'PERSON', 'customer_id', 'updated_at', 0, TRUE);\n\n-- Define matching rules\nINSERT INTO idr_meta.rule VALUES\n  ('email_exact', 'EMAIL', 1, TRUE, 10000),\n  ('phone_exact', 'PHONE', 2, TRUE, 5000);\n\n-- Map identifiers to columns\nINSERT INTO idr_meta.identifier_mapping VALUES\n  ('customers', 'EMAIL', 'email', TRUE),\n  ('customers', 'PHONE', 'phone', TRUE);\n</code></pre>"},{"location":"getting-started/platforms/duckdb/#run-idr","title":"Run IDR","text":""},{"location":"getting-started/platforms/duckdb/#dry-run-preview","title":"Dry Run (Preview)","text":"<pre><code>python sql/duckdb/idr_run.py \\\n  --db=idr.duckdb \\\n  --run-mode=FULL \\\n  --dry-run\n</code></pre>"},{"location":"getting-started/platforms/duckdb/#live-run","title":"Live Run","text":"<pre><code>python sql/duckdb/idr_run.py \\\n  --db=idr.duckdb \\\n  --run-mode=FULL\n</code></pre>"},{"location":"getting-started/platforms/duckdb/#incremental-run","title":"Incremental Run","text":"<pre><code>python sql/duckdb/idr_run.py \\\n  --db=idr.duckdb \\\n  --run-mode=INCR\n</code></pre>"},{"location":"getting-started/platforms/duckdb/#cli-options","title":"CLI Options","text":"Option Description Default <code>--db</code> Path to DuckDB database file Required <code>--run-mode</code> <code>FULL</code> or <code>INCR</code> <code>INCR</code> <code>--max-iters</code> Max label propagation iterations 30 <code>--dry-run</code> Preview mode (no commits) False"},{"location":"getting-started/platforms/duckdb/#verify-results","title":"Verify Results","text":"<pre><code>-- Check run history\nSELECT run_id, status, entities_processed, duration_seconds \nFROM idr_out.run_history \nORDER BY started_at DESC;\n\n-- View cluster distribution\nSELECT cluster_size, COUNT(*) as count\nFROM idr_out.identity_clusters_current\nGROUP BY cluster_size\nORDER BY cluster_size;\n\n-- Lookup specific entity\nSELECT * FROM idr_out.identity_resolved_membership_current\nWHERE entity_key = 'customers:12345';\n</code></pre>"},{"location":"getting-started/platforms/duckdb/#scheduling","title":"Scheduling","text":""},{"location":"getting-started/platforms/duckdb/#cron","title":"Cron","text":"<pre><code># Run every hour\n0 * * * * cd /path/to/repo &amp;&amp; python sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=INCR\n</code></pre>"},{"location":"getting-started/platforms/duckdb/#airflow","title":"Airflow","text":"<p>See Scheduling Guide for DAG templates.</p>"},{"location":"getting-started/platforms/duckdb/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Incremental Mode: After initial FULL run, use INCR for better performance</li> <li>Set max_group_size: Limit overly generic identifiers (e.g., <code>test@test.com</code>)</li> <li>Index source tables: Add indexes on entity key and watermark columns</li> </ol>"},{"location":"getting-started/platforms/duckdb/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>Dry Run Mode</li> <li>Troubleshooting</li> </ul>"},{"location":"getting-started/platforms/snowflake/","title":"Snowflake Setup","text":"<p>Snowflake is ideal for enterprise data warehouses with existing Snowflake infrastructure.</p>"},{"location":"getting-started/platforms/snowflake/#prerequisites","title":"Prerequisites","text":"<ul> <li>Snowflake account with ACCOUNTADMIN or CREATE SCHEMA privileges</li> <li>Snowflake worksheet or SnowSQL CLI</li> </ul>"},{"location":"getting-started/platforms/snowflake/#create-schemas-and-tables","title":"Create Schemas and Tables","text":"<p>Run the DDL script in a Snowflake worksheet:</p> <pre><code>-- Run contents of sql/snowflake/00_ddl_all.sql\n\n-- Verify schemas created\nSHOW SCHEMAS;\n</code></pre> <p>This creates:</p> Schema Purpose <code>IDR_META</code> Configuration tables <code>IDR_WORK</code> Transient processing tables <code>IDR_OUT</code> Output tables"},{"location":"getting-started/platforms/snowflake/#create-stored-procedure","title":"Create Stored Procedure","text":"<p>The IDR runner is implemented as a JavaScript stored procedure:</p> <pre><code>-- Run contents of sql/snowflake/IDR_Run.sql\n-- This creates the idr_run() procedure\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#configure-sources","title":"Configure Sources","text":"<pre><code>-- Register source table\nINSERT INTO idr_meta.source_table VALUES\n  ('customers', 'SALES.CUSTOMERS', 'PERSON', 'CUSTOMER_ID', 'UPDATED_AT', 0, TRUE);\n\n-- Define matching rules\nINSERT INTO idr_meta.rule VALUES\n  ('email_exact', 'EMAIL', 1, TRUE, 10000),\n  ('phone_exact', 'PHONE', 2, TRUE, 5000);\n\n-- Map identifiers\nINSERT INTO idr_meta.identifier_mapping VALUES\n  ('customers', 'EMAIL', 'EMAIL', TRUE),\n  ('customers', 'PHONE', 'PHONE', TRUE);\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#run-idr","title":"Run IDR","text":""},{"location":"getting-started/platforms/snowflake/#procedure-signature","title":"Procedure Signature","text":"<pre><code>CALL idr_run(\n  RUN_MODE,    -- VARCHAR: 'FULL' or 'INCR'\n  MAX_ITERS,   -- INTEGER: Max label propagation iterations\n  DRY_RUN      -- BOOLEAN: TRUE = preview, FALSE = commit\n);\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#dry-run-preview","title":"Dry Run (Preview)","text":"<pre><code>CALL idr_run('FULL', 30, TRUE);\n</code></pre> <p>Output: <pre><code>DRY_RUN_COMPLETE: run_id=dry_run_abc123, new_entities=1234, moved_entities=89, duration=5s | DRY RUN - NO CHANGES COMMITTED\n</code></pre></p>"},{"location":"getting-started/platforms/snowflake/#live-run","title":"Live Run","text":"<pre><code>CALL idr_run('FULL', 30, FALSE);\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#incremental-run","title":"Incremental Run","text":"<pre><code>CALL idr_run('INCR', 30, FALSE);\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#review-dry-run-results","title":"Review Dry Run Results","text":"<pre><code>-- View proposed changes\nSELECT entity_key, current_resolved_id, proposed_resolved_id, change_type\nFROM idr_out.dry_run_results\nWHERE run_id = 'dry_run_abc123'\nORDER BY change_type;\n\n-- Summary statistics\nSELECT * FROM idr_out.dry_run_summary\nWHERE run_id = 'dry_run_abc123';\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#verify-results","title":"Verify Results","text":"<pre><code>-- Run history\nSELECT run_id, status, entities_processed, duration_seconds, warnings\nFROM idr_out.run_history\nORDER BY started_at DESC\nLIMIT 10;\n\n-- Cluster distribution\nSELECT cluster_size, COUNT(*) as count\nFROM idr_out.identity_clusters_current\nGROUP BY cluster_size\nORDER BY cluster_size;\n\n-- Metrics\nSELECT metric_name, metric_value, recorded_at\nFROM idr_out.metrics_export\nWHERE run_id = 'run_abc123';\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#scheduling-with-snowflake-tasks","title":"Scheduling with Snowflake Tasks","text":"<pre><code>-- Create a task to run every hour\nCREATE OR REPLACE TASK idr_hourly_task\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = '60 MINUTE'\nAS\n  CALL idr_run('INCR', 30, FALSE);\n\n-- Enable the task\nALTER TASK idr_hourly_task RESUME;\n\n-- Check task history\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())\nWHERE NAME = 'IDR_HOURLY_TASK'\nORDER BY SCHEDULED_TIME DESC;\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Use a Service Role: Create a dedicated role for IDR execution</li> <li>Grant Minimal Permissions: Only SELECT on source tables, full access to IDR schemas</li> <li>Enable Query Tagging: Add query tags for cost attribution</li> </ol> <pre><code>-- Create dedicated role\nCREATE ROLE IDR_EXECUTOR;\n\n-- Grant required permissions\nGRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE IDR_EXECUTOR;\nGRANT USAGE ON DATABASE ANALYTICS TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA IDR_META TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA IDR_WORK TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA IDR_OUT TO ROLE IDR_EXECUTOR;\nGRANT SELECT ON ALL TABLES IN SCHEMA SALES TO ROLE IDR_EXECUTOR;\n</code></pre>"},{"location":"getting-started/platforms/snowflake/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>Dry Run Mode</li> <li>Scheduling</li> </ul>"},{"location":"guides/configuration/","title":"Configuration Guide","text":"<p>Learn how to configure SQL Identity Resolution for your use case.</p>"},{"location":"guides/configuration/#configuration-overview","title":"Configuration Overview","text":"<p>IDR uses three main configuration tables:</p> Table Purpose <code>idr_meta.source_table</code> Which tables to process <code>idr_meta.rule</code> Matching rules and limits <code>idr_meta.identifier_mapping</code> Column-to-identifier mappings"},{"location":"guides/configuration/#registering-source-tables","title":"Registering Source Tables","text":""},{"location":"guides/configuration/#required-fields","title":"Required Fields","text":"<pre><code>INSERT INTO idr_meta.source_table (\n    table_id,                    -- Unique identifier (your choice)\n    table_fqn,                   -- Fully qualified table name\n    entity_type,                 -- PERSON, ACCOUNT, HOUSEHOLD\n    entity_key_expr,             -- SQL expression for entity key\n    watermark_column,            -- Column for incremental processing\n    watermark_lookback_minutes,  -- Buffer for late-arriving data\n    is_active                    -- TRUE to include in processing\n) VALUES (...);\n</code></pre>"},{"location":"guides/configuration/#examples","title":"Examples","text":"SimpleMultiple SourcesComposite Key <pre><code>-- Single customer table\nINSERT INTO idr_meta.source_table VALUES\n  ('customers', 'crm.customers', 'PERSON', 'customer_id', 'updated_at', 0, TRUE);\n</code></pre> <pre><code>-- Multiple sources\nINSERT INTO idr_meta.source_table VALUES\n  ('customers', 'crm.customers', 'PERSON', 'customer_id', 'updated_at', 0, TRUE),\n  ('orders', 'ecom.orders', 'PERSON', 'user_id', 'order_date', 60, TRUE),\n  ('support', 'helpdesk.tickets', 'PERSON', 'contact_id', 'created_at', 0, TRUE);\n</code></pre> <pre><code>-- Use SQL expression for composite key\nINSERT INTO idr_meta.source_table VALUES\n  ('transactions', \n   'finance.transactions', \n   'PERSON', \n   'account_type || '':'' || account_id',  -- composite key\n   'transaction_date', \n   0, \n   TRUE);\n</code></pre>"},{"location":"guides/configuration/#defining-matching-rules","title":"Defining Matching Rules","text":""},{"location":"guides/configuration/#rule-properties","title":"Rule Properties","text":"Column Description <code>rule_id</code> Unique identifier <code>identifier_type</code> EMAIL, PHONE, LOYALTY, etc. <code>priority</code> Lower = higher priority <code>is_active</code> Include in processing <code>max_group_size</code> Limit entities per identifier"},{"location":"guides/configuration/#common-rules","title":"Common Rules","text":"<pre><code>INSERT INTO idr_meta.rule VALUES\n  -- Email matching (highest priority)\n  ('email_exact', 'EMAIL', 1, TRUE, 10000),\n\n  -- Phone matching\n  ('phone_exact', 'PHONE', 2, TRUE, 5000),\n\n  -- Loyalty ID (should be unique)\n  ('loyalty_id', 'LOYALTY', 3, TRUE, 1),\n\n  -- SSN (highly sensitive, low group size)\n  ('ssn_exact', 'SSN', 4, TRUE, 5);\n</code></pre>"},{"location":"guides/configuration/#max_group_size-guidelines","title":"max_group_size Guidelines","text":"Identifier Type Recommended max_group_size Reason SSN, Loyalty ID 1-5 Should be unique Phone 1000-5000 Shared family phones Email 5000-10000 Shared/generic emails Name 50000+ Very common names"},{"location":"guides/configuration/#mapping-identifiers","title":"Mapping Identifiers","text":""},{"location":"guides/configuration/#basic-mapping","title":"Basic Mapping","text":"<pre><code>INSERT INTO idr_meta.identifier_mapping (\n    table_id,              -- FK to source_table\n    identifier_type,       -- Must match a rule\n    column_expr,           -- Column or SQL expression\n    requires_normalization -- Apply standard normalization\n) VALUES (...);\n</code></pre>"},{"location":"guides/configuration/#examples_1","title":"Examples","text":"Direct ColumnMultiple ColumnsSQL Expression <pre><code>INSERT INTO idr_meta.identifier_mapping VALUES\n  ('customers', 'EMAIL', 'email', TRUE),\n  ('customers', 'PHONE', 'phone', TRUE);\n</code></pre> <pre><code>-- Map multiple email columns\nINSERT INTO idr_meta.identifier_mapping VALUES\n  ('customers', 'EMAIL', 'primary_email', TRUE),\n  ('customers', 'EMAIL', 'secondary_email', TRUE);\n</code></pre> <pre><code>-- Concatenate fields\nINSERT INTO idr_meta.identifier_mapping VALUES\n  ('customers', 'NAME', 'first_name || '' '' || last_name', TRUE);\n</code></pre>"},{"location":"guides/configuration/#identifier-exclusions","title":"Identifier Exclusions","text":"<p>Exclude known bad identifiers from matching:</p>"},{"location":"guides/configuration/#exact-match-exclusions","title":"Exact Match Exclusions","text":"<pre><code>INSERT INTO idr_meta.identifier_exclusion VALUES\n  ('EMAIL', 'test@test.com', FALSE, 'Generic test email'),\n  ('EMAIL', 'null@null.com', FALSE, 'Null placeholder'),\n  ('PHONE', '0000000000', FALSE, 'Invalid phone'),\n  ('PHONE', '1111111111', FALSE, 'Invalid phone');\n</code></pre>"},{"location":"guides/configuration/#pattern-exclusions","title":"Pattern Exclusions","text":"<pre><code>INSERT INTO idr_meta.identifier_exclusion VALUES\n  ('EMAIL', '%@example.com', TRUE, 'Example domain'),\n  ('EMAIL', 'noreply@%', TRUE, 'No-reply addresses'),\n  ('EMAIL', '%test%@%', TRUE, 'Test addresses');\n</code></pre>"},{"location":"guides/configuration/#configuration-settings","title":"Configuration Settings","text":""},{"location":"guides/configuration/#available-settings","title":"Available Settings","text":"<pre><code>INSERT INTO idr_meta.config (config_key, config_value, description) VALUES\n  ('dry_run_retention_days', '7', 'Days to retain dry run results'),\n  ('large_cluster_threshold', '5000', 'Warn on clusters larger than this');\n</code></pre>"},{"location":"guides/configuration/#reading-configuration","title":"Reading Configuration","text":"<p>In your runner, use the <code>get_config</code> helper:</p> PythonSnowflake <pre><code>threshold = int(get_config('large_cluster_threshold', '5000'))\n</code></pre> <pre><code>var threshold = parseInt(getConfig('large_cluster_threshold', '5000'));\n</code></pre>"},{"location":"guides/configuration/#multi-entity-type-setup","title":"Multi-Entity Type Setup","text":"<p>For different entity types (PERSON, ACCOUNT, HOUSEHOLD):</p> <pre><code>-- Person sources\nINSERT INTO idr_meta.source_table VALUES\n  ('contacts', 'crm.contacts', 'PERSON', 'contact_id', 'updated_at', 0, TRUE);\n\n-- Account sources (separate entity type)\nINSERT INTO idr_meta.source_table VALUES\n  ('companies', 'crm.companies', 'ACCOUNT', 'company_id', 'updated_at', 0, TRUE);\n\n-- Rules for accounts\nINSERT INTO idr_meta.rule VALUES\n  ('company_domain', 'DOMAIN', 1, TRUE, 100),\n  ('company_duns', 'DUNS', 2, TRUE, 1);\n</code></pre> <p>Note</p> <p>Entities of different types are processed separately and will not be matched together.</p>"},{"location":"guides/configuration/#validation-queries","title":"Validation Queries","text":""},{"location":"guides/configuration/#check-configuration","title":"Check Configuration","text":"<pre><code>-- Verify source tables\nSELECT table_id, table_fqn, is_active\nFROM idr_meta.source_table;\n\n-- Verify rules\nSELECT rule_id, identifier_type, max_group_size, is_active\nFROM idr_meta.rule\nORDER BY priority;\n\n-- Verify mappings\nSELECT s.table_id, m.identifier_type, m.column_expr\nFROM idr_meta.source_table s\nJOIN idr_meta.identifier_mapping m ON s.table_id = m.table_id\nWHERE s.is_active = TRUE;\n\n-- Check for unmapped identifiers\nSELECT r.identifier_type\nFROM idr_meta.rule r\nLEFT JOIN idr_meta.identifier_mapping m ON r.identifier_type = m.identifier_type\nWHERE m.identifier_type IS NULL AND r.is_active = TRUE;\n</code></pre>"},{"location":"guides/configuration/#updating-configuration","title":"Updating Configuration","text":""},{"location":"guides/configuration/#adding-a-new-source","title":"Adding a New Source","text":"<pre><code>-- 1. Register source\nINSERT INTO idr_meta.source_table VALUES\n  ('new_source', 'schema.new_table', 'PERSON', 'id', 'updated_at', 0, TRUE);\n\n-- 2. Map identifiers\nINSERT INTO idr_meta.identifier_mapping VALUES\n  ('new_source', 'EMAIL', 'email_address', TRUE);\n\n-- 3. Run dry run to validate\n-- python idr_run.py --dry-run\n</code></pre>"},{"location":"guides/configuration/#disabling-a-source","title":"Disabling a Source","text":"<pre><code>UPDATE idr_meta.source_table \nSET is_active = FALSE \nWHERE table_id = 'old_source';\n</code></pre>"},{"location":"guides/configuration/#changing-max_group_size","title":"Changing max_group_size","text":"<pre><code>UPDATE idr_meta.rule \nSET max_group_size = 5000 \nWHERE rule_id = 'email_exact';\n</code></pre>"},{"location":"guides/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Dry Run Mode - Test your configuration</li> <li>Production Hardening - Optimize for production</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"guides/dry-run-mode/","title":"Dry Run Mode","text":"<p>Preview the impact of identity resolution changes before committing to production.</p>"},{"location":"guides/dry-run-mode/#why-dry-run","title":"Why Dry Run?","text":"<p>Production Safety</p> <p>Identity resolution changes can affect millions of records. A bad configuration could incorrectly merge unrelated entities or split valid clusters. Always dry run first.</p> <p>Dry run mode: - \u2705 Processes all entities and builds edges - \u2705 Runs label propagation algorithm - \u2705 Computes what changes would happen - \u274c Does NOT update production tables - \u274c Does NOT update watermarks</p>"},{"location":"guides/dry-run-mode/#enabling-dry-run","title":"Enabling Dry Run","text":"DuckDBSnowflakeBigQueryDatabricks <pre><code>python sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=FULL --dry-run\n</code></pre> <pre><code>CALL idr_run('FULL', 30, TRUE);  -- 3rd parameter = dry run\n</code></pre> <pre><code>python sql/bigquery/idr_run.py --project=my-project --dry-run\n</code></pre> <p>Set widget <code>DRY_RUN</code> to <code>true</code> before running.</p>"},{"location":"guides/dry-run-mode/#dry-run-output","title":"Dry Run Output","text":""},{"location":"guides/dry-run-mode/#console-summary","title":"Console Summary","text":"<pre><code>============================================================\nDRY RUN SUMMARY (No changes committed)\n============================================================\nRun ID:          dry_run_e8a7b3c2\nMode:            FULL (DRY RUN)\nDuration:        12s\nStatus:          DRY_RUN_COMPLETE\n\nIMPACT PREVIEW:\n  New Entities:      1,234\n  Moved Entities:    89\n  Unchanged:         45,678\n  Edges Would Create: 5,432\n  Largest Cluster:   523 entities\n\nREVIEW QUERIES:\n  \u2192 All changes:  SELECT * FROM idr_out.dry_run_results WHERE run_id = 'dry_run_e8a7b3c2'\n  \u2192 Moved only:   SELECT * FROM idr_out.dry_run_results WHERE run_id = 'dry_run_e8a7b3c2' AND change_type = 'MOVED'\n  \u2192 Summary:      SELECT * FROM idr_out.dry_run_summary WHERE run_id = 'dry_run_e8a7b3c2'\n\n\u26a0\ufe0f  THIS WAS A DRY RUN - NO CHANGES COMMITTED\n============================================================\n</code></pre>"},{"location":"guides/dry-run-mode/#output-tables","title":"Output Tables","text":"<p>Two tables are populated during dry runs:</p>"},{"location":"guides/dry-run-mode/#dry_run_results","title":"dry_run_results","text":"<p>Per-entity change details:</p> Column Description <code>entity_key</code> The entity identifier <code>current_resolved_id</code> Current cluster (NULL if new entity) <code>proposed_resolved_id</code> What the cluster would be <code>change_type</code> NEW, MOVED, or UNCHANGED <code>current_cluster_size</code> Current cluster size <code>proposed_cluster_size</code> Proposed cluster size"},{"location":"guides/dry-run-mode/#dry_run_summary","title":"dry_run_summary","text":"<p>Aggregate statistics:</p> Column Description <code>total_entities</code> Total entities analyzed <code>new_entities</code> Entities with no previous cluster <code>moved_entities</code> Entities changing clusters <code>unchanged_entities</code> Entities staying in same cluster <code>largest_proposed_cluster</code> Size of biggest proposed cluster"},{"location":"guides/dry-run-mode/#analyzing-dry-run-results","title":"Analyzing Dry Run Results","text":""},{"location":"guides/dry-run-mode/#view-all-changes","title":"View All Changes","text":"<pre><code>SELECT \n    entity_key,\n    change_type,\n    current_resolved_id,\n    proposed_resolved_id,\n    current_cluster_size,\n    proposed_cluster_size\nFROM idr_out.dry_run_results\nWHERE run_id = 'dry_run_e8a7b3c2'\nORDER BY change_type, entity_key;\n</code></pre>"},{"location":"guides/dry-run-mode/#focus-on-moved-entities","title":"Focus on Moved Entities","text":"<p>Moved entities are the most important to review - they represent entities changing clusters.</p> <pre><code>SELECT \n    entity_key,\n    current_resolved_id,\n    proposed_resolved_id,\n    current_cluster_size AS from_size,\n    proposed_cluster_size AS to_size\nFROM idr_out.dry_run_results\nWHERE run_id = 'dry_run_e8a7b3c2'\n  AND change_type = 'MOVED'\nORDER BY proposed_cluster_size DESC;\n</code></pre>"},{"location":"guides/dry-run-mode/#investigate-large-clusters","title":"Investigate Large Clusters","text":"<p>Check if any clusters are growing suspiciously large:</p> <pre><code>SELECT \n    proposed_resolved_id,\n    COUNT(*) AS entity_count,\n    SUM(CASE WHEN change_type = 'MOVED' THEN 1 ELSE 0 END) AS moved_in\nFROM idr_out.dry_run_results\nWHERE run_id = 'dry_run_e8a7b3c2'\nGROUP BY proposed_resolved_id\nHAVING COUNT(*) &gt; 100\nORDER BY entity_count DESC;\n</code></pre>"},{"location":"guides/dry-run-mode/#compare-beforeafter","title":"Compare Before/After","text":"<pre><code>WITH current_state AS (\n    SELECT resolved_id, COUNT(*) as current_size\n    FROM idr_out.identity_resolved_membership_current\n    GROUP BY resolved_id\n),\nproposed_state AS (\n    SELECT proposed_resolved_id AS resolved_id, COUNT(*) as proposed_size\n    FROM idr_out.dry_run_results\n    WHERE run_id = 'dry_run_e8a7b3c2'\n    GROUP BY proposed_resolved_id\n)\nSELECT \n    COALESCE(c.resolved_id, p.resolved_id) AS cluster,\n    COALESCE(c.current_size, 0) AS current_size,\n    COALESCE(p.proposed_size, 0) AS proposed_size,\n    COALESCE(p.proposed_size, 0) - COALESCE(c.current_size, 0) AS change\nFROM current_state c\nFULL OUTER JOIN proposed_state p ON c.resolved_id = p.resolved_id\nWHERE c.current_size != p.proposed_size\nORDER BY ABS(COALESCE(p.proposed_size, 0) - COALESCE(c.current_size, 0)) DESC;\n</code></pre>"},{"location":"guides/dry-run-mode/#decision-workflow","title":"Decision Workflow","text":"<pre><code>graph TD\n    A[Run Dry Run] --&gt; B{Review Summary}\n    B --&gt; C{New/Moved Count OK?}\n    C --&gt;|No, too many| D[Investigate Changes]\n    D --&gt; E{Find Issues?}\n    E --&gt;|Yes| F[Adjust Configuration]\n    F --&gt; A\n    E --&gt;|No, false alarm| G[Proceed to Live Run]\n    C --&gt;|Yes, expected| G\n    G --&gt; H[Run Without --dry-run]</code></pre>"},{"location":"guides/dry-run-mode/#common-issues-and-fixes","title":"Common Issues and Fixes","text":""},{"location":"guides/dry-run-mode/#too-many-entities-moving","title":"Too Many Entities Moving","text":"<p>Symptom: Large number of MOVED entities unexpectedly</p> <p>Possible causes: 1. New identifier type added that broadly matches 2. max_group_size too high</p> <p>Fix: <pre><code>-- Check which identifier types are causing merges\nSELECT \n    d.proposed_resolved_id,\n    COUNT(DISTINCT d.entity_key) as entities,\n    array_agg(DISTINCT i.identifier_type) as identifier_types\nFROM idr_out.dry_run_results d\nJOIN idr_work.edges_new e ON d.entity_key = e.entity_a OR d.entity_key = e.entity_b\nJOIN idr_work.identifiers i ON i.entity_key = d.entity_key\nWHERE d.run_id = 'dry_run_xxx' AND d.change_type = 'MOVED'\nGROUP BY d.proposed_resolved_id\nORDER BY entities DESC;\n</code></pre></p>"},{"location":"guides/dry-run-mode/#giant-cluster-forming","title":"Giant Cluster Forming","text":"<p>Symptom: One cluster with thousands of entities</p> <p>Fix: 1. Check for generic identifiers (test@test.com, 0000000000) 2. Add to exclusion list 3. Lower max_group_size for the identifier type</p> <pre><code>-- Find the culprit identifier\nSELECT identifier_type, identifier_value_norm, COUNT(*) as matches\nFROM idr_work.identifiers\nWHERE identifier_value_norm IN (\n    SELECT identifier_value_norm \n    FROM idr_work.identifiers \n    GROUP BY identifier_value_norm \n    HAVING COUNT(*) &gt; 1000\n)\nGROUP BY identifier_type, identifier_value_norm\nORDER BY matches DESC;\n</code></pre>"},{"location":"guides/dry-run-mode/#retention-and-cleanup","title":"Retention and Cleanup","text":"<p>Dry run results are retained based on configuration:</p> <pre><code>-- Check retention setting\nSELECT config_value FROM idr_meta.config \nWHERE config_key = 'dry_run_retention_days';\n\n-- Update retention (default: 7 days)\nUPDATE idr_meta.config \nSET config_value = '14' \nWHERE config_key = 'dry_run_retention_days';\n</code></pre> <p>Manual cleanup: <pre><code>DELETE FROM idr_out.dry_run_results \nWHERE created_at &lt; CURRENT_TIMESTAMP - INTERVAL '7 days';\n\nDELETE FROM idr_out.dry_run_summary \nWHERE created_at &lt; CURRENT_TIMESTAMP - INTERVAL '7 days';\n</code></pre></p>"},{"location":"guides/dry-run-mode/#best-practices","title":"Best Practices","text":"<ol> <li>Always dry run before production - Especially for first runs or configuration changes</li> <li>Review MOVED entities - These are the most likely to indicate issues</li> <li>Check large clusters - Investigate any cluster growing beyond expectations</li> <li>Keep dry run history - Useful for debugging later issues</li> <li>Automate dry run validation - Add assertions in CI/CD</li> </ol>"},{"location":"guides/dry-run-mode/#next-steps","title":"Next Steps","text":"<ul> <li>Production Hardening - max_group_size, exclusions</li> <li>Metrics &amp; Monitoring - Track dry run metrics</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"guides/metrics-monitoring/","title":"Metrics &amp; Monitoring","text":"<p>Set up observability for your identity resolution pipeline.</p>"},{"location":"guides/metrics-monitoring/#built-in-metrics","title":"Built-in Metrics","text":"<p>Every run automatically records metrics to <code>idr_out.metrics_export</code>:</p> Metric Name Type Description <code>idr_run_duration_seconds</code> gauge Total run duration <code>idr_entities_processed</code> gauge Entities processed this run <code>idr_edges_created</code> counter Edges created <code>idr_clusters_impacted</code> gauge Clusters affected <code>idr_lp_iterations</code> gauge Label propagation iterations <code>idr_groups_skipped</code> counter Groups skipped (max_group_size) <code>idr_large_clusters</code> gauge Clusters exceeding threshold"},{"location":"guides/metrics-monitoring/#querying-metrics","title":"Querying Metrics","text":""},{"location":"guides/metrics-monitoring/#view-recent-metrics","title":"View Recent Metrics","text":"<pre><code>SELECT \n    run_id,\n    metric_name,\n    metric_value,\n    metric_type,\n    recorded_at\nFROM idr_out.metrics_export\nWHERE run_id = 'run_xyz'\nORDER BY recorded_at;\n</code></pre>"},{"location":"guides/metrics-monitoring/#metrics-over-time","title":"Metrics Over Time","text":"<pre><code>SELECT \n    DATE(recorded_at) as date,\n    metric_name,\n    AVG(metric_value) as avg_value,\n    MAX(metric_value) as max_value\nFROM idr_out.metrics_export\nWHERE metric_name = 'idr_run_duration_seconds'\n  AND recorded_at &gt;= CURRENT_DATE - 30\nGROUP BY DATE(recorded_at), metric_name\nORDER BY date DESC;\n</code></pre>"},{"location":"guides/metrics-monitoring/#metrics-exporter","title":"Metrics Exporter","text":"<p>The <code>tools/metrics_exporter.py</code> script exports metrics to external systems.</p>"},{"location":"guides/metrics-monitoring/#installation","title":"Installation","text":"<pre><code>pip install requests prometheus-client\n</code></pre>"},{"location":"guides/metrics-monitoring/#usage","title":"Usage","text":"<pre><code># Export to stdout (debugging)\npython tools/metrics_exporter.py \\\n    --platform=duckdb \\\n    --connection=idr.duckdb \\\n    --exporter=stdout\n\n# Export to Prometheus (scrapeable endpoint)\npython tools/metrics_exporter.py \\\n    --platform=duckdb \\\n    --connection=idr.duckdb \\\n    --exporter=prometheus \\\n    --prometheus-port=9090\n\n# Export to DataDog\npython tools/metrics_exporter.py \\\n    --platform=duckdb \\\n    --connection=idr.duckdb \\\n    --exporter=datadog \\\n    --datadog-api-key=$DD_API_KEY\n\n# Export to webhook\npython tools/metrics_exporter.py \\\n    --platform=duckdb \\\n    --connection=idr.duckdb \\\n    --exporter=webhook \\\n    --webhook-url=https://hooks.slack.com/services/xxx\n</code></pre>"},{"location":"guides/metrics-monitoring/#prometheus-integration","title":"Prometheus Integration","text":""},{"location":"guides/metrics-monitoring/#start-exporter","title":"Start Exporter","text":"<pre><code>python tools/metrics_exporter.py \\\n    --platform=snowflake \\\n    --exporter=prometheus \\\n    --prometheus-port=9090\n</code></pre>"},{"location":"guides/metrics-monitoring/#prometheus-configuration","title":"Prometheus Configuration","text":"<pre><code># prometheus.yml\nscrape_configs:\n  - job_name: 'idr'\n    static_configs:\n      - targets: ['localhost:9090']\n    scrape_interval: 60s\n</code></pre>"},{"location":"guides/metrics-monitoring/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>Import the provided dashboard or create panels for:</p> <ul> <li>Run duration trend</li> <li>Entities processed per run</li> <li>Cluster growth over time</li> <li>Skipped groups alerts</li> </ul>"},{"location":"guides/metrics-monitoring/#datadog-integration","title":"DataDog Integration","text":""},{"location":"guides/metrics-monitoring/#export-metrics","title":"Export Metrics","text":"<pre><code>export DD_API_KEY=your_api_key\npython tools/metrics_exporter.py \\\n    --platform=bigquery \\\n    --project=my-project \\\n    --exporter=datadog\n</code></pre>"},{"location":"guides/metrics-monitoring/#datadog-dashboard","title":"DataDog Dashboard","text":"<p>Create monitors for:</p> <pre><code># Example monitor: High skipped groups\nname: \"IDR - High Skipped Groups\"\ntype: metric alert\nquery: \"avg(last_5m):sum:idr.groups_skipped{*} &gt; 100\"\nmessage: \"More than 100 identifier groups skipped. Review max_group_size settings.\"\n\n# Example monitor: Long run duration\nname: \"IDR - Slow Run\"\ntype: metric alert\nquery: \"avg(last_1h):max:idr.run_duration_seconds{*} &gt; 3600\"\nmessage: \"IDR run took over 1 hour. Investigate performance.\"\n</code></pre>"},{"location":"guides/metrics-monitoring/#alerting","title":"Alerting","text":""},{"location":"guides/metrics-monitoring/#sql-based-alerts","title":"SQL-Based Alerts","text":"<p>Run these queries periodically and alert on non-empty results:</p> <pre><code>-- Alert: Run failures\nSELECT run_id, status, ended_at\nFROM idr_out.run_history\nWHERE status = 'FAILED'\n  AND ended_at &gt;= CURRENT_TIMESTAMP - INTERVAL '1 hour';\n\n-- Alert: High skipped groups\nSELECT run_id, groups_skipped\nFROM idr_out.run_history\nWHERE groups_skipped &gt; 100\n  AND ended_at &gt;= CURRENT_TIMESTAMP - INTERVAL '1 hour';\n\n-- Alert: Giant clusters forming\nSELECT resolved_id, cluster_size\nFROM idr_out.identity_clusters_current\nWHERE cluster_size &gt; 10000;\n</code></pre>"},{"location":"guides/metrics-monitoring/#platform-specific-alerts","title":"Platform-Specific Alerts","text":"SnowflakeBigQueryDatabricks <pre><code>-- Use Snowflake Alerts (Snowsight)\nCREATE ALERT idr_failure_alert\n  WAREHOUSE = compute_wh\n  SCHEDULE = '1 MINUTE'\n  IF (EXISTS (\n    SELECT 1 FROM idr_out.run_history \n    WHERE status = 'FAILED' \n      AND ended_at &gt;= CURRENT_TIMESTAMP - INTERVAL '5 minutes'\n  ))\n  THEN CALL SYSTEM$SEND_EMAIL(...);\n</code></pre> <pre><code>-- Use Cloud Monitoring alert policies\n-- Create log-based metric from run_history inserts\n</code></pre> <pre><code># Use Databricks SQL Alerts\n# Configure in Databricks SQL workspace\n</code></pre>"},{"location":"guides/metrics-monitoring/#dashboards","title":"Dashboards","text":""},{"location":"guides/metrics-monitoring/#key-metrics-to-display","title":"Key Metrics to Display","text":"Panel Query Run Success Rate <code>COUNT(status='SUCCESS') / COUNT(*)</code> Average Duration <code>AVG(duration_seconds)</code> Entities Per Run <code>AVG(entities_processed)</code> Cluster Growth <code>MAX(cluster_size) over time</code> Skipped Groups Trend <code>SUM(groups_skipped) by date</code>"},{"location":"guides/metrics-monitoring/#sample-sql-dashboard-query","title":"Sample SQL Dashboard Query","text":"<pre><code>WITH run_stats AS (\n    SELECT \n        DATE(started_at) as run_date,\n        COUNT(*) as runs,\n        SUM(CASE WHEN status LIKE 'SUCCESS%' THEN 1 ELSE 0 END) as successful,\n        AVG(duration_seconds) as avg_duration,\n        SUM(entities_processed) as total_entities,\n        SUM(groups_skipped) as total_skipped\n    FROM idr_out.run_history\n    WHERE started_at &gt;= CURRENT_DATE - 30\n    GROUP BY DATE(started_at)\n)\nSELECT \n    run_date,\n    runs,\n    ROUND(100.0 * successful / runs, 1) as success_rate,\n    ROUND(avg_duration, 0) as avg_duration_sec,\n    total_entities,\n    total_skipped\nFROM run_stats\nORDER BY run_date DESC;\n</code></pre>"},{"location":"guides/metrics-monitoring/#health-checks","title":"Health Checks","text":""},{"location":"guides/metrics-monitoring/#pre-run-health-check","title":"Pre-Run Health Check","text":"<pre><code>-- Check sources are accessible\nSELECT table_id, table_fqn\nFROM idr_meta.source_table\nWHERE is_active = TRUE;\n\n-- Check for stale watermarks (no runs in 24h)\nSELECT table_id, last_run_ts\nFROM idr_meta.run_state\nWHERE last_run_ts &lt; CURRENT_TIMESTAMP - INTERVAL '24 hours';\n</code></pre>"},{"location":"guides/metrics-monitoring/#post-run-validation","title":"Post-Run Validation","text":"<pre><code>-- Verify outputs populated\nSELECT \n    'membership' as table_name, \n    COUNT(*) as row_count \nFROM idr_out.identity_resolved_membership_current\nUNION ALL\nSELECT \n    'clusters', \n    COUNT(*) \nFROM idr_out.identity_clusters_current;\n\n-- Check for orphaned clusters\nSELECT COUNT(*) as orphaned\nFROM idr_out.identity_clusters_current c\nLEFT JOIN idr_out.identity_resolved_membership_current m \n    ON c.resolved_id = m.resolved_id\nWHERE m.resolved_id IS NULL;\n</code></pre>"},{"location":"guides/metrics-monitoring/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting</li> <li>CI/CD</li> <li>Production Hardening</li> </ul>"},{"location":"guides/production-bigquery/","title":"Production Deployment: BigQuery","text":"<p>This guide details the exact steps to deploy SQL Identity Resolution (IDR) to a production Google BigQuery environment.</p>"},{"location":"guides/production-bigquery/#prerequisites","title":"Prerequisites","text":"<ul> <li>GCP Project: A Google Cloud Project with BigQuery API enabled.</li> <li>Service Account: A service account with <code>BigQuery Admin</code> or <code>BigQuery Data Editor</code> + <code>BigQuery Job User</code> roles.</li> <li>Python Environment: For running <code>load_metadata.py</code> and <code>idr_run.py</code>.</li> </ul>"},{"location":"guides/production-bigquery/#step-1-schema-setup","title":"Step 1: Schema Setup","text":""},{"location":"guides/production-bigquery/#11-create-datasets","title":"1.1 Create Datasets","text":"<p>Create the three required datasets in your project.</p> <pre><code>export PROJECT_ID=\"your-project-id\"\n\nbq mk --dataset ${PROJECT_ID}:idr_meta\nbq mk --dataset ${PROJECT_ID}:idr_work\nbq mk --dataset ${PROJECT_ID}:idr_out\n</code></pre>"},{"location":"guides/production-bigquery/#12-create-tables","title":"1.2 Create Tables","text":"<p>Execute the DDL script to create all necessary tables.</p> <pre><code>bq query --use_legacy_sql=false &lt; sql/bigquery/core/00_ddl_all.sql\n</code></pre>"},{"location":"guides/production-bigquery/#step-2-configuration","title":"Step 2: Configuration","text":"<p>Create a <code>production.yaml</code> file defining your rules and sources.</p> <p>Example <code>production.yaml</code>: <pre><code>rules:\n  - rule_id: email_exact\n    identifier_type: EMAIL\n    priority: 1\n    settings:\n      canonicalize: LOWERCASE\n\nsources:\n  - table_id: ga4_events\n    table_fqn: your-project-id.analytics_123456.events_*\n    entity_key_expr: user_pseudo_id\n    trust_rank: 2\n    identifiers:\n      - type: COOKIE\n        expr: user_pseudo_id\n</code></pre></p>"},{"location":"guides/production-bigquery/#step-3-metadata-loading","title":"Step 3: Metadata Loading","text":"<p>Use the <code>load_metadata.py</code> tool to push your configuration to BigQuery.</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/key.json\"\n\npython tools/load_metadata.py \\\n  --platform=bigquery \\\n  --project=your-project-id \\\n  --config=production.yaml\n</code></pre>"},{"location":"guides/production-bigquery/#step-4-execution-scheduling","title":"Step 4: Execution &amp; Scheduling","text":"<p>The IDR process is a Python script (<code>sql/bigquery/core/idr_run.py</code>) that orchestrates BigQuery jobs.</p>"},{"location":"guides/production-bigquery/#option-a-cloud-run-jobs-recommended","title":"Option A: Cloud Run Jobs (Recommended)","text":"<ol> <li>Containerize the application (Dockerfile provided in repo).</li> <li>Deploy to Cloud Run Jobs or Google Kubernetes Engine (GKE).</li> <li>Schedule with Cloud Scheduler.</li> </ol> <p>Command: <pre><code>python sql/bigquery/core/idr_run.py \\\n  --project=your-project-id \\\n  --run-mode=FULL\n</code></pre></p>"},{"location":"guides/production-bigquery/#option-b-cloud-composer-airflow","title":"Option B: Cloud Composer (Airflow)","text":"<p>Use the <code>BashOperator</code> or <code>KubernetesPodOperator</code> to run the script.</p> <pre><code>run_idr = BashOperator(\n    task_id='run_idr',\n    bash_command='python sql/bigquery/core/idr_run.py --project={{ var.value.gcp_project }} --run-mode=FULL',\n    dag=dag\n)\n</code></pre>"},{"location":"guides/production-bigquery/#step-5-monitoring","title":"Step 5: Monitoring","text":"<p>Monitor the pipeline using the <code>idr_out</code> tables.</p> <p>Check Run History: <pre><code>SELECT run_id, status, duration_seconds, entities_processed \nFROM `your-project-id.idr_out.run_history` \nORDER BY started_at DESC \nLIMIT 10;\n</code></pre></p>"},{"location":"guides/production-databricks/","title":"Production Deployment: Databricks","text":"<p>This guide details the exact steps to deploy SQL Identity Resolution (IDR) to a production Databricks environment.</p>"},{"location":"guides/production-databricks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Databricks Workspace: Azure Databricks, AWS Databricks, or GCP Databricks.</li> <li>Unity Catalog (Recommended): For best governance, though Hive Metastore is supported.</li> <li>SQL Warehouse: Required for running SQL workloads.</li> </ul>"},{"location":"guides/production-databricks/#step-1-schema-setup","title":"Step 1: Schema Setup","text":""},{"location":"guides/production-databricks/#11-import-notebooks","title":"1.1 Import Notebooks","text":"<p>Import the following notebooks into your workspace (e.g., <code>/Shared/IDR/core/</code>): - <code>sql/databricks/core/00_ddl_all.sql</code> (can be run as a notebook or query) - <code>sql/databricks/core/IDR_Run.py</code></p>"},{"location":"guides/production-databricks/#12-create-schemas-tables","title":"1.2 Create Schemas &amp; Tables","text":"<p>Run the DDL SQL to set up the environment.</p> <pre><code>%sql\n-- In a SQL Notebook or SQL Editor\nCREATE SCHEMA IF NOT EXISTS idr_meta;\nCREATE SCHEMA IF NOT EXISTS idr_work;\nCREATE SCHEMA IF NOT EXISTS idr_out;\n\n-- Run the contents of 00_ddl_all.sql here or %run it\n</code></pre>"},{"location":"guides/production-databricks/#step-2-configuration","title":"Step 2: Configuration","text":"<p>Create a <code>production.yaml</code> file defining your rules and sources.</p> <p>Example <code>production.yaml</code>: <pre><code>rules:\n  - rule_id: email_exact\n    identifier_type: EMAIL\n    priority: 1\n    settings:\n      canonicalize: LOWERCASE\n\nsources:\n  - table_id: delta_customers\n    table_fqn: catalog.schema.customers\n    entity_key_expr: id\n    identifiers:\n      - type: EMAIL\n        expr: email\n</code></pre></p>"},{"location":"guides/production-databricks/#step-3-metadata-loading","title":"Step 3: Metadata Loading","text":"<p>Use the <code>load_metadata.py</code> tool to push your configuration. You can run this from your laptop or a CI/CD pipeline, connecting via Databricks SQL Connector.</p> <pre><code>export DATABRICKS_HOST=\"https://adb-1234.5.azuredatabricks.net\"\nexport DATABRICKS_HTTP_PATH=\"/sql/1.0/warehouses/...\"\nexport DATABRICKS_TOKEN=\"dapi...\"\n\npython tools/load_metadata.py \\\n  --platform=databricks \\\n  --config=production.yaml\n</code></pre>"},{"location":"guides/production-databricks/#step-4-execution-scheduling","title":"Step 4: Execution &amp; Scheduling","text":"<p>The IDR process is a PySpark notebook (<code>IDR_Run.py</code>).</p>"},{"location":"guides/production-databricks/#option-a-databricks-workflows-recommended","title":"Option A: Databricks Workflows (Recommended)","text":"<ol> <li>Create a new Job.</li> <li>Task type: Notebook.</li> <li>Path: <code>/Shared/IDR/core/IDR_Run</code>.</li> <li>Parameters:<ul> <li><code>RUN_MODE</code>: <code>FULL</code> (or <code>INCR</code>)</li> <li><code>MAX_ITERS</code>: <code>30</code></li> <li><code>DRY_RUN</code>: <code>false</code></li> </ul> </li> </ol> <p>Schedule this job to run daily or hourly.</p>"},{"location":"guides/production-databricks/#option-b-dbt","title":"Option B: dbt","text":"<p>If using dbt-databricks, you can wrap the logic in a dbt model or use a pre-hook to call the notebook if supported, though Databricks Workflows is the native path.</p>"},{"location":"guides/production-databricks/#step-5-monitoring","title":"Step 5: Monitoring","text":"<p>Monitor the pipeline using the <code>idr_out</code> tables.</p> <p>Check Run History: <pre><code>SELECT run_id, status, duration_seconds \nFROM idr_out.run_history \nORDER BY started_at DESC;\n</code></pre></p>"},{"location":"guides/production-duckdb/","title":"Production Deployment: DuckDB","text":"<p>While DuckDB is primarily embedded, it can be used in \"production\" for single-node analytical workloads, data apps (Streamlit/Rill), or generating extracts for other systems.</p>"},{"location":"guides/production-duckdb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python Environment: Python 3.9+.</li> <li>Persistent Storage: A location for the <code>.duckdb</code> file (e.g., EBS volume, local disk).</li> </ul>"},{"location":"guides/production-duckdb/#step-1-schema-setup","title":"Step 1: Schema Setup","text":"<p>Initialize the database file with the schema.</p> <pre><code># Initialize persisted database\nduckdb proddb.duckdb &lt; sql/duckdb/core/00_ddl_all.sql\n</code></pre>"},{"location":"guides/production-duckdb/#step-2-configuration","title":"Step 2: Configuration","text":"<p>Create a <code>production.yaml</code> config file.</p> <pre><code>rules:\n  - rule_id: email_exact\n    identifier_type: EMAIL\n    settings:\n      canonicalize: LOWERCASE\nsources:\n  - table_id: local_csv\n    table_fqn: \"read_csv_auto('data/*.csv')\"\n    entity_key_expr: user_id\n    identifiers:\n      - type: EMAIL\n        expr: email\n</code></pre>"},{"location":"guides/production-duckdb/#step-3-metadata-loading","title":"Step 3: Metadata Loading","text":"<p>Load configuration into the specific database file.</p> <pre><code>python tools/load_metadata.py \\\n  --platform=duckdb \\\n  --db=proddb.duckdb \\\n  --config=production.yaml\n</code></pre>"},{"location":"guides/production-duckdb/#step-4-execution-scheduling","title":"Step 4: Execution &amp; Scheduling","text":"<p>Run the <code>idr_run.py</code> script pointing to your production database file.</p>"},{"location":"guides/production-duckdb/#cron-job","title":"Cron Job","text":"<pre><code># Run daily at 3 AM\n0 3 * * * python sql/duckdb/core/idr_run.py --db=/path/to/proddb.duckdb --run-mode=FULL &gt;&gt; /var/log/idr.log 2&gt;&amp;1\n</code></pre>"},{"location":"guides/production-duckdb/#running-as-a-library","title":"Running as a Library","text":"<p>You can also import the logic if you refactor <code>idr_run.py</code> to be callable, allowing you to embed identity resolution directly in your FastAPI or Flask app.</p>"},{"location":"guides/production-duckdb/#step-5-consuming-results","title":"Step 5: Consuming Results","text":"<p>You can query the results directly using the DuckDB CLI or Python client.</p> <pre><code>duckdb proddb.duckdb \"SELECT * FROM idr_out.golden_profile_current LIMIT 5\"\n</code></pre>"},{"location":"guides/production-hardening/","title":"Production Hardening","text":"<p>Best practices for running SQL Identity Resolution in production environments.</p>"},{"location":"guides/production-hardening/#data-quality-controls","title":"Data Quality Controls","text":""},{"location":"guides/production-hardening/#max_group_size","title":"max_group_size","text":"<p>Prevents generic identifiers from creating mega-clusters.</p> <pre><code>-- Set appropriate limits\nUPDATE idr_meta.rule SET max_group_size = 10000 WHERE identifier_type = 'EMAIL';\nUPDATE idr_meta.rule SET max_group_size = 5000 WHERE identifier_type = 'PHONE';\nUPDATE idr_meta.rule SET max_group_size = 1 WHERE identifier_type = 'SSN';\n</code></pre> <p>What happens when exceeded: 1. Identifier group is skipped 2. Entities become singletons (resolved_id = entity_key) 3. Logged to <code>idr_out.skipped_identifier_groups</code></p> <p>Review skipped groups: <pre><code>SELECT \n    identifier_type,\n    identifier_value_norm,\n    group_size,\n    max_allowed,\n    sample_entity_keys\nFROM idr_out.skipped_identifier_groups\nWHERE run_id = 'run_xyz'\nORDER BY group_size DESC;\n</code></pre></p>"},{"location":"guides/production-hardening/#identifier-exclusions","title":"Identifier Exclusions","text":"<p>Block known bad identifiers:</p> <pre><code>-- Exact matches\nINSERT INTO idr_meta.identifier_exclusion VALUES\n  ('EMAIL', 'test@test.com', FALSE, 'Generic test'),\n  ('EMAIL', 'null@null.com', FALSE, 'Null placeholder'),\n  ('PHONE', '0000000000', FALSE, 'Invalid');\n\n-- Patterns (LIKE syntax)\nINSERT INTO idr_meta.identifier_exclusion VALUES\n  ('EMAIL', '%@example.com', TRUE, 'Example domain'),\n  ('EMAIL', 'noreply@%', TRUE, 'No-reply'),\n  ('EMAIL', '%@mailinator.%', TRUE, 'Disposable');\n</code></pre>"},{"location":"guides/production-hardening/#large-cluster-monitoring","title":"Large Cluster Monitoring","text":""},{"location":"guides/production-hardening/#configure-threshold","title":"Configure Threshold","text":"<pre><code>INSERT INTO idr_meta.config VALUES\n  ('large_cluster_threshold', '5000', 'Warn on clusters larger than this', NOW());\n</code></pre>"},{"location":"guides/production-hardening/#monitor-large-clusters","title":"Monitor Large Clusters","text":"<pre><code>-- Current large clusters\nSELECT resolved_id, cluster_size\nFROM idr_out.identity_clusters_current\nWHERE cluster_size &gt;= 5000\nORDER BY cluster_size DESC;\n\n-- Growth over time\nSELECT \n    DATE(updated_ts) as date,\n    COUNT(*) as large_cluster_count,\n    MAX(cluster_size) as max_size\nFROM idr_out.identity_clusters_current\nWHERE cluster_size &gt;= 5000\nGROUP BY DATE(updated_ts)\nORDER BY date DESC;\n</code></pre>"},{"location":"guides/production-hardening/#alerting","title":"Alerting","text":"<p>Run warnings appear in <code>idr_out.run_history</code>:</p> <pre><code>SELECT \n    run_id,\n    status,\n    large_clusters,\n    groups_skipped,\n    warnings\nFROM idr_out.run_history\nWHERE status = 'SUCCESS_WITH_WARNINGS'\nORDER BY started_at DESC;\n</code></pre>"},{"location":"guides/production-hardening/#incremental-processing","title":"Incremental Processing","text":""},{"location":"guides/production-hardening/#use-incr-mode","title":"Use INCR Mode","text":"<p>After initial FULL run, use INCR for efficiency:</p> <pre><code># First time\npython idr_run.py --run-mode=FULL\n\n# Subsequent runs\npython idr_run.py --run-mode=INCR\n</code></pre>"},{"location":"guides/production-hardening/#watermark-management","title":"Watermark Management","text":"<pre><code>-- Check watermark status\nSELECT \n    table_id,\n    last_watermark_value,\n    last_run_id,\n    last_run_ts\nFROM idr_meta.run_state;\n\n-- Reset watermark (force reprocess)\nUPDATE idr_meta.run_state \nSET last_watermark_value = '1900-01-01'::TIMESTAMP\nWHERE table_id = 'customers';\n</code></pre>"},{"location":"guides/production-hardening/#lookback-buffer","title":"Lookback Buffer","text":"<p>For late-arriving data:</p> <pre><code>UPDATE idr_meta.source_table \nSET watermark_lookback_minutes = 60  -- 1 hour buffer\nWHERE table_id = 'customers';\n</code></pre>"},{"location":"guides/production-hardening/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/production-hardening/#index-source-tables","title":"Index Source Tables","text":"<pre><code>-- DuckDB\nCREATE INDEX idx_customers_updated ON customers(updated_at);\nCREATE INDEX idx_customers_email ON customers(LOWER(email));\n\n-- Snowflake (clustering)\nALTER TABLE customers CLUSTER BY (updated_at);\n\n-- BigQuery (partitioning)\nCREATE TABLE customers\nPARTITION BY DATE(updated_at)\nAS SELECT * FROM raw_customers;\n</code></pre>"},{"location":"guides/production-hardening/#limit-lp-iterations","title":"Limit LP Iterations","text":"<p>For very large graphs, reduce max iterations:</p> <pre><code>python idr_run.py --max-iters=20  # Default: 30\n</code></pre>"},{"location":"guides/production-hardening/#parallel-processing","title":"Parallel Processing","text":"SnowflakeBigQueryDatabricks <pre><code>-- Use larger warehouse\nALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'LARGE';\nCALL idr_run('INCR', 30, FALSE);\n</code></pre> <pre><code># Slot reservations reduce processing time\n# Configure in BigQuery Console\n</code></pre> <pre><code># Use larger cluster\n# Enable autoscaling\n</code></pre>"},{"location":"guides/production-hardening/#audit-trail","title":"Audit Trail","text":""},{"location":"guides/production-hardening/#run-history","title":"Run History","text":"<p>Every run is logged:</p> <pre><code>SELECT \n    run_id,\n    run_mode,\n    started_at,\n    duration_seconds,\n    entities_processed,\n    edges_created,\n    clusters_impacted,\n    status,\n    warnings\nFROM idr_out.run_history\nORDER BY started_at DESC\nLIMIT 20;\n</code></pre>"},{"location":"guides/production-hardening/#skipped-groups-audit","title":"Skipped Groups Audit","text":"<pre><code>SELECT *\nFROM idr_out.skipped_identifier_groups\nWHERE run_id = 'run_xyz';\n</code></pre>"},{"location":"guides/production-hardening/#stage-metrics","title":"Stage Metrics","text":"<pre><code>SELECT \n    stage_name,\n    rows_affected,\n    duration_seconds\nFROM idr_out.stage_metrics\nWHERE run_id = 'run_xyz'\nORDER BY started_at;\n</code></pre>"},{"location":"guides/production-hardening/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"guides/production-hardening/#backup-configuration","title":"Backup Configuration","text":"<pre><code>-- Export configuration\nCREATE TABLE backup.source_table AS SELECT * FROM idr_meta.source_table;\nCREATE TABLE backup.rule AS SELECT * FROM idr_meta.rule;\nCREATE TABLE backup.identifier_mapping AS SELECT * FROM idr_meta.identifier_mapping;\n</code></pre>"},{"location":"guides/production-hardening/#rollback-procedure","title":"Rollback Procedure","text":"<p>If a bad run needs to be rolled back:</p> <ol> <li>Stop any scheduled jobs</li> <li>Identify the bad run_id</li> <li>Reset watermarks (if needed):    <pre><code>UPDATE idr_meta.run_state \nSET last_watermark_value = (\n    SELECT last_watermark_value \n    FROM idr_out.run_history \n    WHERE run_id = 'previous_good_run'\n);\n</code></pre></li> <li>Restore from backup (if available)</li> <li>Re-run with corrected configuration</li> </ol>"},{"location":"guides/production-hardening/#security-best-practices","title":"Security Best Practices","text":""},{"location":"guides/production-hardening/#least-privilege","title":"Least Privilege","text":"<p>Create dedicated roles:</p> SnowflakeBigQuery <pre><code>CREATE ROLE IDR_EXECUTOR;\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE IDR_EXECUTOR;\nGRANT SELECT ON ALL TABLES IN SCHEMA crm TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_meta TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_work TO ROLE IDR_EXECUTOR;\nGRANT ALL ON SCHEMA idr_out TO ROLE IDR_EXECUTOR;\n</code></pre> <pre><code># Create service account with minimal permissions\ngcloud iam service-accounts create idr-runner\n\n# Grant BigQuery Job User + specific dataset access\nbq query --use_legacy_sql=false \\\n  \"GRANT \\`roles/bigquery.dataEditor\\` ON SCHEMA idr_out TO 'serviceAccount:idr-runner@project.iam.gserviceaccount.com'\"\n</code></pre>"},{"location":"guides/production-hardening/#secrets-management","title":"Secrets Management","text":"<ul> <li>Never hardcode credentials in scripts</li> <li>Use environment variables or secret managers</li> <li>Rotate credentials regularly</li> </ul>"},{"location":"guides/production-hardening/#pre-production-checklist","title":"Pre-Production Checklist","text":"<ul> <li>[ ] All source tables registered and active</li> <li>[ ] Identifier mappings complete</li> <li>[ ] max_group_size configured appropriately</li> <li>[ ] Known bad identifiers excluded</li> <li>[ ] Dry run completed successfully</li> <li>[ ] Large cluster threshold set</li> <li>[ ] Monitoring/alerting configured</li> <li>[ ] Backup procedures documented</li> <li>[ ] Access controls verified</li> <li>[ ] Scheduling configured</li> </ul>"},{"location":"guides/production-hardening/#next-steps","title":"Next Steps","text":"<ul> <li>Metrics &amp; Monitoring</li> <li>Troubleshooting</li> <li>CI/CD</li> </ul>"},{"location":"guides/production-snowflake/","title":"Production Deployment: Snowflake","text":"<p>This guide details the exact steps to deploy SQL Identity Resolution (IDR) to a production Snowflake environment.</p>"},{"location":"guides/production-snowflake/#prerequisites","title":"Prerequisites","text":"<ul> <li>Snowflake Account: Usage of <code>ACCOUNTADMIN</code> or a role with <code>CREATE DATABASE/SCHEMA</code> privileges.</li> <li>Python Environment: For running the metadata loader (CI/CD or orchestration server).</li> <li>Source Data: Read access to the tables you wish to resolve.</li> </ul>"},{"location":"guides/production-snowflake/#step-1-schema-setup","title":"Step 1: Schema Setup","text":"<p>You need to create the tables and the stored procedure runner.</p>"},{"location":"guides/production-snowflake/#11-create-tables","title":"1.1 Create Tables","text":"<p>Execute the content of <code>sql/snowflake/core/00_ddl_all.sql</code>. This creates: - <code>idr_meta</code>: Configuration and rules. - <code>idr_work</code>: Transient intermediate tables. - <code>idr_out</code>: Final output tables (clusters, golden profiles).</p>"},{"location":"guides/production-snowflake/#12-deploy-stored-procedure","title":"1.2 Deploy Stored Procedure","text":"<p>Execute the content of <code>sql/snowflake/core/IDR_Run.sql</code>. This creates the <code>idr_run</code> stored procedure, which contains the entire resolution logic.</p> <pre><code>-- Verify deployment\nSHOW PROCEDURES LIKE 'idr_run';\n</code></pre>"},{"location":"guides/production-snowflake/#step-2-configuration","title":"Step 2: Configuration","text":"<p>Create a <code>production.yaml</code> file defining your rules and sources.</p> <p>Example <code>production.yaml</code>: <pre><code>rules:\n  - rule_id: email_exact\n    identifier_type: EMAIL\n    priority: 1\n    settings:\n      canonicalize: LOWERCASE\n\nsources:\n  - table_id: crm_customers\n    table_fqn: RAW.CRM.CUSTOMERS\n    entity_key_expr: customer_id\n    trust_rank: 1\n    identifiers:\n      - type: EMAIL\n        expr: email_address\n    attributes:\n      first_name: fname\n      last_name: lname\n</code></pre></p>"},{"location":"guides/production-snowflake/#step-3-metadata-loading","title":"Step 3: Metadata Loading","text":"<p>Use the <code>load_metadata.py</code> tool to push your configuration to Snowflake. This ensures your config is version-controlled and deployed safely.</p> <p>Run via CI/CD (GitHub Actions, Jenkins) or Airflow:</p> <pre><code>export SNOWFLAKE_ACCOUNT=\"your_account\"\nexport SNOWFLAKE_USER=\"idr_service_user\"\nexport SNOWFLAKE_PASSWORD=\"***\"\nexport SNOWFLAKE_DATABASE=\"IDR_PROD\"\nexport SNOWFLAKE_WAREHOUSE=\"COMPUTE_WH\"\n\npython tools/load_metadata.py \\\n  --platform=snowflake \\\n  --config=production.yaml\n</code></pre>"},{"location":"guides/production-snowflake/#step-4-execution-scheduling","title":"Step 4: Execution &amp; Scheduling","text":"<p>The IDR process is triggered by calling the stored procedure. You can schedule this using Snowflake Tasks or an external orchestrator (Airflow, Prefect, dbt).</p>"},{"location":"guides/production-snowflake/#option-a-snowflake-tasks-recommended","title":"Option A: Snowflake Tasks (Recommended)","text":"<pre><code>CREATE OR REPLACE TASK run_idr_daily\n  WAREHOUSE = COMPUTE_WH\n  SCHEDULE = 'USING CRON 0 2 * * * America/Los_Angeles' -- Daily at 2AM\nAS\n  CALL idr_run('FULL', 30, FALSE);\n</code></pre>"},{"location":"guides/production-snowflake/#option-b-dbt","title":"Option B: dbt","text":"<p>You can use a <code>run-operation</code> in dbt:</p> <pre><code>-- macros/run_idr.sql\n{% macro run_idr() %}\n  {% do run_query(\"CALL idr_run('FULL', 30, FALSE)\") %}\n{% endmacro %}\n</code></pre>"},{"location":"guides/production-snowflake/#step-5-monitoring","title":"Step 5: Monitoring","text":"<p>Monitor the pipeline using the <code>idr_out</code> tables.</p> <p>Check Run Status: <pre><code>SELECT run_id, status, duration_seconds, entities_processed, clusters_impacted \nFROM idr_out.run_history \nORDER BY started_at DESC \nLIMIT 10;\n</code></pre></p> <p>Check for Warnings: <pre><code>SELECT * FROM idr_out.run_history \nWHERE warnings IS NOT NULL \nORDER BY started_at DESC;\n</code></pre></p>"},{"location":"guides/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions for SQL Identity Resolution.</p>"},{"location":"guides/troubleshooting/#run-failures","title":"Run Failures","text":""},{"location":"guides/troubleshooting/#no-active-source-tables-found","title":"\"No active source tables found\"","text":"<p>Symptom: Run fails immediately with this error.</p> <p>Cause: No tables registered or all disabled.</p> <p>Solution: <pre><code>-- Check what's registered\nSELECT * FROM idr_meta.source_table;\n\n-- Ensure at least one is active\nUPDATE idr_meta.source_table SET is_active = TRUE WHERE table_id = 'customers';\n</code></pre></p>"},{"location":"guides/troubleshooting/#source-table-does-not-exist","title":"\"Source table does not exist\"","text":"<p>Symptom: Run fails during entity extraction.</p> <p>Cause: <code>table_fqn</code> in source_table doesn't match actual table.</p> <p>Solution: <pre><code>-- Verify table exists\nSELECT * FROM information_schema.tables \nWHERE table_schema = 'crm' AND table_name = 'customers';\n\n-- Update if incorrect\nUPDATE idr_meta.source_table \nSET table_fqn = 'crm.customers' \nWHERE table_id = 'customers';\n</code></pre></p>"},{"location":"guides/troubleshooting/#no-edges-created","title":"\"No edges created\"","text":"<p>Symptom: Run completes but no matching happens.</p> <p>Causes: 1. No identifier mappings defined 2. All identifiers are NULL 3. All groups exceed max_group_size</p> <p>Diagnosis: <pre><code>-- Check mappings exist\nSELECT * FROM idr_meta.identifier_mapping;\n\n-- Check if identifiers extracted\nSELECT COUNT(*) FROM idr_work.identifiers;\n\n-- Check if groups were skipped\nSELECT COUNT(*) FROM idr_out.skipped_identifier_groups \nWHERE run_id = 'run_xyz';\n</code></pre></p>"},{"location":"guides/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"guides/troubleshooting/#run-takes-too-long","title":"Run takes too long","text":"<p>Causes: 1. Too many iterations 2. Large data volume 3. Missing indexes</p> <p>Solutions:</p> <pre><code># Reduce max iterations\npython idr_run.py --max-iters=20\n</code></pre> <pre><code>-- Add indexes on source tables\nCREATE INDEX idx_updated ON customers(updated_at);\n\n-- Use incremental mode\n-- (after first FULL run)\n</code></pre>"},{"location":"guides/troubleshooting/#out-of-memory","title":"Out of memory","text":"<p>Causes: 1. Giant clusters 2. Too much data in single run</p> <p>Solutions: <pre><code>-- Lower max_group_size\nUPDATE idr_meta.rule SET max_group_size = 5000;\n\n-- Process in batches (reduce lookback)\nUPDATE idr_meta.source_table \nSET watermark_lookback_minutes = 0;\n</code></pre></p>"},{"location":"guides/troubleshooting/#data-quality-issues","title":"Data Quality Issues","text":""},{"location":"guides/troubleshooting/#giant-cluster-forming","title":"Giant cluster forming","text":"<p>Symptom: One cluster has thousands of entities.</p> <p>Diagnosis: <pre><code>-- Find the giant cluster\nSELECT resolved_id, cluster_size\nFROM idr_out.identity_clusters_current\nORDER BY cluster_size DESC\nLIMIT 5;\n\n-- Find what's connecting them\nSELECT \n    i.identifier_type,\n    i.identifier_value_norm,\n    COUNT(DISTINCT i.entity_key) as entity_count\nFROM idr_out.identity_resolved_membership_current m\nJOIN idr_work.identifiers i ON m.entity_key = i.entity_key\nWHERE m.resolved_id = 'giant_cluster_id'\nGROUP BY i.identifier_type, i.identifier_value_norm\nORDER BY entity_count DESC;\n</code></pre></p> <p>Solution: 1. Add the problematic identifier to exclusion list 2. Lower max_group_size for that identifier type</p>"},{"location":"guides/troubleshooting/#entities-not-matching","title":"Entities not matching","text":"<p>Symptom: Entities with same identifier are in different clusters.</p> <p>Diagnosis: <pre><code>-- Check if identifiers match\nSELECT entity_key, identifier_value_norm\nFROM idr_work.identifiers\nWHERE entity_key IN ('entity_a', 'entity_b');\n\n-- Check normalization\n-- Are they exactly equal?\n</code></pre></p> <p>Causes: 1. Normalization issues (different whitespace, case) 2. Character encoding differences 3. max_group_size exceeded</p>"},{"location":"guides/troubleshooting/#wrong-entities-matching","title":"Wrong entities matching","text":"<p>Symptom: Unrelated entities are clustered together.</p> <p>Diagnosis: <pre><code>-- Find the connecting path\nWITH cluster_members AS (\n    SELECT entity_key \n    FROM idr_out.identity_resolved_membership_current\n    WHERE resolved_id = 'problematic_cluster'\n)\nSELECT \n    i.entity_key,\n    i.identifier_type,\n    i.identifier_value_norm\nFROM idr_work.identifiers i\nJOIN cluster_members c ON i.entity_key = c.entity_key\nORDER BY i.identifier_value_norm, i.entity_key;\n</code></pre></p> <p>Solution: 1. Identify the bad identifier 2. Add to exclusion list 3. Lower max_group_size</p>"},{"location":"guides/troubleshooting/#platform-specific-issues","title":"Platform-Specific Issues","text":""},{"location":"guides/troubleshooting/#duckdb","title":"DuckDB","text":"<p>\"database is locked\" <pre><code># Only one connection at a time\n# Close other DuckDB connections\n</code></pre></p>"},{"location":"guides/troubleshooting/#snowflake","title":"Snowflake","text":"<p>\"Warehouse is suspended\" <pre><code>-- Resume warehouse\nALTER WAREHOUSE compute_wh RESUME;\n</code></pre></p> <p>\"Insufficient privileges\" <pre><code>-- Grant required permissions\nGRANT USAGE ON WAREHOUSE compute_wh TO ROLE idr_executor;\nGRANT ALL ON SCHEMA idr_meta TO ROLE idr_executor;\n</code></pre></p>"},{"location":"guides/troubleshooting/#bigquery","title":"BigQuery","text":"<p>\"Quota exceeded\" <pre><code># Reduce query frequency\n# Request quota increase in GCP Console\n</code></pre></p> <p>\"Dataset not found\" <pre><code>bq mk --dataset your_project:idr_meta\n</code></pre></p>"},{"location":"guides/troubleshooting/#databricks","title":"Databricks","text":"<p>\"Cluster terminated\" <pre><code># Check cluster auto-termination settings\n# Extend timeout or keep cluster running\n</code></pre></p>"},{"location":"guides/troubleshooting/#dry-run-issues","title":"Dry Run Issues","text":""},{"location":"guides/troubleshooting/#dry_run_results-is-empty","title":"\"dry_run_results is empty\"","text":"<p>Cause: No changes would be made.</p> <p>Diagnosis: <pre><code>-- Check if any entities processed\nSELECT COUNT(*) FROM idr_work.entities_delta;\n\n-- Check if any edges created\nSELECT COUNT(*) FROM idr_work.edges_new;\n</code></pre></p>"},{"location":"guides/troubleshooting/#dry-run-shows-unexpected-changes","title":"Dry run shows unexpected changes","text":"<p>Diagnosis: <pre><code>-- Review moved entities\nSELECT *\nFROM idr_out.dry_run_results\nWHERE run_id = 'dry_run_xyz' \n  AND change_type = 'MOVED';\n\n-- Check what identifier caused the move\n-- (cross-reference with edges_new)\n</code></pre></p>"},{"location":"guides/troubleshooting/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"guides/troubleshooting/#rollback-a-bad-run","title":"Rollback a bad run","text":"<pre><code>-- 1. Identify the bad run\nSELECT * FROM idr_out.run_history ORDER BY started_at DESC;\n\n-- 2. Note the previous good watermark\nSELECT * FROM idr_meta.run_state;\n\n-- 3. If you have backups, restore from them\n\n-- 4. Otherwise, re-run in FULL mode after fixing config\n</code></pre>"},{"location":"guides/troubleshooting/#reset-to-clean-state","title":"Reset to clean state","text":"<p>This deletes all output data</p> <p>Use only if you want to start from scratch.</p> <pre><code>-- Reset output tables\nTRUNCATE TABLE idr_out.identity_resolved_membership_current;\nTRUNCATE TABLE idr_out.identity_clusters_current;\nTRUNCATE TABLE idr_out.golden_profile_current;\n\n-- Reset watermarks\nUPDATE idr_meta.run_state \nSET last_watermark_value = '1900-01-01'::TIMESTAMP;\n</code></pre>"},{"location":"guides/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li>Check run history: <code>SELECT * FROM idr_out.run_history ORDER BY started_at DESC</code></li> <li>Check warnings: Look at the <code>warnings</code> column</li> <li>Check skipped groups: <code>SELECT * FROM idr_out.skipped_identifier_groups</code></li> <li>Enable verbose logging: Platform-specific debug options</li> <li>Open an issue: GitHub Issues</li> </ol>"},{"location":"guides/troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration</li> <li>Production Hardening</li> <li>Metrics &amp; Monitoring</li> </ul>"},{"location":"performance/benchmark-results/","title":"IDR Performance Benchmark Results","text":"<p>Status: DuckDB tested, cloud platforms pending Dataset: Retail customer data (deterministic seed: 42) Last Updated: 2026-01-04</p>"},{"location":"performance/benchmark-results/#test-environment-summary","title":"Test Environment Summary","text":"Platform Configuration Instance Type Notes DuckDB Local / Docker MacBook Pro M1/M2 Single-node, 16GB RAM Snowflake TBD Warehouse size BigQuery On-demand Serverless Auto-scaling, pay-per-query Databricks TBD Cluster config"},{"location":"performance/benchmark-results/#10-million-rows-duckdb-baseline","title":"10 Million Rows (DuckDB Baseline)","text":""},{"location":"performance/benchmark-results/#timing-results","title":"Timing Results","text":"Platform Data Load Entity Extract Edge Build Label Prop Output Gen Total DuckDB 1s 7s 33s 81s 12s 143s Snowflake 5s 17s 58s 53s 26s 168s BigQuery 5s 10s 50s 101s 91s 295s Databricks 14s 36s 77s 115s ~75s 317s"},{"location":"performance/benchmark-results/#metrics-results","title":"Metrics Results","text":"Platform Entities Edges Clusters Largest Singletons LP Iters DuckDB 10,000,000 16,124,751 1,839,324 TBD TBD 6 Snowflake 10,000,000 16,124,751 1,839,324 TBD TBD 6 BigQuery 10,000,000 16,124,751 1,839,324 TBD TBD 6 Databricks 10,000,000 16,124,751 1,839,324 TBD TBD 6"},{"location":"performance/benchmark-results/#consistency-check","title":"Consistency Check","text":"<ul> <li>[ ] All platforms produced same cluster count</li> <li>[ ] Largest cluster size matches across platforms</li> <li>[ ] Singleton count matches across platforms</li> </ul>"},{"location":"performance/benchmark-results/#50-million-rows-planned","title":"50 Million Rows (Planned)","text":""},{"location":"performance/benchmark-results/#timing-results_1","title":"Timing Results","text":"Platform Data Load Entity Extract Edge Build Label Prop Output Gen Total DuckDB TBD TBD TBD TBD TBD TBD Snowflake TBD TBD TBD TBD TBD TBD BigQuery TBD TBD TBD TBD TBD TBD Databricks TBD TBD TBD TBD TBD TBD"},{"location":"performance/benchmark-results/#metrics-results_1","title":"Metrics Results","text":"Platform Entities Edges Clusters Largest Singletons LP Iters DuckDB TBD TBD TBD TBD TBD TBD Snowflake TBD TBD TBD TBD TBD TBD BigQuery TBD TBD TBD TBD TBD TBD Databricks TBD TBD TBD TBD TBD TBD"},{"location":"performance/benchmark-results/#consistency-check_1","title":"Consistency Check","text":"<ul> <li>[ ] All platforms produced same cluster count</li> <li>[ ] Largest cluster size matches across platforms</li> <li>[ ] Singleton count matches across platforms</li> </ul>"},{"location":"performance/benchmark-results/#100-million-rows-planned","title":"100 Million Rows (Planned)","text":""},{"location":"performance/benchmark-results/#timing-results_2","title":"Timing Results","text":"Platform Data Load Entity Extract Edge Build Label Prop Output Gen Total DuckDB TBD TBD TBD TBD TBD TBD Snowflake TBD TBD TBD TBD TBD TBD BigQuery TBD TBD TBD TBD TBD TBD Databricks TBD TBD TBD TBD TBD TBD"},{"location":"performance/benchmark-results/#metrics-results_2","title":"Metrics Results","text":"Platform Entities Edges Clusters Largest Singletons LP Iters DuckDB TBD TBD TBD TBD TBD TBD Snowflake TBD TBD TBD TBD TBD TBD BigQuery TBD TBD TBD TBD TBD TBD Databricks TBD TBD TBD TBD TBD TBD"},{"location":"performance/benchmark-results/#consistency-check_2","title":"Consistency Check","text":"<ul> <li>[ ] All platforms produced same cluster count</li> <li>[ ] Largest cluster size matches across platforms</li> <li>[ ] Singleton count matches across platforms</li> </ul>"},{"location":"performance/benchmark-results/#performance-charts","title":"Performance Charts","text":""},{"location":"performance/benchmark-results/#total-duration-by-platform-scale","title":"Total Duration by Platform &amp; Scale","text":"<p><pre><code>Platform     | 10M       | 50M      | 100M\n-------------|-----------|----------|----------\nDuckDB       | \u2588\u2588\u2588 143s  |          |         \nSnowflake    | \u2588\u2588\u2588\u2588 168s |          |         \nBigQuery     | \u2588\u2588\u2588\u2588\u2588\u2588 295s|         |         \nDatabricks   | \u2588\u2588\u2588\u2588\u2588\u2588 317s|         |         \n</code></pre> (To be replaced with actual Chart.js visualization)</p>"},{"location":"performance/benchmark-results/#cost-analysis-cloud-only","title":"Cost Analysis (Cloud Only)","text":"Platform 10M Cost 50M Cost 100M Cost Notes DuckDB Free Free Free Local/Docker Snowflake ~$0.25 TBD TBD XS Warehouse: 0.1 credits BigQuery ~$0.50 TBD TBD On-demand: $6.25/TB scanned Databricks $TBD $TBD $TBD Serverless SQL Warehouse"},{"location":"performance/benchmark-results/#observations-insights","title":"Observations &amp; Insights","text":""},{"location":"performance/benchmark-results/#duckdb","title":"DuckDB","text":"<ul> <li>10M rows in 143 seconds (~2.4 min) - excellent for local/dev workloads</li> <li>Label Propagation dominates: 81s (57% of total) - bottleneck on both platforms</li> <li>Edge Building: 33s (23%), fast single-node execution</li> <li>Output Gen: 12s (8%), efficient local writes</li> <li>Throughput: ~70,000 entities/second</li> <li>Created 16.1M edges from 10M entities</li> <li>Resolved into 1.84M clusters (~5.4 entities/cluster average)</li> <li>RAM usage: ~8-12GB peak for 10M entities</li> </ul>"},{"location":"performance/benchmark-results/#snowflake","title":"Snowflake","text":"<ul> <li>10M rows in 168 seconds (~2.8 min) - fastest cloud platform!</li> <li>Only 1.17x slower than local DuckDB</li> <li>Label Propagation: 53s (32%) - fastest LP of all cloud platforms</li> <li>Edge Building: 58s (35%), excellent parallel execution</li> <li>Entity + Identifier Extraction: 22s combined (13%)</li> <li>Output Gen: 26s (15%), efficient MERGE operations</li> <li>Identical metrics: 16.1M edges, 1.84M clusters, 6 LP iterations</li> <li>Throughput: ~59,500 entities/second</li> <li>Warehouse: IDR_WH (standard size)</li> </ul>"},{"location":"performance/benchmark-results/#bigquery","title":"BigQuery","text":"<ul> <li>10M rows in 295 seconds (~4.9 min) - 2.1x slower than DuckDB</li> <li>Label Propagation: 101s (34%) - serverless overhead on iterative queries</li> <li>Output Gen: 91s (31%) - MERGE operations have high network overhead</li> <li>Edge Building: 50s (17%), good parallel execution</li> <li>Identical metrics: 16.1M edges, 1.84M clusters, 6 LP iterations</li> <li>Throughput: ~34,000 entities/second</li> <li>Estimated cost: ~$0.50 for 10M rows (on-demand pricing)</li> <li>BigQuery wins at larger scales due to horizontal scaling</li> </ul>"},{"location":"performance/benchmark-results/#databricks","title":"Databricks","text":"<ul> <li>10M rows in 317 seconds (~5.3 min) - 2.2x slower than DuckDB</li> <li>Similar to BigQuery performance</li> <li>Identical metrics: 16.1M edges, 1.84M clusters, 6 LP iterations</li> <li>Throughput: ~31,500 entities/second</li> <li>Unity Catalog adds overhead for table operations</li> <li>Better suited for larger datasets with Spark parallelism</li> </ul>"},{"location":"performance/benchmark-results/#recommendations","title":"Recommendations","text":""},{"location":"performance/benchmark-results/#when-to-use-each-platform","title":"When to Use Each Platform","text":"Use Case Recommended Platform Rationale &lt; 10M rows, local dev DuckDB Fast, free, no infra 10-50M rows, batch TBD Based on testing 50-100M rows TBD Based on testing &gt; 100M rows TBD Based on testing Real-time/streaming TBD Based on latency needs"},{"location":"performance/benchmark-results/#test-commands","title":"Test Commands","text":"<pre><code># Generate 20M dataset\npython tools/scale_test/data_generator.py --rows=20000000 --seed=42 --output=data/\n\n# Run DuckDB benchmark\npython tools/scale_test/benchmark.py \\\n    --platform=duckdb \\\n    --data=data/retail_customers_20m.parquet \\\n    --rows=20000000 \\\n    --db=idr_benchmark.duckdb\n\n# Generate 50M dataset\npython tools/scale_test/data_generator.py --rows=50000000 --seed=42 --output=data/\n\n# Generate 100M dataset\npython tools/scale_test/data_generator.py --rows=100000000 --seed=42 --output=data/\n</code></pre>"},{"location":"performance/benchmark-results/#appendix-test-data-distribution","title":"Appendix: Test Data Distribution","text":"<p>Target distribution (retail industry standard):</p> Cluster Size Percentage @ 20M @ 50M @ 100M 1 (singleton) 35% 7M 17.5M 35M 2 (pairs) 25% 5M 12.5M 25M 3-5 (small) 20% 4M 10M 20M 6-15 (medium) 12% 2.4M 6M 12M 16-50 (large) 5% 1M 2.5M 5M 51-200 (v.large) 2% 400K 1M 2M 201-1000 (massive) 1% 200K 500K 1M <p>Identifier match rates: - Email: 55% - Phone: 25% - Loyalty ID: 10% - Address: 10% - Chain patterns: 15%</p>"},{"location":"reference/cli-reference/","title":"CLI Reference","text":"<p>Command-line interface reference for SQL Identity Resolution runners.</p>"},{"location":"reference/cli-reference/#duckdb-cli","title":"DuckDB CLI","text":""},{"location":"reference/cli-reference/#usage","title":"Usage","text":"<pre><code>python sql/duckdb/idr_run.py [OPTIONS]\n</code></pre>"},{"location":"reference/cli-reference/#options","title":"Options","text":"Option Type Default Description <code>--db</code> STRING Required Path to DuckDB database file <code>--run-mode</code> ENUM <code>INCR</code> <code>FULL</code> or <code>INCR</code> <code>--max-iters</code> INT <code>30</code> Max label propagation iterations <code>--dry-run</code> FLAG <code>False</code> Preview mode (no commits)"},{"location":"reference/cli-reference/#examples","title":"Examples","text":"<pre><code># Full run\npython sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=FULL\n\n# Incremental run\npython sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=INCR\n\n# Dry run\npython sql/duckdb/idr_run.py --db=idr.duckdb --run-mode=FULL --dry-run\n\n# Custom max iterations\npython sql/duckdb/idr_run.py --db=idr.duckdb --max-iters=50\n</code></pre>"},{"location":"reference/cli-reference/#exit-codes","title":"Exit Codes","text":"Code Meaning <code>0</code> Success <code>1</code> Error"},{"location":"reference/cli-reference/#bigquery-cli","title":"BigQuery CLI","text":""},{"location":"reference/cli-reference/#usage_1","title":"Usage","text":"<pre><code>python sql/bigquery/idr_run.py [OPTIONS]\n</code></pre>"},{"location":"reference/cli-reference/#options_1","title":"Options","text":"Option Type Default Description <code>--project</code> STRING Required GCP project ID <code>--run-mode</code> ENUM <code>INCR</code> <code>FULL</code> or <code>INCR</code> <code>--max-iters</code> INT <code>30</code> Max label propagation iterations <code>--dry-run</code> FLAG <code>False</code> Preview mode (no commits)"},{"location":"reference/cli-reference/#environment-variables","title":"Environment Variables","text":"Variable Description <code>GOOGLE_APPLICATION_CREDENTIALS</code> Path to service account JSON"},{"location":"reference/cli-reference/#examples_1","title":"Examples","text":"<pre><code># Set credentials\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n\n# Full run\npython sql/bigquery/idr_run.py --project=my-project --run-mode=FULL\n\n# Dry run\npython sql/bigquery/idr_run.py --project=my-project --dry-run\n</code></pre>"},{"location":"reference/cli-reference/#snowflake-stored-procedure","title":"Snowflake Stored Procedure","text":""},{"location":"reference/cli-reference/#signature","title":"Signature","text":"<pre><code>CALL idr_run(\n    RUN_MODE VARCHAR,      -- 'FULL' or 'INCR'\n    MAX_ITERS INT,         -- Max iterations (e.g., 30)\n    DRY_RUN BOOLEAN        -- TRUE = preview only\n);\n</code></pre>"},{"location":"reference/cli-reference/#examples_2","title":"Examples","text":"<pre><code>-- Full run\nCALL idr_run('FULL', 30, FALSE);\n\n-- Incremental run\nCALL idr_run('INCR', 30, FALSE);\n\n-- Dry run\nCALL idr_run('FULL', 30, TRUE);\n</code></pre>"},{"location":"reference/cli-reference/#return-value","title":"Return Value","text":"<p>Returns a VARCHAR with run summary:</p> <pre><code>SUCCESS: run_id=run_abc123, entities=1234, edges=5678, iterations=5, duration=12s\n</code></pre> <p>Or for dry runs:</p> <pre><code>DRY_RUN_COMPLETE: run_id=dry_run_abc123, new_entities=100, moved_entities=50, duration=8s | DRY RUN - NO CHANGES COMMITTED\n</code></pre>"},{"location":"reference/cli-reference/#databricks-widgets","title":"Databricks Widgets","text":""},{"location":"reference/cli-reference/#widget-parameters","title":"Widget Parameters","text":"Widget Type Options Default Description <code>RUN_MODE</code> dropdown <code>INCR</code>, <code>FULL</code> <code>INCR</code> Processing mode <code>MAX_ITERS</code> text Integer <code>30</code> Max iterations <code>DRY_RUN</code> dropdown <code>true</code>, <code>false</code> <code>false</code> Preview mode <code>RUN_ID</code> text String Auto-generated Custom run ID"},{"location":"reference/cli-reference/#programmatic-access","title":"Programmatic Access","text":"<pre><code># Read widget values\nrun_mode = dbutils.widgets.get(\"RUN_MODE\")\ndry_run = dbutils.widgets.get(\"DRY_RUN\") == \"true\"\n\n# Set widget defaults\ndbutils.widgets.dropdown(\"RUN_MODE\", \"INCR\", [\"INCR\", \"FULL\"])\ndbutils.widgets.dropdown(\"DRY_RUN\", \"false\", [\"true\", \"false\"])\n</code></pre>"},{"location":"reference/cli-reference/#running-via-jobs-api","title":"Running via Jobs API","text":"<pre><code>{\n  \"notebook_task\": {\n    \"notebook_path\": \"/Repos/org/repo/sql/databricks/notebooks/IDR_Run\",\n    \"base_parameters\": {\n      \"RUN_MODE\": \"INCR\",\n      \"DRY_RUN\": \"false\",\n      \"MAX_ITERS\": \"30\"\n    }\n  }\n}\n</code></pre>"},{"location":"reference/cli-reference/#metrics-exporter-cli","title":"Metrics Exporter CLI","text":""},{"location":"reference/cli-reference/#usage_2","title":"Usage","text":"<pre><code>python tools/metrics_exporter.py [OPTIONS]\n</code></pre>"},{"location":"reference/cli-reference/#options_2","title":"Options","text":"Option Type Default Description <code>--platform</code> ENUM Required <code>duckdb</code>, <code>snowflake</code>, <code>bigquery</code> <code>--connection</code> STRING DuckDB: path; Others: connection string <code>--exporter</code> ENUM <code>stdout</code> <code>stdout</code>, <code>prometheus</code>, <code>datadog</code>, <code>webhook</code> <code>--prometheus-port</code> INT <code>9090</code> Port for Prometheus metrics <code>--datadog-api-key</code> STRING DataDog API key <code>--webhook-url</code> STRING Webhook endpoint URL <code>--run-id</code> STRING Export specific run only"},{"location":"reference/cli-reference/#examples_3","title":"Examples","text":"<pre><code># Print to stdout\npython tools/metrics_exporter.py --platform=duckdb --connection=idr.duckdb\n\n# Prometheus endpoint\npython tools/metrics_exporter.py --platform=duckdb --connection=idr.duckdb \\\n    --exporter=prometheus --prometheus-port=9090\n\n# DataDog\npython tools/metrics_exporter.py --platform=snowflake \\\n    --connection=\"account=xxx;user=xxx;password=xxx\" \\\n    --exporter=datadog --datadog-api-key=$DD_API_KEY\n\n# Webhook\npython tools/metrics_exporter.py --platform=bigquery \\\n    --connection=\"project=my-project\" \\\n    --exporter=webhook --webhook-url=https://hooks.slack.com/xxx\n</code></pre>"},{"location":"reference/cli-reference/#dashboard-generator-cli","title":"Dashboard Generator CLI","text":""},{"location":"reference/cli-reference/#usage_3","title":"Usage","text":"<pre><code>python tools/dashboard/generator.py [OPTIONS]\n</code></pre>"},{"location":"reference/cli-reference/#options_3","title":"Options","text":"Option Type Default Description <code>--platform</code> ENUM Required <code>duckdb</code>, <code>snowflake</code>, <code>bigquery</code>, <code>databricks</code> <code>--connection</code> STRING Required Platform-specific connection string <code>--output</code> STRING <code>dashboard.html</code> Output file path <code>--run-id</code> STRING Focus on specific run"},{"location":"reference/cli-reference/#examples_4","title":"Examples","text":"<pre><code># Generate from DuckDB\npython tools/dashboard/generator.py \\\n    --platform=duckdb \\\n    --connection=idr.duckdb \\\n    --output=dashboard.html\n\n# Open in browser\nopen dashboard.html\n</code></pre>"},{"location":"reference/cli-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/cli-reference/#cicd-integration","title":"CI/CD Integration","text":"<pre><code>#!/bin/bash\n# ci-run.sh\n\n# Dry run first\npython sql/duckdb/idr_run.py --db=idr.duckdb --dry-run\nif [ $? -ne 0 ]; then\n    echo \"Dry run failed\"\n    exit 1\nfi\n\n# Check for unexpected changes\nMOVED=$(duckdb idr.duckdb -c \"SELECT COUNT(*) FROM idr_out.dry_run_results WHERE change_type='MOVED'\" | tail -1)\nif [ \"$MOVED\" -gt 1000 ]; then\n    echo \"Too many moved entities: $MOVED\"\n    exit 1\nfi\n\n# Live run\npython sql/duckdb/idr_run.py --db=idr.duckdb\n</code></pre>"},{"location":"reference/cli-reference/#scheduled-run-with-logging","title":"Scheduled Run with Logging","text":"<pre><code>#!/bin/bash\n# scheduled-run.sh\n\nLOG_FILE=\"/var/log/idr/$(date +%Y%m%d_%H%M%S).log\"\n\npython sql/duckdb/idr_run.py \\\n    --db=/data/idr.duckdb \\\n    --run-mode=INCR \\\n    2&gt;&amp;1 | tee \"$LOG_FILE\"\n\n# Check exit code\nif [ ${PIPESTATUS[0]} -ne 0 ]; then\n    # Send alert\n    curl -X POST https://hooks.slack.com/xxx \\\n        -d \"{\\\"text\\\": \\\"IDR run failed. See $LOG_FILE\\\"}\"\nfi\n</code></pre>"},{"location":"reference/cli-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Schema Reference</li> <li>Metrics Reference</li> <li>Troubleshooting</li> </ul>"},{"location":"reference/metrics-reference/","title":"Metrics Reference","text":"<p>Reference for all metrics exported by SQL Identity Resolution.</p>"},{"location":"reference/metrics-reference/#built-in-metrics","title":"Built-in Metrics","text":"<p>These metrics are automatically recorded during each run.</p> Metric Name Type Unit Description <code>idr_run_duration_seconds</code> gauge seconds Total run duration <code>idr_entities_processed</code> gauge count Entities processed this run <code>idr_edges_created</code> counter count Edges created <code>idr_clusters_impacted</code> gauge count Clusters affected <code>idr_lp_iterations</code> gauge count Label propagation iterations <code>idr_groups_skipped</code> counter count Groups skipped (max_group_size) <code>idr_values_excluded</code> counter count Values excluded (exclusion list) <code>idr_large_clusters</code> gauge count Clusters exceeding threshold"},{"location":"reference/metrics-reference/#metric-types","title":"Metric Types","text":""},{"location":"reference/metrics-reference/#gauge","title":"Gauge","text":"<p>Current point-in-time value. Can go up or down.</p> <p>Examples: - <code>idr_run_duration_seconds</code> - <code>idr_clusters_impacted</code> - <code>idr_large_clusters</code></p>"},{"location":"reference/metrics-reference/#counter","title":"Counter","text":"<p>Cumulative count that only increases.</p> <p>Examples: - <code>idr_edges_created</code> - <code>idr_groups_skipped</code></p>"},{"location":"reference/metrics-reference/#dimensions","title":"Dimensions","text":"<p>Metrics can include dimensions for filtering:</p> Dimension Values Description <code>run_mode</code> <code>FULL</code>, <code>INCR</code> Processing mode <code>platform</code> <code>duckdb</code>, <code>snowflake</code>, <code>bigquery</code>, <code>databricks</code> Platform <code>status</code> <code>SUCCESS</code>, <code>FAILED</code>, <code>DRY_RUN_COMPLETE</code> Run status <p>Example with dimensions: <pre><code>{\n  \"metric_name\": \"idr_run_duration_seconds\",\n  \"metric_value\": 42,\n  \"dimensions\": {\n    \"run_mode\": \"INCR\",\n    \"platform\": \"snowflake\"\n  }\n}\n</code></pre></p>"},{"location":"reference/metrics-reference/#querying-metrics","title":"Querying Metrics","text":""},{"location":"reference/metrics-reference/#basic-query","title":"Basic Query","text":"<pre><code>SELECT \n    run_id,\n    metric_name,\n    metric_value,\n    recorded_at\nFROM idr_out.metrics_export\nWHERE run_id = 'run_abc123';\n</code></pre>"},{"location":"reference/metrics-reference/#time-series","title":"Time Series","text":"<pre><code>SELECT \n    DATE(recorded_at) as date,\n    metric_name,\n    AVG(metric_value) as avg_value,\n    MAX(metric_value) as max_value,\n    COUNT(*) as samples\nFROM idr_out.metrics_export\nWHERE metric_name = 'idr_run_duration_seconds'\n  AND recorded_at &gt;= CURRENT_DATE - 30\nGROUP BY DATE(recorded_at), metric_name\nORDER BY date DESC;\n</code></pre>"},{"location":"reference/metrics-reference/#percentiles","title":"Percentiles","text":"<pre><code>SELECT \n    metric_name,\n    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY metric_value) as p50,\n    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY metric_value) as p90,\n    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY metric_value) as p99\nFROM idr_out.metrics_export\nWHERE metric_name = 'idr_run_duration_seconds'\n  AND recorded_at &gt;= CURRENT_DATE - 30\nGROUP BY metric_name;\n</code></pre>"},{"location":"reference/metrics-reference/#prometheus-format","title":"Prometheus Format","text":"<p>When using the Prometheus exporter:</p> <pre><code># HELP idr_run_duration_seconds Total run duration\n# TYPE idr_run_duration_seconds gauge\nidr_run_duration_seconds{run_id=\"run_abc123\",run_mode=\"INCR\"} 42\n\n# HELP idr_entities_processed Entities processed this run\n# TYPE idr_entities_processed gauge\nidr_entities_processed{run_id=\"run_abc123\"} 1234\n\n# HELP idr_edges_created Edges created\n# TYPE idr_edges_created counter\nidr_edges_created{run_id=\"run_abc123\"} 5678\n</code></pre>"},{"location":"reference/metrics-reference/#datadog-format","title":"DataDog Format","text":"<p>When using the DataDog exporter:</p> <pre><code>{\n  \"series\": [\n    {\n      \"metric\": \"idr.run_duration_seconds\",\n      \"type\": \"gauge\",\n      \"points\": [[1704067200, 42]],\n      \"tags\": [\"run_id:run_abc123\", \"run_mode:INCR\"]\n    },\n    {\n      \"metric\": \"idr.entities_processed\",\n      \"type\": \"gauge\",\n      \"points\": [[1704067200, 1234]],\n      \"tags\": [\"run_id:run_abc123\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/metrics-reference/#alerting-thresholds","title":"Alerting Thresholds","text":""},{"location":"reference/metrics-reference/#recommended-alerts","title":"Recommended Alerts","text":"Metric Condition Severity Action <code>idr_run_duration_seconds</code> &gt; 3600 Warning Investigate performance <code>idr_run_duration_seconds</code> &gt; 7200 Critical Immediate attention <code>idr_groups_skipped</code> &gt; 100 Warning Review max_group_size <code>idr_large_clusters</code> &gt; 10 Warning Investigate cluster growth <code>idr_lp_iterations</code> = max_iters Warning May not have converged"},{"location":"reference/metrics-reference/#prometheus-alert-rules","title":"Prometheus Alert Rules","text":"<pre><code>groups:\n  - name: idr_alerts\n    rules:\n      - alert: IDRRunTooLong\n        expr: idr_run_duration_seconds &gt; 3600\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"IDR run taking too long\"\n          description: \"Run {{ $labels.run_id }} has been running for {{ $value }} seconds\"\n\n      - alert: IDRManySkippedGroups\n        expr: idr_groups_skipped &gt; 100\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Many identifier groups skipped\"\n          description: \"{{ $value }} groups were skipped in run {{ $labels.run_id }}\"\n</code></pre>"},{"location":"reference/metrics-reference/#datadog-monitors","title":"DataDog Monitors","text":"<pre><code>name: \"IDR - High Skipped Groups\"\ntype: metric alert\nquery: \"avg(last_5m):sum:idr.groups_skipped{*} &gt; 100\"\nmessage: \"More than 100 identifier groups skipped. Review max_group_size settings.\"\ntags:\n  - \"service:identity-resolution\"\n  - \"team:data\"\n</code></pre>"},{"location":"reference/metrics-reference/#custom-metrics","title":"Custom Metrics","text":""},{"location":"reference/metrics-reference/#adding-custom-metrics","title":"Adding Custom Metrics","text":"<p>In Python runners: <pre><code>record_metric('my_custom_metric', value, {'dimension': 'value'}, 'gauge')\n</code></pre></p> <p>In Snowflake: <pre><code>recordMetric('my_custom_metric', value, {dimension: 'value'}, 'gauge');\n</code></pre></p>"},{"location":"reference/metrics-reference/#metric-naming-conventions","title":"Metric Naming Conventions","text":"<ul> <li>Use <code>idr_</code> prefix</li> <li>Use snake_case</li> <li>Include unit in name if applicable (<code>_seconds</code>, <code>_bytes</code>, <code>_count</code>)</li> <li>Be specific: <code>idr_email_edges_created</code> instead of just <code>edges</code></li> </ul>"},{"location":"reference/metrics-reference/#retention","title":"Retention","text":""},{"location":"reference/metrics-reference/#cleanup-old-metrics","title":"Cleanup Old Metrics","text":"<pre><code>-- Delete metrics older than 90 days\nDELETE FROM idr_out.metrics_export\nWHERE recorded_at &lt; CURRENT_TIMESTAMP - INTERVAL '90 days';\n</code></pre>"},{"location":"reference/metrics-reference/#partitioning-bigquery","title":"Partitioning (BigQuery)","text":"<pre><code>CREATE TABLE idr_out.metrics_export\nPARTITION BY DATE(recorded_at)\nOPTIONS (\n  partition_expiration_days = 90\n);\n</code></pre>"},{"location":"reference/metrics-reference/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference</li> <li>Schema Reference</li> <li>Metrics &amp; Monitoring Guide</li> </ul>"},{"location":"reference/schema-reference/","title":"Schema Reference","text":"<p>Complete DDL reference for all SQL Identity Resolution tables.</p>"},{"location":"reference/schema-reference/#quick-links","title":"Quick Links","text":"<ul> <li>idr_meta Schema</li> <li>idr_work Schema</li> <li>idr_out Schema</li> </ul>"},{"location":"reference/schema-reference/#idr_meta-configuration","title":"idr_meta (Configuration)","text":""},{"location":"reference/schema-reference/#source_table","title":"source_table","text":"<pre><code>CREATE TABLE idr_meta.source_table (\n    table_id VARCHAR PRIMARY KEY,\n    table_fqn VARCHAR NOT NULL,\n    entity_type VARCHAR NOT NULL,\n    entity_key_expr VARCHAR NOT NULL,\n    watermark_column VARCHAR NOT NULL,\n    watermark_lookback_minutes INT DEFAULT 0,\n    is_active BOOLEAN DEFAULT TRUE\n);\n</code></pre>"},{"location":"reference/schema-reference/#rule","title":"rule","text":"<pre><code>CREATE TABLE idr_meta.rule (\n    rule_id VARCHAR PRIMARY KEY,\n    identifier_type VARCHAR NOT NULL,\n    priority INT NOT NULL,\n    is_active BOOLEAN DEFAULT TRUE,\n    max_group_size INT DEFAULT 10000\n);\n</code></pre>"},{"location":"reference/schema-reference/#identifier_mapping","title":"identifier_mapping","text":"<pre><code>CREATE TABLE idr_meta.identifier_mapping (\n    table_id VARCHAR NOT NULL,\n    identifier_type VARCHAR NOT NULL,\n    column_expr VARCHAR NOT NULL,\n    requires_normalization BOOLEAN DEFAULT TRUE,\n    PRIMARY KEY (table_id, identifier_type, column_expr)\n);\n</code></pre>"},{"location":"reference/schema-reference/#run_state","title":"run_state","text":"<pre><code>CREATE TABLE idr_meta.run_state (\n    table_id VARCHAR PRIMARY KEY,\n    last_watermark_value TIMESTAMP,\n    last_run_id VARCHAR,\n    last_run_ts TIMESTAMP\n);\n</code></pre>"},{"location":"reference/schema-reference/#config","title":"config","text":"<pre><code>CREATE TABLE idr_meta.config (\n    config_key VARCHAR PRIMARY KEY,\n    config_value VARCHAR NOT NULL,\n    description VARCHAR,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Default values\nINSERT INTO idr_meta.config VALUES\n    ('dry_run_retention_days', '7', 'Days to retain dry run results', CURRENT_TIMESTAMP),\n    ('large_cluster_threshold', '5000', 'Threshold for large cluster warnings', CURRENT_TIMESTAMP);\n</code></pre>"},{"location":"reference/schema-reference/#identifier_exclusion","title":"identifier_exclusion","text":"<pre><code>CREATE TABLE idr_meta.identifier_exclusion (\n    identifier_type VARCHAR NOT NULL,\n    identifier_pattern VARCHAR NOT NULL,\n    is_pattern BOOLEAN DEFAULT FALSE,\n    reason VARCHAR,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (identifier_type, identifier_pattern)\n);\n</code></pre>"},{"location":"reference/schema-reference/#idr_work-processing","title":"idr_work (Processing)","text":"<p>These are transient/temporary tables created during processing.</p>"},{"location":"reference/schema-reference/#entities_delta","title":"entities_delta","text":"<pre><code>CREATE TABLE idr_work.entities_delta (\n    entity_key VARCHAR PRIMARY KEY,\n    table_id VARCHAR NOT NULL,\n    watermark_value TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#identifiers","title":"identifiers","text":"<pre><code>CREATE TABLE idr_work.identifiers (\n    entity_key VARCHAR NOT NULL,\n    identifier_type VARCHAR NOT NULL,\n    identifier_value_raw VARCHAR,\n    identifier_value_norm VARCHAR NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#edges_new","title":"edges_new","text":"<pre><code>CREATE TABLE idr_work.edges_new (\n    entity_a VARCHAR NOT NULL,\n    entity_b VARCHAR NOT NULL,\n    identifier_type VARCHAR NOT NULL,\n    PRIMARY KEY (entity_a, entity_b, identifier_type)\n);\n</code></pre>"},{"location":"reference/schema-reference/#lp_labels","title":"lp_labels","text":"<pre><code>CREATE TABLE idr_work.lp_labels (\n    entity_key VARCHAR PRIMARY KEY,\n    label VARCHAR NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#membership_updates","title":"membership_updates","text":"<pre><code>CREATE TABLE idr_work.membership_updates (\n    entity_key VARCHAR PRIMARY KEY,\n    resolved_id VARCHAR NOT NULL,\n    updated_ts TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#cluster_sizes_updates","title":"cluster_sizes_updates","text":"<pre><code>CREATE TABLE idr_work.cluster_sizes_updates (\n    resolved_id VARCHAR PRIMARY KEY,\n    cluster_size INT NOT NULL,\n    updated_ts TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#idr_out-output","title":"idr_out (Output)","text":""},{"location":"reference/schema-reference/#identity_resolved_membership_current","title":"identity_resolved_membership_current","text":"<pre><code>CREATE TABLE idr_out.identity_resolved_membership_current (\n    entity_key VARCHAR PRIMARY KEY,\n    resolved_id VARCHAR NOT NULL,\n    updated_ts TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#identity_clusters_current","title":"identity_clusters_current","text":"<pre><code>CREATE TABLE idr_out.identity_clusters_current (\n    resolved_id VARCHAR PRIMARY KEY,\n    cluster_size INT NOT NULL,\n    updated_ts TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#golden_profile_current","title":"golden_profile_current","text":"<pre><code>CREATE TABLE idr_out.golden_profile_current (\n    resolved_id VARCHAR PRIMARY KEY,\n    email_primary VARCHAR,\n    phone_primary VARCHAR,\n    first_name VARCHAR,\n    last_name VARCHAR,\n    updated_ts TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"reference/schema-reference/#run_history","title":"run_history","text":"<pre><code>CREATE TABLE idr_out.run_history (\n    run_id VARCHAR PRIMARY KEY,\n    run_mode VARCHAR NOT NULL,\n    started_at TIMESTAMP NOT NULL,\n    ended_at TIMESTAMP,\n    status VARCHAR,\n    entities_processed INT,\n    edges_created INT,\n    clusters_impacted INT,\n    lp_iterations INT,\n    duration_seconds INT,\n    groups_skipped INT DEFAULT 0,\n    values_excluded INT DEFAULT 0,\n    large_clusters INT DEFAULT 0,\n    warnings VARCHAR\n);\n</code></pre>"},{"location":"reference/schema-reference/#dry_run_results","title":"dry_run_results","text":"<pre><code>CREATE TABLE idr_out.dry_run_results (\n    run_id VARCHAR NOT NULL,\n    entity_key VARCHAR NOT NULL,\n    current_resolved_id VARCHAR,\n    proposed_resolved_id VARCHAR,\n    change_type VARCHAR NOT NULL,\n    current_cluster_size INT,\n    proposed_cluster_size INT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (run_id, entity_key)\n);\n</code></pre>"},{"location":"reference/schema-reference/#dry_run_summary","title":"dry_run_summary","text":"<pre><code>CREATE TABLE idr_out.dry_run_summary (\n    run_id VARCHAR PRIMARY KEY,\n    total_entities INT NOT NULL,\n    new_entities INT NOT NULL,\n    moved_entities INT NOT NULL,\n    unchanged_entities INT NOT NULL,\n    merged_clusters INT DEFAULT 0,\n    split_clusters INT DEFAULT 0,\n    largest_proposed_cluster INT,\n    edges_would_create INT,\n    groups_would_skip INT,\n    values_would_exclude INT,\n    execution_time_seconds INT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"reference/schema-reference/#metrics_export","title":"metrics_export","text":"<pre><code>CREATE TABLE idr_out.metrics_export (\n    metric_id VARCHAR PRIMARY KEY DEFAULT gen_random_uuid(),\n    run_id VARCHAR NOT NULL,\n    metric_name VARCHAR NOT NULL,\n    metric_value DOUBLE NOT NULL,\n    metric_type VARCHAR DEFAULT 'gauge',\n    dimensions VARCHAR,\n    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    exported_at TIMESTAMP\n);\n</code></pre>"},{"location":"reference/schema-reference/#skipped_identifier_groups","title":"skipped_identifier_groups","text":"<pre><code>CREATE TABLE idr_out.skipped_identifier_groups (\n    run_id VARCHAR NOT NULL,\n    identifier_type VARCHAR NOT NULL,\n    identifier_value_norm VARCHAR NOT NULL,\n    group_size INT NOT NULL,\n    max_allowed INT NOT NULL,\n    sample_entity_keys VARCHAR,\n    reason VARCHAR,\n    skipped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (run_id, identifier_type, identifier_value_norm)\n);\n</code></pre>"},{"location":"reference/schema-reference/#stage_metrics","title":"stage_metrics","text":"<pre><code>CREATE TABLE idr_out.stage_metrics (\n    run_id VARCHAR NOT NULL,\n    stage_name VARCHAR NOT NULL,\n    started_at TIMESTAMP NOT NULL,\n    ended_at TIMESTAMP,\n    rows_affected INT,\n    duration_seconds INT,\n    PRIMARY KEY (run_id, stage_name)\n);\n</code></pre>"},{"location":"reference/schema-reference/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"reference/schema-reference/#duckdb","title":"DuckDB","text":"<ul> <li>Uses <code>gen_random_uuid()</code> for UUID generation</li> <li>All types are standard SQL types</li> </ul>"},{"location":"reference/schema-reference/#snowflake","title":"Snowflake","text":"<ul> <li>Uses <code>UUID_STRING()</code> for UUID generation</li> <li><code>VARCHAR</code> maps to <code>VARCHAR</code></li> <li>Transient tables for work schema</li> </ul>"},{"location":"reference/schema-reference/#bigquery","title":"BigQuery","text":"<ul> <li>Uses <code>GENERATE_UUID()</code> for UUID generation</li> <li><code>VARCHAR</code> maps to <code>STRING</code></li> <li><code>BOOLEAN</code> maps to <code>BOOL</code></li> <li><code>TIMESTAMP</code> maps to <code>TIMESTAMP</code></li> </ul>"},{"location":"reference/schema-reference/#databricks","title":"Databricks","text":"<ul> <li>Uses <code>uuid()</code> for UUID generation</li> <li>Delta Lake format recommended</li> <li>Supports Unity Catalog</li> </ul>"},{"location":"reference/schema-reference/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference</li> <li>Metrics Reference</li> <li>Data Model</li> </ul>"}]}